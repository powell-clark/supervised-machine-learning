{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X5: Interpretability & Explainability - Understanding Model Decisions\n",
    "\n",
    "In production machine learning, it is not enough to have an accurate model. You must be able to **explain** why the model makes its predictions. This is critical for:\n",
    "\n",
    "- **Regulatory Compliance**: EU AI Act, GDPR right to explanation, financial regulations\n",
    "- **Trust**: Stakeholders need to understand and trust model decisions\n",
    "- **Debugging**: Identifying when and why models fail\n",
    "- **Fairness**: Detecting bias and discrimination\n",
    "- **Business Value**: Understanding feature importance drives business insights\n",
    "\n",
    "This notebook covers the essential interpretability and explainability techniques used in production machine learning in 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Why Interpretability Matters](#why-interpretability)\n",
    "2. [Model-Specific Interpretability](#model-specific)\n",
    "   - Linear Models (coefficients)\n",
    "   - Decision Trees (feature importance, tree structure)\n",
    "   - Random Forests (MDI, permutation importance)\n",
    "3. [Model-Agnostic Methods](#model-agnostic)\n",
    "   - SHAP (SHapley Additive exPlanations)\n",
    "   - LIME (Local Interpretable Model-agnostic Explanations)\n",
    "   - Permutation Feature Importance\n",
    "4. [Partial Dependence Plots](#pdp)\n",
    "5. [Individual Conditional Expectation (ICE)](#ice)\n",
    "6. [Global vs Local Explanations](#global-local)\n",
    "7. [Best Practices](#best-practices)\n",
    "8. [Real-World Application](#real-world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for interpretability\n",
    "# Uncomment if running locally\n",
    "# !pip install shap lime\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install SHAP and LIME if not available\n",
    "try:\n",
    "    import shap\n",
    "    print(f'‚úÖ SHAP version {shap.__version__} found')\n",
    "except ImportError:\n",
    "    print('Installing SHAP...')\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"shap\"])\n",
    "    import shap\n",
    "    print(f'‚úÖ SHAP version {shap.__version__} installed')\n",
    "\n",
    "try:\n",
    "    import lime\n",
    "    print(f'‚úÖ LIME found')\n",
    "except ImportError:\n",
    "    print('Installing LIME...')\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lime\"])\n",
    "    import lime\n",
    "    print(f'‚úÖ LIME installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "# Interpretability libraries\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ All libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"why-interpretability\"></a>\n",
    "## 1. Why Interpretability Matters\n",
    "\n",
    "### The Accuracy-Interpretability Tradeoff\n",
    "\n",
    "Traditionally, there was a tradeoff:\n",
    "- **Linear models**: Highly interpretable, lower accuracy\n",
    "- **Neural networks, ensembles**: Higher accuracy, \"black boxes\"\n",
    "\n",
    "**Modern reality in 2025**: You need BOTH accuracy AND interpretability. Techniques like SHAP and LIME let you explain any model.\n",
    "\n",
    "### Real-World Requirements\n",
    "\n",
    "**Healthcare**: \"Why did the model predict this patient is high-risk?\"\n",
    "- Doctors need to understand and trust predictions\n",
    "- Regulatory bodies require explainability\n",
    "- Liability concerns\n",
    "\n",
    "**Finance**: \"Why was this loan application rejected?\"\n",
    "- Fair lending laws require explanations\n",
    "- Applicants have right to know\n",
    "- Detect discriminatory patterns\n",
    "\n",
    "**Hiring**: \"Why wasn't this candidate recommended?\"\n",
    "- Anti-discrimination laws\n",
    "- Ethical considerations\n",
    "- Legal liability\n",
    "\n",
    "### EU AI Act (2024) Requirements\n",
    "\n",
    "High-risk AI systems must provide:\n",
    "1. Explanations of how the system works\n",
    "2. Information about training data\n",
    "3. Accuracy metrics and limitations\n",
    "4. Human oversight mechanisms\n",
    "\n",
    "**Bottom line**: Interpretability is not optional in 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-specific\"></a>\n",
    "## 2. Model-Specific Interpretability\n",
    "\n",
    "Some models are inherently interpretable. We should use model-specific interpretation when available because it is faster and often more accurate than model-agnostic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models: Coefficients Tell the Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset for classification\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, log_reg.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, log_reg.predict(X_test))\n",
    "\n",
    "print(f'Logistic Regression Performance:')\n",
    "print(f'  Training Accuracy: {train_acc:.4f}')\n",
    "print(f'  Test Accuracy:     {test_acc:.4f}')\n",
    "\n",
    "# Get coefficients and feature importance\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(f'\\nüìä Top 10 Most Important Features (by absolute coefficient):')\n",
    "print(coefficients.head(10)[['Feature', 'Coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance for linear model\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_features = coefficients.head(15)\n",
    "colors = ['green' if c > 0 else 'red' for c in top_features['Coefficient']]\n",
    "\n",
    "ax.barh(range(len(top_features)), top_features['Coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'])\n",
    "ax.set_xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Logistic Regression: Feature Importance\\n(Positive = increases cancer probability)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='green', alpha=0.7, label='Increases cancer probability'),\n",
    "                   Patch(facecolor='red', alpha=0.7, label='Decreases cancer probability')]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° Interpretation:')\n",
    "print('  Green bars: Features that increase probability of malignant cancer')\n",
    "print('  Red bars: Features that decrease probability (protective factors)')\n",
    "print('  Longer bars: Stronger influence on prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees: Visual Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple decision tree for visualization\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(f'Decision Tree Performance:')\n",
    "print(f'  Training Accuracy: {accuracy_score(y_train, tree.predict(X_train)):.4f}')\n",
    "print(f'  Test Accuracy:     {accuracy_score(y_test, tree.predict(X_test)):.4f}')\n",
    "\n",
    "# Feature importance from decision tree\n",
    "tree_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': tree.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f'\\nüìä Feature Importance from Decision Tree:')\n",
    "print(tree_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision tree structure\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(tree, \n",
    "          feature_names=feature_names,\n",
    "          class_names=['Malignant', 'Benign'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10,\n",
    "          ax=ax)\n",
    "ax.set_title('Decision Tree Structure (max_depth=3)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° How to Read This Tree:')\n",
    "print('  ‚Ä¢ Each box is a decision node')\n",
    "print('  ‚Ä¢ Top line: Decision rule (e.g., \"worst radius <= 16.8\")')\n",
    "print('  ‚Ä¢ Middle: Gini impurity (lower = purer node)')\n",
    "print('  ‚Ä¢ Bottom: Number of samples [malignant, benign]')\n",
    "print('  ‚Ä¢ Color: Orange = malignant, Blue = benign')\n",
    "print('  ‚Ä¢ You can follow any prediction path from root to leaf!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f'Random Forest Performance:')\n",
    "print(f'  Training Accuracy: {accuracy_score(y_train, rf.predict(X_train)):.4f}')\n",
    "print(f'  Test Accuracy:     {accuracy_score(y_test, rf.predict(X_test)):.4f}')\n",
    "\n",
    "# Mean Decrease in Impurity (MDI) - built-in feature importance\n",
    "mdi_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'MDI_Importance': rf.feature_importances_\n",
    "}).sort_values('MDI_Importance', ascending=False)\n",
    "\n",
    "print(f'\\nüìä Feature Importance (MDI - Mean Decrease in Impurity):')\n",
    "print(mdi_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Permutation importance (more reliable)\n",
    "perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Perm_Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "}).sort_values('Perm_Importance', ascending=False)\n",
    "\n",
    "print(f'\\nüìä Permutation Importance (more reliable for correlated features):')\n",
    "print(perm_importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MDI vs Permutation Importance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# MDI Importance\n",
    "top_mdi = mdi_importance.head(15)\n",
    "ax1.barh(range(len(top_mdi)), top_mdi['MDI_Importance'], alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.set_yticks(range(len(top_mdi)))\n",
    "ax1.set_yticklabels(top_mdi['Feature'])\n",
    "ax1.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('MDI Feature Importance\\n(Mean Decrease in Impurity)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Permutation Importance\n",
    "top_perm = perm_importance_df.head(15)\n",
    "ax2.barh(range(len(top_perm)), top_perm['Perm_Importance'], \n",
    "         xerr=top_perm['Std'], alpha=0.7, color='coral', edgecolor='black')\n",
    "ax2.set_yticks(range(len(top_perm)))\n",
    "ax2.set_yticklabels(top_perm['Feature'])\n",
    "ax2.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Permutation Feature Importance\\n(More robust to correlated features)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n‚öñÔ∏è MDI vs Permutation Importance:')\n",
    "print('  MDI (left): Fast, built-in, but biased toward high-cardinality features')\n",
    "print('  Permutation (right): Slower, more reliable, especially with correlated features')\n",
    "print('  ‚úÖ Best practice: Use permutation importance for production explanations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-agnostic\"></a>\n",
    "## 3. Model-Agnostic Methods\n",
    "\n",
    "These methods work with ANY model - linear models, tree ensembles, neural networks, anything. This is critical because production systems often use complex ensemble models or neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP values are based on game theory (Shapley values). They answer:\n",
    "**\"How much does each feature contribute to moving the prediction away from the baseline (average) prediction?\"**\n",
    "\n",
    "**Key properties**:\n",
    "- **Consistency**: If a feature contributes more, its SHAP value increases\n",
    "- **Local accuracy**: Feature contributions sum to prediction - baseline\n",
    "- **Missingness**: Features not used have zero SHAP value\n",
    "\n",
    "**Industry standard in 2025**: SHAP is THE most widely used explainability method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for random forest\n",
    "print('Creating SHAP explainer (this may take a minute)...')\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "\n",
    "# Calculate SHAP values for test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print('‚úÖ SHAP values computed!')\n",
    "print(f'   Shape: {shap_values[1].shape}')\n",
    "print(f'   Interpretation: SHAP value for each feature, for each test sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot - Global Importance\n",
    "print('\\nüìä SHAP Summary Plot (Global Feature Importance)\\n')\n",
    "\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° How to Read This Plot:')\n",
    "print('  ‚Ä¢ Features sorted by importance (top = most important)')\n",
    "print('  ‚Ä¢ Each dot = one sample')\n",
    "print('  ‚Ä¢ X-axis: SHAP value (impact on prediction)')\n",
    "print('  ‚Ä¢ Color: Feature value (red = high, blue = low)')\n",
    "print('  ‚Ä¢ Example: High \"worst concave points\" strongly increases cancer probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Force Plot - Individual Prediction Explanation\n",
    "# Let's explain a specific prediction\n",
    "sample_idx = 0\n",
    "sample = X_test[sample_idx]\n",
    "actual_class = 'Benign' if y_test[sample_idx] == 1 else 'Malignant'\n",
    "predicted_prob = rf.predict_proba(sample.reshape(1, -1))[0, 1]\n",
    "\n",
    "print(f'\\nüìã Explaining Individual Prediction (Sample {sample_idx}):')\n",
    "print(f'   Actual class: {actual_class}')\n",
    "print(f'   Predicted probability of Benign: {predicted_prob:.3f}')\n",
    "print(f'\\n   SHAP Force Plot shows which features pushed prediction higher or lower:\\n')\n",
    "\n",
    "# Force plot\n",
    "shap.force_plot(explainer.expected_value[1], \n",
    "                shap_values[1][sample_idx], \n",
    "                sample, \n",
    "                feature_names=feature_names,\n",
    "                matplotlib=True,\n",
    "                show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° Interpretation:')\n",
    "print('  ‚Ä¢ Base value: Average model prediction')\n",
    "print('  ‚Ä¢ Red features: Push prediction toward Benign (higher)')\n",
    "print('  ‚Ä¢ Blue features: Push prediction toward Malignant (lower)')\n",
    "print('  ‚Ä¢ Final prediction: Sum of all contributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Waterfall Plot - Even clearer individual explanation\n",
    "print(f'\\nüìä SHAP Waterfall Plot (Sample {sample_idx}):\\n')\n",
    "\n",
    "shap.plots.waterfall(shap.Explanation(values=shap_values[1][sample_idx],\n",
    "                                       base_values=explainer.expected_value[1],\n",
    "                                       data=sample,\n",
    "                                       feature_names=feature_names),\n",
    "                     show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° This is the BEST plot for explaining individual predictions to stakeholders!')\n",
    "print('   Shows step-by-step how each feature contributes to final prediction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "LIME explains individual predictions by:\n",
    "1. Creating perturbations (variations) of the instance\n",
    "2. Getting model predictions for perturbations\n",
    "3. Fitting a simple linear model locally\n",
    "4. Using linear model coefficients as explanations\n",
    "\n",
    "**When to use**:\n",
    "- Works with any model (even neural networks)\n",
    "- Good for text and image data\n",
    "- Fast for individual predictions\n",
    "\n",
    "**Limitation**: Can be unstable (different runs give different explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LIME explainer\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['Malignant', 'Benign'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Explain the same sample we used for SHAP\n",
    "lime_exp = lime_explainer.explain_instance(\n",
    "    data_row=sample,\n",
    "    predict_fn=rf.predict_proba,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print(f'\\nüìä LIME Explanation (Sample {sample_idx}):\\n')\n",
    "lime_exp.show_in_notebook(show_table=True)\n",
    "\n",
    "print('\\nüí° LIME Explanation Components:')\n",
    "print('  ‚Ä¢ Prediction probabilities: Model outputs')\n",
    "print('  ‚Ä¢ Feature contributions: How each feature pushes prediction')\n",
    "print('  ‚Ä¢ Value ranges: Actual feature values for this instance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP vs LIME Comparison\n",
    "\n",
    "| Aspect | SHAP | LIME |\n",
    "|--------|------|------|\n",
    "| **Theoretical foundation** | Game theory (Shapley values) | Local linear approximation |\n",
    "| **Consistency** | Guaranteed | Not guaranteed |\n",
    "| **Stability** | Stable (same input ‚Üí same output) | Can vary between runs |\n",
    "| **Speed** | Slower for complex models | Faster |\n",
    "| **Global explanations** | Yes (summary plots) | No (local only) |\n",
    "| **Industry adoption** | Very high (standard in 2025) | Moderate |\n",
    "\n",
    "**Recommendation**: Use SHAP for production systems. Use LIME for quick prototyping or when SHAP is too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"pdp\"></a>\n",
    "## 4. Partial Dependence Plots (PDP)\n",
    "\n",
    "PDPs show the marginal effect of a feature on predictions, averaging over all other features.\n",
    "\n",
    "**Question answered**: \"How does the prediction change as this feature changes, on average?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plots for top features\n",
    "print('\\nüìä Partial Dependence Plots (showing average effect of features)\\n')\n",
    "\n",
    "# Select top 4 features from SHAP\n",
    "top_feature_indices = [20, 27, 22, 23]  # worst concave points, worst radius, worst area, worst smoothness\n",
    "top_feature_names_pdp = [feature_names[i] for i in top_feature_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf,\n",
    "    X_train,\n",
    "    features=top_feature_indices,\n",
    "    feature_names=feature_names,\n",
    "    grid_resolution=50,\n",
    "    ax=ax\n",
    ")\n",
    "plt.suptitle('Partial Dependence Plots - Top 4 Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° How to Read PDPs:')\n",
    "print('  ‚Ä¢ X-axis: Feature value')\n",
    "print('  ‚Ä¢ Y-axis: Average predicted probability')\n",
    "print('  ‚Ä¢ Slope: How prediction changes with feature')\n",
    "print('  ‚Ä¢ Example: Higher \"worst concave points\" ‚Üí higher cancer probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ice\"></a>\n",
    "## 5. Individual Conditional Expectation (ICE)\n",
    "\n",
    "ICE plots show how predictions change for individual instances as a feature varies. Unlike PDPs which average, ICE shows individual trajectories.\n",
    "\n",
    "**Useful for**: Detecting interactions and heterogeneous effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICE plots for a single feature\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "print('\\nüìä ICE Plot (Individual Conditional Expectation)\\n')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf,\n",
    "    X_test[:50],  # Use subset for clarity\n",
    "    features=[20],  # worst concave points\n",
    "    kind='both',  # Show both PDP and ICE\n",
    "    feature_names=feature_names,\n",
    "    ax=ax\n",
    ")\n",
    "plt.suptitle('ICE Plot: worst concave points\\n(Yellow = PDP average, Blue = individual instances)',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° ICE vs PDP:')\n",
    "print('  ‚Ä¢ Blue lines: How prediction changes for each individual instance')\n",
    "print('  ‚Ä¢ Yellow line: Average (PDP)')\n",
    "print('  ‚Ä¢ Parallel lines: Feature effect is consistent across instances')\n",
    "print('  ‚Ä¢ Diverging lines: Feature interactions present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"global-local\"></a>\n",
    "## 6. Global vs Local Explanations\n",
    "\n",
    "### Global Explanations\n",
    "Describe overall model behavior:\n",
    "- Feature importance (MDI, permutation, SHAP)\n",
    "- Partial Dependence Plots\n",
    "- Model coefficients (linear models)\n",
    "\n",
    "**Use when**: Understanding general model behavior, debugging, feature selection\n",
    "\n",
    "### Local Explanations\n",
    "Explain individual predictions:\n",
    "- SHAP force plots / waterfall plots\n",
    "- LIME explanations\n",
    "- Individual feature contributions\n",
    "\n",
    "**Use when**: Explaining specific decisions, regulatory compliance, building trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"best-practices\"></a>\n",
    "## 7. Best Practices for Production Interpretability\n",
    "\n",
    "### ‚úÖ DO:\n",
    "\n",
    "1. **Use multiple methods**: SHAP + permutation importance + domain knowledge\n",
    "2. **Validate explanations**: Do they match domain expertise?\n",
    "3. **Document**: Save explanations with predictions for audit trail\n",
    "4. **Sanity check**: Remove top feature, does performance drop?\n",
    "5. **Communicate clearly**: Match explanation complexity to audience\n",
    "6. **Monitor**: Track feature importance over time, detect drift\n",
    "\n",
    "### ‚ùå DON'T:\n",
    "\n",
    "1. **Trust single method**: Different methods can disagree\n",
    "2. **Ignore domain expertise**: ML finds correlations, not causation\n",
    "3. **Over-interpret**: Feature importance ‚â† causation\n",
    "4. **Forget data quality**: Garbage in = garbage explanations out\n",
    "5. **Assume stability**: Explanations can change with retraining\n",
    "\n",
    "### Production Checklist:\n",
    "\n",
    "```python\n",
    "# Recommended production explanation pipeline\n",
    "def explain_prediction(model, instance, X_train, feature_names):\n",
    "    \"\"\"Complete explanation for production.\"\"\"\n",
    "    \n",
    "    # 1. Get prediction\n",
    "    prediction = model.predict_proba(instance.reshape(1, -1))\n",
    "    \n",
    "    # 2. SHAP values (most reliable)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(instance)\n",
    "    \n",
    "    # 3. Feature contributions\n",
    "    contributions = dict(zip(feature_names, shap_values[1]))\n",
    "    top_features = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "    \n",
    "    # 4. Return structured explanation\n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'top_features': top_features,\n",
    "        'shap_values': shap_values,\n",
    "        'timestamp': datetime.now(),\n",
    "        'model_version': model_version\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"real-world\"></a>\n",
    "## 8. Real-World Application: Explaining to Stakeholders\n",
    "\n",
    "Different audiences need different explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_to_doctor(sample_idx):\n",
    "    \"\"\"Explanation for medical professional.\"\"\"\n",
    "    sample = X_test[sample_idx]\n",
    "    prediction = rf.predict_proba(sample.reshape(1, -1))[0]\n",
    "    shap_values_sample = explainer.shap_values(sample.reshape(1, -1))[1][0]\n",
    "    \n",
    "    # Get top contributing features\n",
    "    feature_contributions = list(zip(feature_names, shap_values_sample, sample))\n",
    "    feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print('='*70)\n",
    "    print('CLINICAL DECISION SUPPORT - BREAST CANCER PREDICTION')\n",
    "    print('='*70)\n",
    "    print(f'\\nüè• Model Prediction:')\n",
    "    print(f'   Probability of BENIGN: {prediction[1]:.1%}')\n",
    "    print(f'   Probability of MALIGNANT: {prediction[0]:.1%}')\n",
    "    print(f'   Classification: {\"BENIGN\" if prediction[1] > 0.5 else \"MALIGNANT\"}')\n",
    "    \n",
    "    print(f'\\nüìä Top 5 Contributing Factors:')\n",
    "    for i, (feature, contribution, value) in enumerate(feature_contributions[:5], 1):\n",
    "        direction = \"increases\" if contribution > 0 else \"decreases\"\n",
    "        print(f'   {i}. {feature}')\n",
    "        print(f'      Value: {value:.4f}')\n",
    "        print(f'      This {direction} benign probability by {abs(contribution):.3f}')\n",
    "        print()\n",
    "    \n",
    "    print('‚ö†Ô∏è  IMPORTANT:')\n",
    "    print('   This is a decision support tool, not a diagnosis.')\n",
    "    print('   Final diagnosis must be made by qualified medical professional.')\n",
    "    print('   Consider patient history, additional tests, and clinical judgment.')\n",
    "    print('='*70)\n",
    "\n",
    "# Example\n",
    "explain_to_doctor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_to_executive():\n",
    "    \"\"\"High-level explanation for business stakeholders.\"\"\"\n",
    "    \n",
    "    # Global feature importance\n",
    "    shap_importance = np.abs(shap_values[1]).mean(axis=0)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': shap_importance\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    print('='*70)\n",
    "    print('EXECUTIVE SUMMARY - BREAST CANCER PREDICTION MODEL')\n",
    "    print('='*70)\n",
    "    print(f'\\nüìà Model Performance:')\n",
    "    print(f'   Accuracy on test set: {accuracy_score(y_test, rf.predict(X_test)):.1%}')\n",
    "    print(f'   This means we correctly classify {accuracy_score(y_test, rf.predict(X_test)):.1%} of cases')\n",
    "    \n",
    "    print(f'\\nüéØ Top 10 Most Important Diagnostic Features:')\n",
    "    for i, row in importance_df.iterrows():\n",
    "        print(f'   {row[\"Feature\"]}: {row[\"Importance\"]:.4f}')\n",
    "    \n",
    "    print(f'\\nüíº Business Impact:')\n",
    "    print(f'   ‚Ä¢ Assists doctors in early cancer detection')\n",
    "    print(f'   ‚Ä¢ Focuses attention on most predictive measurements')\n",
    "    print(f'   ‚Ä¢ Explainable predictions build clinician trust')\n",
    "    print(f'   ‚Ä¢ Compliant with medical AI regulations')\n",
    "    \n",
    "    print(f'\\n‚öñÔ∏è  Risk Management:')\n",
    "    print(f'   ‚Ä¢ All predictions must be reviewed by physicians')\n",
    "    print(f'   ‚Ä¢ Model explanations provided for audit trail')\n",
    "    print(f'   ‚Ä¢ Regular monitoring for performance degradation')\n",
    "    print('='*70)\n",
    "\n",
    "explain_to_executive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Interpretability in 2025\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Interpretability is mandatory**: Regulatory requirements (EU AI Act, GDPR) make it non-optional\n",
    "\n",
    "2. **Use appropriate methods**:\n",
    "   - Simple models: Use model-specific methods (coefficients, tree structure)\n",
    "   - Complex models: Use SHAP (industry standard)\n",
    "   - Quick prototyping: Use LIME or permutation importance\n",
    "\n",
    "3. **Global + Local**:\n",
    "   - Global explanations: Feature importance, PDPs (understand overall behavior)\n",
    "   - Local explanations: SHAP force plots, LIME (explain specific predictions)\n",
    "\n",
    "4. **Validate explanations**:\n",
    "   - Do they match domain expertise?\n",
    "   - Are they stable across similar instances?\n",
    "   - Can stakeholders understand them?\n",
    "\n",
    "5. **Tailor to audience**:\n",
    "   - Doctors: Clinical language, individual predictions\n",
    "   - Executives: Business impact, aggregate metrics\n",
    "   - Regulators: Compliance documentation, audit trails\n",
    "\n",
    "### Production Recommendations:\n",
    "\n",
    "```python\n",
    "# Minimum viable interpretability for production\n",
    "1. SHAP values for all predictions (save to database)\n",
    "2. Feature importance monitoring (track over time)\n",
    "3. Individual explanation API (for stakeholders)\n",
    "4. Regular validation against domain expertise\n",
    "5. Documentation of model decisions (audit trail)\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Practice**: Apply these methods to your own models\n",
    "- **Learn more**: Read SHAP documentation, interpret neural networks\n",
    "- **Build trust**: Use explanations to communicate with stakeholders\n",
    "- **Stay updated**: Interpretability research is rapidly evolving\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **SHAP**: https://github.com/slundberg/shap\n",
    "- **LIME**: https://github.com/marcotcr/lime\n",
    "- **Interpretable ML Book**: https://christophm.github.io/interpretable-ml-book/\n",
    "- **EU AI Act**: Official guidelines on AI explainability\n",
    "\n",
    "**Remember**: Accuracy without interpretability is increasingly unacceptable in production ML. Master these techniques to build trustworthy, deployable systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
