{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 0A: Linear Regression Theory",
    "",
    "<a name=\"introduction\"></a>",
    "## Introduction",
    "",
    "Linear regression is the foundation of machine learning - the algorithm you should learn first, before logistic regression, decision trees, or neural networks.",
    "",
    "Think about predicting house prices. You know intuitively that bigger houses cost more. If a 1,000 sq ft house costs $200,000, a 2,000 sq ft house probably costs around $400,000. You're drawing a mental straight line through the data points.",
    "",
    "That's linear regression - finding the best straight line (or hyperplane in higher dimensions) that predicts an output from inputs. It's simple, interpretable, and forms the basis for understanding more complex algorithms.",
    "",
    "In this lesson, we'll:",
    "1. Understand what linear regression is and when to use it",
    "2. Learn the mathematical foundations (least squares, gradients)",
    "3. Implement simple and multiple linear regression from scratch",
    "4. Explore the closed-form solution (Normal Equation)",
    "5. Implement gradient descent optimization",
    "6. Apply it to real housing price prediction",
    "",
    "Then in Lesson 0B, we'll:",
    "1. Use Scikit-learn and PyTorch for production implementations",
    "2. Handle polynomial features and feature engineering",
    "3. Add regularization (Ridge, Lasso) to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents",
    "",
    "1. [Introduction](#introduction)",
    "2. [Required libraries](#required-libraries)",
    "3. [What is linear regression?](#what-is-linear-regression)",
    "4. [Simple linear regression](#simple-linear-regression)",
    "   - [The equation](#the-equation)",
    "   - [Finding the best line](#finding-the-best-line)",
    "   - [Worked example](#worked-example)",
    "5. [Multiple linear regression](#multiple-linear-regression)",
    "6. [The cost function](#the-cost-function)",
    "7. [Optimization methods](#optimization-methods)",
    "   - [Normal Equation (closed-form)](#normal-equation)",
    "   - [Gradient descent](#gradient-descent)",
    "8. [Implementation from scratch](#implementation-from-scratch)",
    "9. [California housing dataset](#california-housing-dataset)",
    "10. [Model evaluation](#model-evaluation)",
    "11. [Assumptions of linear regression](#assumptions)",
    "12. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"required-libraries\"></a>",
    "## Required libraries",
    "",
    "<table style=\"margin-left:0\">",
    "<tr><th align=\"left\">Library</th><th align=\"left\">Purpose</th></tr>",
    "<tr><td>Numpy</td><td>Numerical computing</td></tr>",
    "<tr><td>Pandas</td><td>Data manipulation</td></tr>",
    "<tr><td>Matplotlib/Seaborn</td><td>Visualization</td></tr>",
    "<tr><td>Scikit-learn</td><td>Datasets and metrics</td></tr>",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "from sklearn.datasets import fetch_california_housing",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error",
    "from typing import Tuple",
    "from numpy.typing import NDArray",
    "",
    "np.random.seed(42)",
    "plt.style.use('seaborn-v0_8-darkgrid')",
    "%matplotlib inline",
    "",
    "print(\"‚úÖ Libraries loaded!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"what-is-linear-regression\"></a>",
    "## What is linear regression?",
    "",
    "Linear regression models the relationship between:",
    "- **Independent variables** (features, predictors): X",
    "- **Dependent variable** (target, outcome): y",
    "",
    "Using a **linear function**:",
    "",
    "### Simple (1 feature): $y = mx + b$",
    "### Multiple (n features): $y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$",
    "",
    "Or in matrix form: $y = Xw + b$",
    "",
    "**Goal:** Find weights (w) and bias (b) that minimize prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"simple-linear-regression\"></a>",
    "## Simple linear regression",
    "",
    "Let's start with one feature: predicting house price from square footage.",
    "",
    "**Example data:**",
    "- 600 sq ft ‚Üí $150k",
    "- 1000 sq ft ‚Üí $250k",
    "- 1400 sq ft ‚Üí $350k",
    "- 1800 sq ft ‚Üí $450k",
    "",
    "**Find:** Best line y = mx + b"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example data",
    "sqft = np.array([600, 1000, 1400, 1800])",
    "price = np.array([150, 250, 350, 450])  # in thousands",
    "",
    "# Calculate best fit line (using numpy for now)",
    "m, b = np.polyfit(sqft, price, 1)",
    "print(f\"Best fit line: price = {m:.2f} * sqft + {b:.2f}\")",
    "print(f\"Interpretation: Each sq ft adds ${m:.2f}k to price, base price is ${b:.2f}k\")",
    "",
    "# Visualize",
    "plt.figure(figsize=(10, 6))",
    "plt.scatter(sqft, price, s=100, alpha=0.7, label='Actual data')",
    "plt.plot(sqft, m * sqft + b, 'r-', linewidth=2, label=f'y = {m:.2f}x + {b:.2f}')",
    "plt.xlabel('Square Feet', fontsize=12)",
    "plt.ylabel('Price ($1000s)', fontsize=12)",
    "plt.title('Simple Linear Regression: House Price vs Size', fontsize=14, fontweight='bold')",
    "plt.legend()",
    "plt.grid(True, alpha=0.3)",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"the-cost-function\"></a>",
    "## The cost function",
    "",
    "How do we measure how \"good\" a line is?",
    "",
    "**Mean Squared Error (MSE):**",
    "",
    "### $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$",
    "",
    "Where:",
    "- $y_i$ is the actual value",
    "- $\\hat{y}_i$ is the predicted value",
    "- We square the errors to penalize large mistakes more",
    "",
    "**Goal:** Minimize MSE by finding optimal w and b"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_mse(y_true: NDArray, y_pred: NDArray) -> float:",
    "    \"\"\"Compute Mean Squared Error.\"\"\"",
    "    return np.mean((y_true - y_pred) ** 2)",
    "",
    "def compute_rmse(y_true: NDArray, y_pred: NDArray) -> float:",
    "    \"\"\"Compute Root Mean Squared Error.\"\"\"",
    "    return np.sqrt(compute_mse(y_true, y_pred))",
    "",
    "def compute_r2(y_true: NDArray, y_pred: NDArray) -> float:",
    "    \"\"\"Compute R¬≤ score (coefficient of determination).\"\"\"",
    "    ss_res = np.sum((y_true - y_pred) ** 2)",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)",
    "    return 1 - (ss_res / ss_tot)",
    "",
    "# Test with our example",
    "y_pred = m * sqft + b",
    "print(f\"MSE:  {compute_mse(price, y_pred):.2f}\")",
    "print(f\"RMSE: {compute_rmse(price, y_pred):.2f} (in $1000s)\")",
    "print(f\"R¬≤:   {compute_r2(price, y_pred):.4f} (1.0 = perfect fit)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Visualizing the Cost Function Surface\n\nTo truly understand optimization, we need to see what the cost function looks like. For simple linear regression with one feature, we have two parameters (slope $m$ and intercept $b$), so we can visualize the cost function as a 3D surface where the height represents the MSE for different parameter values.\n\nThe shape of this surface tells us everything about how easy or hard it is to find the optimal parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a grid of parameter values to evaluate\nm_vals = np.linspace(-50, 300, 100)\nb_vals = np.linspace(-200, 400, 100)\nM_grid, B_grid = np.meshgrid(m_vals, b_vals)\n\n# Compute cost for each parameter combination\ncosts = np.zeros_like(M_grid)\nfor i in range(len(m_vals)):\n    for j in range(len(b_vals)):\n        predictions = M_grid[j, i] * sqft + B_grid[j, i]\n        costs[j, i] = compute_mse(price, predictions)\n\n# Find optimal parameters (we already know these from earlier)\noptimal_m, optimal_b = m, b\noptimal_cost = compute_mse(price, optimal_m * sqft + optimal_b)\n\n# Create figure with 3D surface and contour plots\nfig = plt.figure(figsize=(18, 6))\n\n# 3D Surface Plot\nax1 = fig.add_subplot(131, projection='3d')\nsurf = ax1.plot_surface(M_grid, B_grid, costs, cmap='viridis', alpha=0.8, edgecolor='none')\nax1.scatter([optimal_m], [optimal_b], [optimal_cost], \n            color='red', s=200, marker='*', edgecolors='black', linewidths=2,\n            label='Global Minimum')\nax1.set_xlabel('Slope (m)', fontsize=11, fontweight='bold')\nax1.set_ylabel('Intercept (b)', fontsize=11, fontweight='bold')\nax1.set_zlabel('Cost (MSE)', fontsize=11, fontweight='bold')\nax1.set_title('3D Cost Function Surface\\n(Convex Bowl Shape)', fontsize=13, fontweight='bold', pad=20)\nax1.view_init(elev=25, azim=45)\nfig.colorbar(surf, ax=ax1, shrink=0.5, label='MSE')\nax1.legend(fontsize=10)\n\n# 2D Contour Plot\nax2 = fig.add_subplot(132)\nlevels = 20\ncontour = ax2.contour(M_grid, B_grid, costs, levels=levels, cmap='viridis', linewidths=1.5)\nax2.clabel(contour, inline=True, fontsize=8, fmt='%0.0f')\nax2.plot(optimal_m, optimal_b, 'r*', markersize=20, label='Optimal Œ∏', \n         markeredgecolor='black', markeredgewidth=2)\nax2.set_xlabel('Slope (m)', fontsize=11, fontweight='bold')\nax2.set_ylabel('Intercept (b)', fontsize=11, fontweight='bold')\nax2.set_title('Cost Function Contours\\n(Each line = same cost)', fontsize=13, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\n# Cross-section through optimal point\nax3 = fig.add_subplot(133)\n# Slice through optimal b, vary m\nm_slice = np.linspace(-50, 300, 200)\ncosts_m = [compute_mse(price, m_val * sqft + optimal_b) for m_val in m_slice]\nax3.plot(m_slice, costs_m, linewidth=3, label='Varying slope (m), fixed intercept (b)', color='blue')\nax3.plot(optimal_m, optimal_cost, 'r*', markersize=20, label='Optimal m', \n         markeredgecolor='black', markeredgewidth=2)\nax3.set_xlabel('Slope (m)', fontsize=11, fontweight='bold')\nax3.set_ylabel('Cost (MSE)', fontsize=11, fontweight='bold')\nax3.set_title('Cost Function Cross-Section\\n(Shows convexity)', fontsize=13, fontweight='bold')\nax3.legend(fontsize=10)\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\n' + '='*70)\nprint('KEY INSIGHTS FROM THE COST FUNCTION SURFACE')\nprint('='*70)\nprint(f'\\nüéØ Optimal Parameters:')\nprint(f'   Slope (m):     {optimal_m:.2f}')\nprint(f'   Intercept (b): {optimal_b:.2f}')\nprint(f'   Minimum Cost:  {optimal_cost:.2f}')\n\nprint(f'\\nüìä Surface Properties:')\nprint(f'   ‚úÖ CONVEX shape (single global minimum, no local minima)')\nprint(f'   ‚úÖ Smooth everywhere (no sharp edges or discontinuities)')\nprint(f'   ‚úÖ Bowl-shaped (any path downhill leads to the minimum)')\n\nprint(f'\\nüß≠ What This Means for Optimization:')\nprint(f'   ‚Ä¢ Gradient descent is GUARANTEED to find the optimal solution')\nprint(f'   ‚Ä¢ Starting from ANY point, following the gradient downhill works')\nprint(f'   ‚Ä¢ No risk of getting stuck in local minima')\nprint(f'   ‚Ä¢ This is why linear regression is mathematically beautiful!')\n\nprint(f'\\nüí° Real-World Insight:')\nprint(f'   Most machine learning problems are NOT this easy!')\nprint(f'   Neural networks have complex, non-convex cost surfaces with')\nprint(f'   many local minima. Linear regression is special because it\\'s convex.')\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"optimization-methods\"></a>",
    "## Optimization methods",
    "",
    "Two ways to find optimal weights:",
    "",
    "<a name=\"normal-equation\"></a>",
    "### 1. Normal Equation (closed-form solution)",
    "",
    "For linear regression, there's a direct mathematical formula:",
    "",
    "### $w = (X^TX)^{-1}X^Ty$",
    "",
    "**Pros:** Exact solution, no iterations",
    "**Cons:** Slow for large datasets (matrix inversion is O(n¬≥))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ‚ö†Ô∏è Important Note on Numerical Stability\n\nThe mathematical formula for the Normal Equation is:\n\n$$\\theta = (X^TX)^{-1}X^Ty$$\n\nHowever, **explicitly computing the matrix inverse can be numerically unstable** when:\n- The matrix $X^TX$ is nearly singular (close to non-invertible)\n- Features are highly correlated or redundant\n- The condition number of the matrix is large\n\n**Best Practice:** Use `np.linalg.lstsq()` instead of computing the inverse explicitly. This function uses more robust numerical algorithms like QR decomposition or Singular Value Decomposition (SVD) that handle poorly conditioned matrices gracefully.\n\n**Think of it like this:** Both approaches give the same answer mathematically, but `lstsq` uses a more careful numerical path that avoids accumulating rounding errors.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class LinearRegressionNormal:\n    \"\"\"Linear Regression using Normal Equation.\"\"\"\n    \n    def __init__(self):\n        self.weights = None\n        self.bias = None\n    \n    def fit(self, X: NDArray, y: NDArray):\n        \"\"\"Fit using normal equation with numerically stable least squares.\"\"\"\n        # Add bias term (column of 1s)\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n        \n        # Normal equation using least squares (numerically stable)\n        # We use lstsq instead of explicitly inverting (X^T X) because:\n        # 1. Matrix inversion can be numerically unstable when X^T X is nearly singular\n        # 2. lstsq uses more robust algorithms (QR decomposition or SVD)\n        # 3. Produces accurate results even with poorly conditioned matrices\n        theta = np.linalg.lstsq(X_b, y, rcond=None)[0]\n        \n        self.bias = theta[0]\n        self.weights = theta[1:]\n    \n    def predict(self, X: NDArray) -> NDArray:\n        \"\"\"Make predictions.\"\"\"\n        return X @ self.weights + self.bias\n\n# Test on simple example\nmodel_normal = LinearRegressionNormal()\nmodel_normal.fit(sqft.reshape(-1, 1), price)\n\nprint(f\"Weights: {model_normal.weights[0]:.2f}\")\nprint(f\"Bias: {model_normal.bias:.2f}\")\nprint(\"‚úÖ Normal equation implementation complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"gradient-descent\"></a>",
    "### 2. Gradient Descent",
    "",
    "Iteratively improve weights by following the gradient:",
    "",
    "### $w := w - \\alpha \\frac{\\partial MSE}{\\partial w}$",
    "",
    "Where Œ± is the learning rate.",
    "",
    "**Gradients:**",
    "- $\\frac{\\partial MSE}{\\partial w} = -\\frac{2}{n}X^T(y - \\hat{y})$",
    "- $\\frac{\\partial MSE}{\\partial b} = -\\frac{2}{n}\\sum(y - \\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LinearRegressionGD:",
    "    \"\"\"Linear Regression using Gradient Descent.\"\"\"",
    "",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):",
    "        self.lr = learning_rate",
    "        self.n_iters = n_iterations",
    "        self.weights = None",
    "        self.bias = None",
    "        self.losses = []",
    "",
    "    def fit(self, X: NDArray, y: NDArray):",
    "        \"\"\"Fit using gradient descent.\"\"\"",
    "        n_samples, n_features = X.shape",
    "",
    "        # Initialize",
    "        self.weights = np.zeros(n_features)",
    "        self.bias = 0",
    "",
    "        # Gradient descent",
    "        for i in range(self.n_iters):",
    "            # Predictions",
    "            y_pred = X @ self.weights + self.bias",
    "",
    "            # Compute gradients",
    "            dw = -(2 / n_samples) * (X.T @ (y - y_pred))",
    "            db = -(2 / n_samples) * np.sum(y - y_pred)",
    "",
    "            # Update parameters",
    "            self.weights -= self.lr * dw",
    "            self.bias -= self.lr * db",
    "",
    "            # Track loss",
    "            if i % 100 == 0:",
    "                loss = compute_mse(y, y_pred)",
    "                self.losses.append(loss)",
    "",
    "    def predict(self, X: NDArray) -> NDArray:",
    "        return X @ self.weights + self.bias",
    "",
    "# Test gradient descent",
    "model_gd = LinearRegressionGD(learning_rate=0.0001, n_iterations=1000)",
    "model_gd.fit(sqft.reshape(-1, 1), price)",
    "print(f\"Weights: {model_gd.weights[0]:.2f}\")",
    "print(f\"Bias: {model_gd.bias:.2f}\")",
    "",
    "# Plot loss curve",
    "plt.figure(figsize=(10, 5))",
    "plt.plot(model_gd.losses, linewidth=2)",
    "plt.xlabel('Iteration (√ó100)', fontsize=12)",
    "plt.ylabel('MSE Loss', fontsize=12)",
    "plt.title('Gradient Descent Convergence', fontsize=14, fontweight='bold')",
    "plt.grid(True, alpha=0.3)",
    "plt.show()",
    "print(\"‚úÖ Gradient descent implementation complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"california-housing-dataset\"></a>",
    "## California housing dataset",
    "",
    "Now let's apply our implementation to real data with multiple features!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load data",
    "housing = fetch_california_housing()",
    "X, y = housing.data, housing.target",
    "",
    "print(f\"Dataset shape: {X.shape}\")",
    "print(f\"Features: {housing.feature_names}\")",
    "print(f\"\\nTarget: Median house value in $100k\")",
    "print(f\"Target range: ${y.min():.1f}k - ${y.max():.1f}k\")",
    "",
    "# Split data",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "# Normalize features (important for gradient descent!)",
    "X_mean = X_train.mean(axis=0)",
    "X_std = X_train.std(axis=0)",
    "X_train_norm = (X_train - X_mean) / X_std",
    "X_test_norm = (X_test - X_mean) / X_std",
    "",
    "print(f\"\\nTraining samples: {len(X_train):,}\")",
    "print(f\"Test samples: {len(X_test):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train both models",
    "print(\"Training models...\\n\")",
    "",
    "# Normal equation",
    "model_normal = LinearRegressionNormal()",
    "model_normal.fit(X_train_norm, y_train)",
    "y_pred_normal = model_normal.predict(X_test_norm)",
    "",
    "# Gradient descent",
    "model_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=2000)",
    "model_gd.fit(X_train_norm, y_train)",
    "y_pred_gd = model_gd.predict(X_test_norm)",
    "",
    "# Evaluate",
    "print(\"Normal Equation:\")",
    "print(f\"  MSE:  {compute_mse(y_test, y_pred_normal):.4f}\")",
    "print(f\"  RMSE: {compute_rmse(y_test, y_pred_normal):.4f}\")",
    "print(f\"  R¬≤:   {compute_r2(y_test, y_pred_normal):.4f}\")",
    "",
    "print(\"\\nGradient Descent:\")",
    "print(f\"  MSE:  {compute_mse(y_test, y_pred_gd):.4f}\")",
    "print(f\"  RMSE: {compute_rmse(y_test, y_pred_gd):.4f}\")",
    "print(f\"  R¬≤:   {compute_r2(y_test, y_pred_gd):.4f}\")",
    "",
    "print(\"\\n‚úÖ Both methods produce similar results!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>",
    "## Conclusion",
    "",
    "**What we learned:**",
    "",
    "1. Linear regression finds the best linear relationship between features and target",
    "2. MSE measures prediction quality",
    "3. Normal Equation: Direct solution, fast for small datasets",
    "4. Gradient Descent: Iterative solution, scales to large datasets",
    "5. Feature normalization is crucial for gradient descent",
    "",
    "**When to use linear regression:**",
    "- ‚úÖ Relationship is approximately linear",
    "- ‚úÖ You need interpretable coefficients",
    "- ‚úÖ Fast predictions required",
    "- ‚ùå Complex non-linear patterns (use trees, neural networks)",
    "",
    "**Next: Lesson 0B** - Production implementations with Scikit-learn, polynomial features, and regularization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}