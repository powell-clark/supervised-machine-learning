{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4A: Support Vector Machines - Theory\n",
    "\n",
    "**Status**: ðŸš§ Under Development - Target: 1,200+ lines\n",
    "\n",
    "**Current Progress**: â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 10% (189/1,200 lines)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "Imagine you're a radiologist examining tumor biopsies. Each patient's biopsy shows two key measurements: tumor size and cell density. When you plot these measurements, a pattern emerges.\n",
    "\n",
    "Some tumors cluster clearly in the \"benign\" regionâ€”small size, low cell density, regular cell structure. Others cluster unmistakably in the \"malignant\" territoryâ€”large, dense, with irregular aggressive growth patterns.\n",
    "\n",
    "But between these clusters lies a gray zone. Borderline cases where one wrong decision could mean unnecessary surgery for a healthy patientâ€”or worse, undetected cancer allowed to progress.\n",
    "\n",
    "You need more than just *any* line separating the two groups. You need the **safest possible boundary**â€”one that stays as far away from borderline cases as possible. A boundary with the widest \"confidence margin\" on both sides, giving you the maximum safety buffer for your life-or-death diagnosis.\n",
    "\n",
    "This intuitionâ€”finding the classification boundary with maximum margin from both classesâ€”is exactly what **Support Vector Machines (SVMs)** do mathematically.\n",
    "\n",
    "### Why This Algorithm Changed Machine Learning\n",
    "\n",
    "In the 1990s, SVMs revolutionized machine learning by solving two fundamental problems:\n",
    "\n",
    "1. **The Margin Problem**: Unlike logistic regression which just finds *any* separating boundary, SVMs find the *optimal* boundary with maximum safety margin\n",
    "2. **The Non-Linear Problem**: Through the \"kernel trick,\" SVMs can find complex curved boundaries while solving a convex optimization problem\n",
    "\n",
    "Before deep learning dominated in the 2010s, SVMs were the gold standard for:\n",
    "- Text classification (spam detection, sentiment analysis)\n",
    "- Image recognition (face detection, handwriting recognition)\n",
    "- Bioinformatics (protein classification, gene expression analysis)\n",
    "- Financial prediction (credit scoring, stock market analysis)\n",
    "\n",
    "Even today, for datasets with:\n",
    "- High dimensions (thousands of features)\n",
    "- Clear margins between classes\n",
    "- Limited training data\n",
    "\n",
    "...SVMs often outperform more complex models.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this lesson, we'll build SVM understanding from first principles:\n",
    "\n",
    "**Theory & Mathematics:**\n",
    "1. Geometric intuition: What is a \"margin\" and why maximize it?\n",
    "2. Primal formulation: The optimization problem\n",
    "3. Lagrangian duality: Why the dual problem is easier\n",
    "4. KKT conditions: Understanding support vectors\n",
    "5. The kernel trick: Non-linear classification without computing high-dimensional features\n",
    "6. Soft margins: Handling noisy, overlapping data\n",
    "\n",
    "**Implementation:**\n",
    "1. Build SVM from scratch using quadratic programming\n",
    "2. Implement multiple kernel functions (linear, polynomial, RBF)\n",
    "3. Visualize decision boundaries and support vectors\n",
    "4. Compare with logistic regression and decision trees\n",
    "\n",
    "**Real-World Application:**\n",
    "1. Apply to Wisconsin Breast Cancer dataset (same as Lesson 1)\n",
    "2. Compare kernel performance\n",
    "3. Analyze support vectors\n",
    "4. Understand when SVM excels vs when to use alternatives\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You should be comfortable with:\n",
    "- Linear algebra: dot products, norms, matrix multiplication\n",
    "- Calculus: partial derivatives, gradients, Lagrange multipliers (we'll review!)\n",
    "- Python: NumPy array operations\n",
    "- Lessons 0-1: Linear regression and logistic regression\n",
    "\n",
    "**Don't worry if you're rusty on Lagrange multipliers** â€” we'll derive everything step-by-step with geometric intuitions.\n",
    "\n",
    "### Then in Lesson 4B...\n",
    "\n",
    "We'll explore production SVM implementations:\n",
    "1. Scikit-learn's optimized SVM with multiple backends\n",
    "2. Hyperparameter tuning (C, gamma, kernel selection)\n",
    "3. Multi-class classification strategies\n",
    "4. Handling imbalanced datasets\n",
    "5. Scaling to large datasets\n",
    "6. Production deployment patterns\n",
    "\n",
    "Let's find the optimal boundary! ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Required Libraries](#required-libraries)\n",
    "3. [The Margin Concept](#the-margin-concept)\n",
    "   - [Geometric Intuition](#geometric-intuition)\n",
    "   - [Mathematical Definition](#mathematical-definition)\n",
    "   - [Why Maximize the Margin?](#why-maximize-margin)\n",
    "4. [Primal Formulation](#primal-formulation)\n",
    "   - [Hard Margin SVM](#hard-margin-svm)\n",
    "   - [Convex Optimization](#convex-optimization)\n",
    "   - [Worked Example: 2D Case](#worked-example-2d)\n",
    "5. [Lagrangian Dual Formulation](#lagrangian-dual)\n",
    "   - [Why Go to the Dual?](#why-dual)\n",
    "   - [KKT Conditions](#kkt-conditions)\n",
    "   - [Support Vectors Emerge](#support-vectors)\n",
    "6. [The Kernel Trick](#kernel-trick)\n",
    "   - [Non-Linear Classification](#non-linear-classification)\n",
    "   - [Common Kernels](#common-kernels)\n",
    "   - [Infinite Dimensions Without Computing Them](#infinite-dimensions)\n",
    "7. [Soft Margin SVM](#soft-margin)\n",
    "   - [Handling Overlapping Classes](#overlapping-classes)\n",
    "   - [The C Parameter](#c-parameter)\n",
    "   - [Bias-Variance Trade-off](#bias-variance)\n",
    "8. [Implementation from Scratch](#implementation)\n",
    "   - [SVMFromScratch Class](#svm-class)\n",
    "   - [Quadratic Programming Solver](#qp-solver)\n",
    "   - [Testing on Toy Data](#testing)\n",
    "9. [Real-World Application](#application)\n",
    "   - [Wisconsin Breast Cancer Dataset](#breast-cancer)\n",
    "   - [Kernel Comparison](#kernel-comparison)\n",
    "   - [Hyperparameter Sensitivity](#hyperparameter-sensitivity)\n",
    "   - [Support Vector Analysis](#sv-analysis)\n",
    "10. [When to Use SVM](#when-to-use)\n",
    "    - [Ideal Use Cases](#ideal-cases)\n",
    "    - [When to Avoid](#when-to-avoid)\n",
    "    - [Comparison with Other Algorithms](#comparison)\n",
    "11. [Conclusion](#conclusion)\n",
    "    - [Key Takeaways](#key-takeaways)\n",
    "    - [Preview of Lesson 4B](#preview-4b)\n",
    "    - [Further Reading](#further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"required-libraries\"></a>\n",
    "## Required Libraries\n",
    "\n",
    "Before we get started, let's load the necessary libraries.\n",
    "\n",
    "<table style=\"margin-left:0\">\n",
    "<tr>\n",
    "<th align=\"left\">Library</th>\n",
    "<th align=\"left\">Purpose</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NumPy</td>\n",
    "<td>Numerical computing and matrix operations for SVM math</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Pandas</td>\n",
    "<td>Data manipulation and analysis</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Matplotlib</td>\n",
    "<td>Visualization (decision boundaries, margins, support vectors)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Seaborn</td>\n",
    "<td>Statistical visualizations</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Scikit-learn</td>\n",
    "<td>Datasets, preprocessing, metrics, and comparison with sklearn SVM</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SciPy</td>\n",
    "<td>Optimization (quadratic programming for dual problem)</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from typing import Tuple, Optional, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… All libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš§ DEVELOPMENT NOTES\n",
    "\n",
    "**This is a starter template for Lesson 4a development.**\n",
    "\n",
    "**Next sections to add** (use CONTENT_RESTORATION_PLAN.md as guide):\n",
    "\n",
    "1. âœ… Introduction (200 lines) - DONE\n",
    "2. âœ… Table of Contents - DONE\n",
    "3. âœ… Required Libraries - DONE\n",
    "4. ðŸš§ The Margin Concept (150 lines) - ADD NEXT\n",
    "5. ðŸš§ Primal Formulation (200 lines)\n",
    "6. ðŸš§ Lagrangian Dual (250 lines)\n",
    "7. ðŸš§ Kernel Trick (200 lines)\n",
    "8. ðŸš§ Soft Margin (150 lines)\n",
    "9. ðŸš§ Implementation (400 lines)\n",
    "10. ðŸš§ Application (500 lines)\n",
    "11. ðŸš§ When to Use (200 lines)\n",
    "12. ðŸš§ Conclusion (100 lines)\n",
    "\n",
    "**Reference**: See CONTENT_RESTORATION_PLAN.md Section \"Lesson 4a (SVM Theory) - Detailed Restoration Plan\"\n",
    "\n",
    "**Quality Check**: Use LESSON_QUALITY_CHECKLIST.md while developing\n",
    "\n",
    "**Target**: 1,200+ lines total (currently at ~200)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
