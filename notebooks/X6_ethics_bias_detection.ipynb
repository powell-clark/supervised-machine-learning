{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X6: Ethics & Bias Detection - Building Fair and Responsible AI\n",
    "\n",
    "Machine learning models can perpetuate and amplify societal biases, leading to unfair outcomes that harm individuals and communities. In 2025, building ethical AI is not optional‚Äîit is a legal, moral, and business imperative.\n",
    "\n",
    "This notebook teaches you how to detect, measure, and mitigate bias in machine learning systems, preparing you to build fair and responsible AI for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ethics and Fairness Matter\n",
    "\n",
    "### Real-World Failures\n",
    "\n",
    "**COMPAS Recidivism Algorithm (2016)**\n",
    "- Used to predict criminal reoffending\n",
    "- Found to be biased against Black defendants\n",
    "- False positive rate for Black defendants was nearly double that of white defendants\n",
    "- Impact: Biased decisions affected real people's freedom\n",
    "\n",
    "**Amazon Hiring Algorithm (2018)**\n",
    "- Trained on historical hiring data (mostly men)\n",
    "- Learned to penalize resumes containing words like \"women's\"\n",
    "- Discriminated against female candidates\n",
    "- Amazon scrapped the system\n",
    "\n",
    "**Facial Recognition Systems**\n",
    "- Higher error rates for people of color, especially women\n",
    "- Led to wrongful arrests\n",
    "- Many cities banned police use of facial recognition\n",
    "\n",
    "**Google Photos (2015)**\n",
    "- Tagged Black people as \"gorillas\"\n",
    "- Massive reputational damage\n",
    "- Revealed training data bias\n",
    "\n",
    "### Legal and Regulatory Requirements (2025)\n",
    "\n",
    "**EU AI Act**\n",
    "- High-risk AI systems must undergo bias testing\n",
    "- Fines up to ‚Ç¨30 million or 6% of global revenue\n",
    "- Mandatory fairness assessments\n",
    "\n",
    "**US Equal Employment Opportunity Laws**\n",
    "- Algorithmic hiring tools must not discriminate\n",
    "- Employers liable for biased algorithms\n",
    "\n",
    "**Fair Housing Act, Fair Lending Laws**\n",
    "- ML models for housing, credit must be demonstrably fair\n",
    "- Regular audits required\n",
    "\n",
    "### Business Case\n",
    "\n",
    "- **Avoid lawsuits**: Discrimination lawsuits are expensive\n",
    "- **Reputation**: Biased AI causes lasting brand damage\n",
    "- **Market access**: Unfair models excluded from regulated markets\n",
    "- **Better decisions**: Fair models often perform better\n",
    "- **Talent**: Engineers increasingly refuse to work on unethical AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Bias in ML](#understanding-bias)\n",
    "2. [Protected Attributes and Fairness](#protected-attributes)\n",
    "3. [Fairness Metrics](#fairness-metrics)\n",
    "   - Demographic Parity\n",
    "   - Equalized Odds\n",
    "   - Equal Opportunity\n",
    "   - Predictive Parity\n",
    "4. [Detecting Bias in Data](#detecting-bias-data)\n",
    "5. [Detecting Bias in Models](#detecting-bias-models)\n",
    "6. [Bias Mitigation Strategies](#mitigation)\n",
    "   - Pre-processing (data)\n",
    "   - In-processing (training)\n",
    "   - Post-processing (predictions)\n",
    "7. [Real-World Case Study](#case-study)\n",
    "8. [Best Practices and Frameworks](#best-practices)\n",
    "9. [Ethical Decision-Making](#ethical-decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fairness libraries\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install fairlearn for fairness metrics and mitigation\n",
    "try:\n",
    "    import fairlearn\n",
    "    print(f'‚úÖ fairlearn version {fairlearn.__version__} found')\n",
    "except ImportError:\n",
    "    print('Installing fairlearn...')\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fairlearn\"])\n",
    "    import fairlearn\n",
    "    print(f'‚úÖ fairlearn version {fairlearn.__version__} installed')\n",
    "\n",
    "# Install aif360 for additional bias detection and mitigation\n",
    "try:\n",
    "    import aif360\n",
    "    print(f'‚úÖ aif360 found')\n",
    "except ImportError:\n",
    "    print('Installing aif360...')\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"aif360\"])\n",
    "    import aif360\n",
    "    print(f'‚úÖ aif360 installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Fairness libraries\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    demographic_parity_difference,\n",
    "    demographic_parity_ratio,\n",
    "    equalized_odds_difference,\n",
    "    equalized_odds_ratio,\n",
    "    selection_rate\n",
    ")\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ All libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"understanding-bias\"></a>\n",
    "## 1. Understanding Bias in ML\n",
    "\n",
    "### Sources of Bias\n",
    "\n",
    "**Historical Bias**\n",
    "- Data reflects historical discrimination\n",
    "- Example: Hiring data shows mostly men in leadership ‚Üí model learns to prefer men\n",
    "- **Not fixable by better algorithms alone** - requires societal awareness\n",
    "\n",
    "**Representation Bias**\n",
    "- Training data doesn't represent all groups equally\n",
    "- Example: Facial recognition trained mostly on white faces ‚Üí fails on other races\n",
    "- **Fix**: Collect diverse, representative data\n",
    "\n",
    "**Measurement Bias**\n",
    "- Features measured differently for different groups\n",
    "- Example: Credit scores systematically lower for certain neighborhoods\n",
    "- **Fix**: Audit measurement processes, use alternative features\n",
    "\n",
    "**Aggregation Bias**\n",
    "- One model for all groups when groups behave differently\n",
    "- Example: Medical diagnostic model trained on adults applied to children\n",
    "- **Fix**: Consider group-specific models or features\n",
    "\n",
    "**Evaluation Bias**\n",
    "- Test data doesn't represent deployment population\n",
    "- Example: Testing fraud detection only on one demographic\n",
    "- **Fix**: Stratified evaluation across all groups\n",
    "\n",
    "### Types of Harm\n",
    "\n",
    "**Allocation Harm**\n",
    "- System allocates resources or opportunities unfairly\n",
    "- Examples: Loan denials, job rejections, healthcare access\n",
    "\n",
    "**Quality-of-Service Harm**\n",
    "- System works better for some groups than others\n",
    "- Examples: Speech recognition, facial recognition, translation\n",
    "\n",
    "**Stereotyping Harm**\n",
    "- System reinforces negative stereotypes\n",
    "- Examples: Gender-biased word associations, racist image tagging\n",
    "\n",
    "**Denigration Harm**\n",
    "- System actively insults or demeans groups\n",
    "- Examples: Offensive auto-completions, hate speech generation\n",
    "\n",
    "**Representation Harm**\n",
    "- System over- or under-represents certain groups\n",
    "- Examples: Search results, recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"protected-attributes\"></a>\n",
    "## 2. Protected Attributes and Fairness\n",
    "\n",
    "### Protected Attributes (Sensitive Features)\n",
    "\n",
    "Attributes that should not be used to discriminate:\n",
    "- **Race / Ethnicity**\n",
    "- **Gender / Sex**\n",
    "- **Age**\n",
    "- **Religion**\n",
    "- **Disability status**\n",
    "- **Sexual orientation**\n",
    "- **National origin**\n",
    "\n",
    "### The \"Fairness Through Unawareness\" Myth\n",
    "\n",
    "**Myth**: \"If we don't include race/gender in the model, it will be fair.\"\n",
    "\n",
    "**Reality**: This doesn't work because:\n",
    "1. **Proxy variables**: Other features correlate with protected attributes\n",
    "   - ZIP code correlates with race\n",
    "   - Name correlates with gender and ethnicity\n",
    "   - Alma mater correlates with socioeconomic status\n",
    "\n",
    "2. **Historical bias in labels**: Even if you exclude protected attributes, historical discrimination is baked into the labels\n",
    "\n",
    "**Correct approach**: \n",
    "- Measure fairness metrics using protected attributes\n",
    "- Don't necessarily exclude them from training (depends on context)\n",
    "- Apply bias mitigation techniques\n",
    "- Regular fairness audits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Dataset for Demonstration\n",
    "\n",
    "We'll create a synthetic loan approval dataset that exhibits realistic bias patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic biased dataset for loan approval\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "# Protected attribute: gender\n",
    "gender = np.random.choice(['Male', 'Female'], size=n_samples, p=[0.55, 0.45])\n",
    "\n",
    "# Features that correlate with loan approval\n",
    "# Income (with gender bias - women systematically paid less historically)\n",
    "income_male = np.random.normal(75000, 25000, size=(gender == 'Male').sum())\n",
    "income_female = np.random.normal(65000, 22000, size=(gender == 'Female').sum())\n",
    "income = np.concatenate([income_male[gender == 'Male'], income_female[gender == 'Female']])\n",
    "income = np.maximum(income, 20000)  # Minimum income\n",
    "\n",
    "# Credit score (with slight gender bias)\n",
    "credit_score_male = np.random.normal(720, 60, size=(gender == 'Male').sum())\n",
    "credit_score_female = np.random.normal(710, 65, size=(gender == 'Female').sum())\n",
    "credit_score = np.concatenate([credit_score_male[gender == 'Male'], \n",
    "                               credit_score_female[gender == 'Female']])\n",
    "credit_score = np.clip(credit_score, 300, 850)\n",
    "\n",
    "# Loan amount requested\n",
    "loan_amount = np.random.normal(250000, 100000, size=n_samples)\n",
    "loan_amount = np.maximum(loan_amount, 50000)\n",
    "\n",
    "# Years employed\n",
    "years_employed = np.random.exponential(5, size=n_samples)\n",
    "years_employed = np.clip(years_employed, 0, 40)\n",
    "\n",
    "# Debt-to-income ratio\n",
    "debt_to_income = np.random.beta(2, 5, size=n_samples) * 0.6\n",
    "\n",
    "# Loan approval (with historical bias)\n",
    "# Base probability from legitimate factors\n",
    "base_prob = (\n",
    "    0.3 * (credit_score - 300) / 550 +\n",
    "    0.25 * (income - 20000) / 180000 +\n",
    "    0.15 * (1 - debt_to_income / 0.6) +\n",
    "    0.15 * np.minimum(years_employed / 10, 1) +\n",
    "    0.15 * (1 - (loan_amount - 50000) / 500000)\n",
    ")\n",
    "\n",
    "# Add gender bias (historical discrimination)\n",
    "# Women have systematically lower approval rates even with same qualifications\n",
    "gender_bias = np.where(gender == 'Male', 0.1, -0.1)\n",
    "approval_prob = np.clip(base_prob + gender_bias, 0, 1)\n",
    "\n",
    "# Generate approvals\n",
    "approved = (np.random.random(n_samples) < approval_prob).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'gender': gender,\n",
    "    'income': income,\n",
    "    'credit_score': credit_score,\n",
    "    'loan_amount': loan_amount,\n",
    "    'years_employed': years_employed,\n",
    "    'debt_to_income': debt_to_income,\n",
    "    'approved': approved\n",
    "})\n",
    "\n",
    "print('Synthetic Loan Approval Dataset Created')\n",
    "print(f'Total samples: {len(df):,}')\n",
    "print(f'\\nGender distribution:')\n",
    "print(df['gender'].value_counts())\n",
    "print(f'\\nOverall approval rate: {df[\"approved\"].mean():.2%}')\n",
    "print(f'\\nApproval rate by gender:')\n",
    "print(df.groupby('gender')['approved'].mean())\n",
    "print(f'\\n‚ö†Ô∏è Notice the approval rate difference - this is the bias we need to detect and address!')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias in data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Income distribution by gender\n",
    "df.boxplot(column='income', by='gender', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Income Distribution by Gender')\n",
    "axes[0, 0].set_ylabel('Income ($)')\n",
    "\n",
    "# Credit score by gender\n",
    "df.boxplot(column='credit_score', by='gender', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Credit Score by Gender')\n",
    "axes[0, 1].set_ylabel('Credit Score')\n",
    "\n",
    "# Approval rate by gender\n",
    "approval_by_gender = df.groupby('gender')['approved'].mean()\n",
    "axes[0, 2].bar(approval_by_gender.index, approval_by_gender.values, \n",
    "               color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_title('Approval Rate by Gender', fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Approval Rate')\n",
    "axes[0, 2].set_ylim([0, 1])\n",
    "for i, v in enumerate(approval_by_gender.values):\n",
    "    axes[0, 2].text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Income vs Credit Score colored by approval\n",
    "for gender_val in ['Male', 'Female']:\n",
    "    gender_data = df[df['gender'] == gender_val]\n",
    "    axes[1, 0].scatter(gender_data['income'], gender_data['credit_score'],\n",
    "                      c=gender_data['approved'], cmap='RdYlGn',\n",
    "                      alpha=0.5, label=gender_val, s=20)\n",
    "axes[1, 0].set_xlabel('Income ($)')\n",
    "axes[1, 0].set_ylabel('Credit Score')\n",
    "axes[1, 0].set_title('Income vs Credit Score\\n(Green=Approved, Red=Denied)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Approval rate by income quintile and gender\n",
    "df['income_quintile'] = pd.qcut(df['income'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "approval_by_quintile = df.groupby(['income_quintile', 'gender'])['approved'].mean().unstack()\n",
    "approval_by_quintile.plot(kind='bar', ax=axes[1, 1], color=['steelblue', 'coral'], alpha=0.7)\n",
    "axes[1, 1].set_title('Approval Rate by Income Quintile and Gender')\n",
    "axes[1, 1].set_xlabel('Income Quintile')\n",
    "axes[1, 1].set_ylabel('Approval Rate')\n",
    "axes[1, 1].legend(title='Gender')\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Approval rate by credit score range and gender\n",
    "df['credit_range'] = pd.cut(df['credit_score'], bins=[300, 600, 650, 700, 750, 850],\n",
    "                            labels=['<600', '600-650', '650-700', '700-750', '750+'])\n",
    "approval_by_credit = df.groupby(['credit_range', 'gender'])['approved'].mean().unstack()\n",
    "approval_by_credit.plot(kind='bar', ax=axes[1, 2], color=['steelblue', 'coral'], alpha=0.7)\n",
    "axes[1, 2].set_title('Approval Rate by Credit Score and Gender')\n",
    "axes[1, 2].set_xlabel('Credit Score Range')\n",
    "axes[1, 2].set_ylabel('Approval Rate')\n",
    "axes[1, 2].legend(title='Gender')\n",
    "axes[1, 2].set_xticklabels(axes[1, 2].get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüîç Key Observations:')\n",
    "print('  ‚Ä¢ Women have systematically lower approval rates across ALL income and credit levels')\n",
    "print('  ‚Ä¢ This suggests bias beyond just income/credit differences')\n",
    "print('  ‚Ä¢ A model trained on this data will learn and perpetuate this bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fairness-metrics\"></a>\n",
    "## 3. Fairness Metrics\n",
    "\n",
    "### The Impossibility Theorem\n",
    "\n",
    "**Important**: You generally **cannot** satisfy all fairness metrics simultaneously (except in trivial cases). You must choose which definition of fairness is most appropriate for your context.\n",
    "\n",
    "### Key Fairness Metrics\n",
    "\n",
    "#### 1. Demographic Parity (Statistical Parity)\n",
    "**Definition**: Positive prediction rate should be the same across groups\n",
    "\n",
    "$P(\\hat{Y}=1 | A=a) = P(\\hat{Y}=1 | A=b)$ for all groups $a, b$\n",
    "\n",
    "**When to use**: \n",
    "- When you want equal representation in outcomes\n",
    "- University admissions, job screening (first round)\n",
    "\n",
    "**Limitation**: Ignores differences in qualification/merit\n",
    "\n",
    "#### 2. Equalized Odds\n",
    "**Definition**: True positive rate AND false positive rate should be equal across groups\n",
    "\n",
    "$P(\\hat{Y}=1 | Y=y, A=a) = P(\\hat{Y}=1 | Y=y, A=b)$ for $y \\in \\{0,1\\}$ and all groups $a, b$\n",
    "\n",
    "**When to use**:\n",
    "- When both types of errors matter equally\n",
    "- Healthcare diagnostics, loan approval\n",
    "\n",
    "**Interpretation**: Model makes same mistakes across all groups\n",
    "\n",
    "#### 3. Equal Opportunity\n",
    "**Definition**: True positive rate should be equal across groups (relaxed equalized odds)\n",
    "\n",
    "$P(\\hat{Y}=1 | Y=1, A=a) = P(\\hat{Y}=1 | Y=1, A=b)$ for all groups $a, b$\n",
    "\n",
    "**When to use**:\n",
    "- When false negatives are more harmful than false positives\n",
    "- Disease screening, fraud detection\n",
    "\n",
    "#### 4. Predictive Parity (Precision Parity)\n",
    "**Definition**: Precision should be equal across groups\n",
    "\n",
    "$P(Y=1 | \\hat{Y}=1, A=a) = P(Y=1 | \\hat{Y}=1, A=b)$ for all groups $a, b$\n",
    "\n",
    "**When to use**:\n",
    "- When false positives are particularly harmful\n",
    "- Criminal justice, accusatory systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline model (will inherit bias from data)\n",
    "# Create features and target\n",
    "feature_cols = ['income', 'credit_score', 'loan_amount', 'years_employed', 'debt_to_income']\n",
    "X = df[feature_cols]\n",
    "y = df['approved']\n",
    "sensitive_feature = df['gender']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "    X, y, sensitive_feature, test_size=0.2, random_state=42, stratify=sensitive_feature\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f'Baseline Model Accuracy: {baseline_accuracy:.4f}')\n",
    "print(f'\\nOverall Classification Report:')\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Denied', 'Approved']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fairness metrics for baseline model\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "# Create MetricFrame for disaggregated metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'precision': precision_score,\n",
    "    'recall': recall_score,\n",
    "    'selection_rate': selection_rate\n",
    "}\n",
    "\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_baseline,\n",
    "    sensitive_features=sensitive_test\n",
    ")\n",
    "\n",
    "print('üìä Disaggregated Metrics by Gender:\\n')\n",
    "print(metric_frame.by_group)\n",
    "\n",
    "print('\\nüìä Fairness Metric Summary:\\n')\n",
    "print(f'Overall metrics:')\n",
    "print(metric_frame.overall)\n",
    "\n",
    "# Calculate specific fairness metrics\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred_baseline, sensitive_features=sensitive_test)\n",
    "dp_ratio = demographic_parity_ratio(y_test, y_pred_baseline, sensitive_features=sensitive_test)\n",
    "eo_diff = equalized_odds_difference(y_test, y_pred_baseline, sensitive_features=sensitive_test)\n",
    "\n",
    "print(f'\\n‚öñÔ∏è Fairness Metrics:')\n",
    "print(f'  Demographic Parity Difference: {dp_diff:.4f}')\n",
    "print(f'  Demographic Parity Ratio: {dp_ratio:.4f}')\n",
    "print(f'  Equalized Odds Difference: {eo_diff:.4f}')\n",
    "\n",
    "print(f'\\nüéØ Interpretation:')\n",
    "print(f'  ‚Ä¢ Demographic Parity Difference = {abs(dp_diff):.4f}')\n",
    "print(f'    This means selection rates differ by {abs(dp_diff):.1%} between groups')\n",
    "print(f'  ‚Ä¢ Demographic Parity Ratio = {dp_ratio:.4f}')\n",
    "print(f'    Ratio should be close to 1.0 for fairness (0.8-1.2 is often considered acceptable)')\n",
    "print(f'  ‚Ä¢ Equalized Odds Difference = {eo_diff:.4f}')\n",
    "print(f'    Measures difference in error rates across groups')\n",
    "\n",
    "if abs(dp_diff) > 0.1:\n",
    "    print(f'\\n‚ö†Ô∏è WARNING: Significant demographic parity violation detected!')\n",
    "    print(f'   The model approves loans at different rates for different genders.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fairness metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Selection rate by group\n",
    "selection_rates = metric_frame.by_group['selection_rate']\n",
    "axes[0].bar(selection_rates.index, selection_rates.values, \n",
    "            color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Selection Rate (Approval Rate)', fontweight='bold')\n",
    "axes[0].set_title('Selection Rate by Gender\\n(Demographic Parity)', fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].axhline(y=metric_frame.overall['selection_rate'], color='red', \n",
    "                linestyle='--', label='Overall Rate')\n",
    "axes[0].legend()\n",
    "for i, v in enumerate(selection_rates.values):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "# True Positive Rate (Recall) by group\n",
    "tpr = metric_frame.by_group['recall']\n",
    "axes[1].bar(tpr.index, tpr.values, \n",
    "            color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('True Positive Rate (Recall)', fontweight='bold')\n",
    "axes[1].set_title('TPR by Gender\\n(Equal Opportunity)', fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].axhline(y=metric_frame.overall['recall'], color='red', \n",
    "                linestyle='--', label='Overall TPR')\n",
    "axes[1].legend()\n",
    "for i, v in enumerate(tpr.values):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Precision by group\n",
    "precision = metric_frame.by_group['precision']\n",
    "axes[2].bar(precision.index, precision.values, \n",
    "            color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n",
    "axes[2].set_ylabel('Precision', fontweight='bold')\n",
    "axes[2].set_title('Precision by Gender\\n(Predictive Parity)', fontweight='bold')\n",
    "axes[2].set_ylim([0, 1])\n",
    "axes[2].axhline(y=metric_frame.overall['precision'], color='red', \n",
    "                linestyle='--', label='Overall Precision')\n",
    "axes[2].legend()\n",
    "for i, v in enumerate(precision.values):\n",
    "    axes[2].text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüìä What These Plots Show:')\n",
    "print('  Left: Demographic Parity - Are approval rates equal across groups?')\n",
    "print('  Middle: Equal Opportunity - Do qualified applicants have equal approval rates?')\n",
    "print('  Right: Predictive Parity - Is precision (% of approvals that should be approved) equal?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"mitigation\"></a>\n",
    "## 6. Bias Mitigation Strategies\n",
    "\n",
    "### Three Approaches\n",
    "\n",
    "**Pre-processing** (Fix the data)\n",
    "- Reweighting samples\n",
    "- Resampling\n",
    "- Learning fair representations\n",
    "\n",
    "**In-processing** (Fix the algorithm)\n",
    "- Add fairness constraints during training\n",
    "- Adversarial debiasing\n",
    "- Fairness-aware regularization\n",
    "\n",
    "**Post-processing** (Fix the predictions)\n",
    "- Threshold optimization\n",
    "- Calibration\n",
    "- Reject option classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation Strategy 1: In-processing with Fairness Constraints\n",
    "# Using Exponentiated Gradient with Demographic Parity constraint\n",
    "\n",
    "print('Training fair model with Demographic Parity constraint...\\n')\n",
    "\n",
    "# Create base estimator\n",
    "estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Apply fairness constraint (Demographic Parity)\n",
    "mitigator_dp = ExponentiatedGradient(\n",
    "    estimator=estimator,\n",
    "    constraints=DemographicParity(),\n",
    "    max_iter=50\n",
    ")\n",
    "\n",
    "# Fit fair model\n",
    "mitigator_dp.fit(X_train, y_train, sensitive_features=sensitive_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_fair_dp = mitigator_dp.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "fair_dp_accuracy = accuracy_score(y_test, y_pred_fair_dp)\n",
    "print(f'Fair Model (Demographic Parity) Accuracy: {fair_dp_accuracy:.4f}')\n",
    "print(f'Baseline Model Accuracy: {baseline_accuracy:.4f}')\n",
    "print(f'Accuracy change: {fair_dp_accuracy - baseline_accuracy:+.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fairness metrics: Baseline vs Fair Model\n",
    "metric_frame_fair_dp = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_fair_dp,\n",
    "    sensitive_features=sensitive_test\n",
    ")\n",
    "\n",
    "# Calculate fairness metrics for fair model\n",
    "dp_diff_fair = demographic_parity_difference(y_test, y_pred_fair_dp, sensitive_features=sensitive_test)\n",
    "dp_ratio_fair = demographic_parity_ratio(y_test, y_pred_fair_dp, sensitive_features=sensitive_test)\n",
    "eo_diff_fair = equalized_odds_difference(y_test, y_pred_fair_dp, sensitive_features=sensitive_test)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Demographic Parity Diff', 'Demographic Parity Ratio', 'Equalized Odds Diff'],\n",
    "    'Baseline Model': [baseline_accuracy, dp_diff, dp_ratio, eo_diff],\n",
    "    'Fair Model (DP)': [fair_dp_accuracy, dp_diff_fair, dp_ratio_fair, eo_diff_fair],\n",
    "    'Change': [\n",
    "        fair_dp_accuracy - baseline_accuracy,\n",
    "        dp_diff_fair - dp_diff,\n",
    "        dp_ratio_fair - dp_ratio,\n",
    "        eo_diff_fair - eo_diff\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('\\nüìä Baseline vs Fair Model Comparison:\\n')\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print('\\n‚úÖ Improvements:')\n",
    "print(f'  ‚Ä¢ Demographic Parity Difference reduced by {abs(dp_diff - dp_diff_fair):.4f}')\n",
    "print(f'  ‚Ä¢ Demographic Parity Ratio now: {dp_ratio_fair:.4f} (closer to 1.0 = fairer)')\n",
    "print(f'  ‚Ä¢ Small accuracy tradeoff: {baseline_accuracy - fair_dp_accuracy:.4f}')\n",
    "print('\\nüí° This demonstrates the accuracy-fairness tradeoff!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation Strategy 2: Post-processing with Threshold Optimization\n",
    "# Optimize thresholds per group to achieve Equalized Odds\n",
    "\n",
    "print('Training fair model with post-processing (Threshold Optimization)...\\n')\n",
    "\n",
    "# First train regular model and get probability predictions\n",
    "baseline_model_proba = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline_model_proba.fit(X_train, y_train)\n",
    "y_pred_proba = baseline_model_proba.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold optimizer for Equalized Odds\n",
    "threshold_optimizer = ThresholdOptimizer(\n",
    "    estimator=baseline_model_proba,\n",
    "    constraints='equalized_odds',\n",
    "    predict_method='predict_proba'\n",
    ")\n",
    "\n",
    "threshold_optimizer.fit(X_train, y_train, sensitive_features=sensitive_train)\n",
    "y_pred_threshold = threshold_optimizer.predict(X_test, sensitive_features=sensitive_test)\n",
    "\n",
    "# Evaluate\n",
    "threshold_accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "print(f'Threshold-Optimized Model Accuracy: {threshold_accuracy:.4f}')\n",
    "print(f'Baseline Model Accuracy: {baseline_accuracy:.4f}')\n",
    "\n",
    "# Fairness metrics\n",
    "dp_diff_threshold = demographic_parity_difference(y_test, y_pred_threshold, sensitive_features=sensitive_test)\n",
    "eo_diff_threshold = equalized_odds_difference(y_test, y_pred_threshold, sensitive_features=sensitive_test)\n",
    "\n",
    "print(f'\\nFairness Metrics:')\n",
    "print(f'  Demographic Parity Diff: {dp_diff_threshold:.4f} (was {dp_diff:.4f})')\n",
    "print(f'  Equalized Odds Diff: {eo_diff_threshold:.4f} (was {eo_diff:.4f})')\n",
    "print(f'\\n‚úÖ Equalized Odds violation reduced by {abs(eo_diff - eo_diff_threshold):.4f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison visualization\n",
    "models = ['Baseline', 'Fair (DP Constraint)', 'Threshold Optimized']\n",
    "accuracies = [baseline_accuracy, fair_dp_accuracy, threshold_accuracy]\n",
    "dp_diffs = [abs(dp_diff), abs(dp_diff_fair), abs(dp_diff_threshold)]\n",
    "eo_diffs = [abs(eo_diff), abs(eo_diff_fair), abs(eo_diff_threshold)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(models, accuracies, color=['red', 'orange', 'green'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[0].set_title('Model Accuracy Comparison', fontweight='bold', fontsize=13)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Demographic Parity violation\n",
    "axes[1].bar(models, dp_diffs, color=['red', 'orange', 'green'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Demographic Parity Difference (abs)', fontweight='bold')\n",
    "axes[1].set_title('Fairness: Demographic Parity\\n(Lower = Fairer)', fontweight='bold', fontsize=13)\n",
    "axes[1].axhline(y=0.1, color='orange', linestyle='--', label='Acceptable threshold')\n",
    "axes[1].legend()\n",
    "for i, v in enumerate(dp_diffs):\n",
    "    axes[1].text(i, v + 0.005, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Equalized Odds violation\n",
    "axes[2].bar(models, eo_diffs, color=['red', 'orange', 'green'], alpha=0.7, edgecolor='black')\n",
    "axes[2].set_ylabel('Equalized Odds Difference (abs)', fontweight='bold')\n",
    "axes[2].set_title('Fairness: Equalized Odds\\n(Lower = Fairer)', fontweight='bold', fontsize=13)\n",
    "axes[2].axhline(y=0.1, color='orange', linestyle='--', label='Acceptable threshold')\n",
    "axes[2].legend()\n",
    "for i, v in enumerate(eo_diffs):\n",
    "    axes[2].text(i, v + 0.005, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüéØ Key Takeaways:')\n",
    "print('  1. Baseline model is most accurate but LEAST fair')\n",
    "print('  2. Fair models reduce bias significantly with small accuracy cost')\n",
    "print('  3. Different mitigation strategies optimize for different fairness definitions')\n",
    "print('  4. You must choose the fairness metric appropriate for your use case')\n",
    "print('\\n‚ö†Ô∏è The accuracy-fairness tradeoff is REAL but often acceptable')\n",
    "print('   A few percentage points of accuracy is a small price for fairness!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"best-practices\"></a>\n",
    "## 8. Best Practices and Frameworks\n",
    "\n",
    "### Production ML Fairness Checklist\n",
    "\n",
    "**Before Training**:\n",
    "- [ ] Audit data for representation bias (are all groups represented?)\n",
    "- [ ] Examine label distribution across protected groups\n",
    "- [ ] Check for proxy variables (features that correlate with protected attributes)\n",
    "- [ ] Document data collection process and potential bias sources\n",
    "- [ ] Define which fairness metric(s) matter for your use case\n",
    "\n",
    "**During Training**:\n",
    "- [ ] Track metrics separately for each protected group\n",
    "- [ ] Consider fairness constraints if needed\n",
    "- [ ] Document model architecture and hyperparameters\n",
    "- [ ] Save training data statistics and distributions\n",
    "\n",
    "**After Training**:\n",
    "- [ ] Measure all relevant fairness metrics\n",
    "- [ ] Create fairness report card\n",
    "- [ ] Test on held-out data from all groups\n",
    "- [ ] Perform error analysis by subgroup\n",
    "- [ ] Consider post-processing if fairness violations detected\n",
    "\n",
    "**Deployment**:\n",
    "- [ ] Monitor fairness metrics in production\n",
    "- [ ] Set up alerts for fairness metric degradation\n",
    "- [ ] Regular fairness audits (quarterly minimum)\n",
    "- [ ] Document model limitations and known biases\n",
    "- [ ] Provide mechanism for users to appeal decisions\n",
    "\n",
    "### Frameworks and Tools\n",
    "\n",
    "**Fairlearn** (Microsoft)\n",
    "- Fairness metrics and mitigation algorithms\n",
    "- Dashboard for comparing models\n",
    "- Python library, well-documented\n",
    "\n",
    "**AIF360** (IBM)\n",
    "- 70+ fairness metrics\n",
    "- 10+ bias mitigation algorithms\n",
    "- More comprehensive than Fairlearn\n",
    "\n",
    "**What-If Tool** (Google)\n",
    "- Visual interface for exploring model behavior\n",
    "- Slice analysis by subgroups\n",
    "- Counterfactual analysis\n",
    "\n",
    "**Aequitas** (University of Chicago)\n",
    "- Bias and fairness audit toolkit\n",
    "- Focus on criminal justice\n",
    "\n",
    "### Choosing the Right Fairness Metric\n",
    "\n",
    "| Use Case | Primary Fairness Metric | Reasoning |\n",
    "|----------|------------------------|----------|\n",
    "| **Loan Approval** | Equalized Odds | Both false positives and false negatives cause harm |\n",
    "| **Disease Screening** | Equal Opportunity | Missing a disease (false negative) is worst error |\n",
    "| **University Admissions** | Demographic Parity | Want representation from all groups |\n",
    "| **Hiring (initial screen)** | Demographic Parity | Equal access to opportunity |\n",
    "| **Criminal Risk Assessment** | Equalized Odds + Calibration | High stakes, both errors matter |\n",
    "| **Content Moderation** | False Positive Rate Parity | Wrongly censoring speech is harmful |\n",
    "\n",
    "### When Fairness Interventions Don't Work\n",
    "\n",
    "Sometimes technical solutions aren't enough:\n",
    "\n",
    "1. **Problem is upstream**: If data collection is fundamentally flawed, no algorithm fixes it\n",
    "2. **Prediction task itself is harmful**: Maybe you shouldn't build this model at all\n",
    "3. **Societal bias too strong**: Historical discrimination may be impossible to fully correct\n",
    "4. **Wrong problem framing**: You're optimizing the wrong objective\n",
    "\n",
    "**Example**: Recidivism prediction\n",
    "- Even \"fair\" models may perpetuate mass incarceration\n",
    "- The question isn't \"how do we make fair predictions?\"\n",
    "- The question is \"should we be making these predictions at all?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ethical-decisions\"></a>\n",
    "## 9. Ethical Decision-Making Framework\n",
    "\n",
    "### Questions to Ask Before Building Any ML System\n",
    "\n",
    "**1. Should this system exist?**\n",
    "- What problem does it solve?\n",
    "- Who benefits? Who is harmed?\n",
    "- Are there less risky alternatives?\n",
    "\n",
    "**2. Who is affected?**\n",
    "- Who are the stakeholders?\n",
    "- Who has power? Who is vulnerable?\n",
    "- Are affected communities consulted?\n",
    "\n",
    "**3. What are the failure modes?**\n",
    "- How does the system fail?\n",
    "- Who bears the cost of failures?\n",
    "- Can failures be detected and corrected?\n",
    "\n",
    "**4. What are the long-term effects?**\n",
    "- Feedback loops that amplify bias?\n",
    "- Concentration of power?\n",
    "- Societal implications?\n",
    "\n",
    "**5. Is there accountability?**\n",
    "- Who is responsible when things go wrong?\n",
    "- Can decisions be appealed?\n",
    "- Is there transparency?\n",
    "\n",
    "### Red Flags (Don't Build This)\n",
    "\n",
    "üö´ **Target variable is itself biased**\n",
    "- Example: \"Predict who will be a good employee\" when historical hires are biased\n",
    "\n",
    "üö´ **High-stakes decisions on vulnerable populations without human oversight**\n",
    "- Example: Fully automated benefit denials\n",
    "\n",
    "üö´ **Surveillance or control of marginalized groups**\n",
    "- Example: Predictive policing in over-policed neighborhoods\n",
    "\n",
    "üö´ **Impossible to achieve acceptable fairness level**\n",
    "- Example: Using fundamentally biased data with no alternative\n",
    "\n",
    "üö´ **No mechanism for recourse**\n",
    "- Example: Opaque decisions that cannot be appealed\n",
    "\n",
    "### Green Lights (Good Practices)\n",
    "\n",
    "‚úÖ **Augments human decision-making, doesn't replace it**\n",
    "- Human has final say, algorithm provides input\n",
    "\n",
    "‚úÖ **Measurable benefit to affected communities**\n",
    "- Not just efficiency for organization\n",
    "\n",
    "‚úÖ **Robust fairness monitoring and correction**\n",
    "- Regular audits, clear thresholds for intervention\n",
    "\n",
    "‚úÖ **Transparency and explainability**\n",
    "- Affected individuals understand how decisions are made\n",
    "\n",
    "‚úÖ **Participatory design**\n",
    "- Affected communities involved in system design\n",
    "\n",
    "‚úÖ **Clear accountability and recourse**\n",
    "- Someone responsible, decisions can be appealed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Building Fair and Responsible AI\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Bias is inevitable** - All ML systems can perpetuate bias. Your job is to measure and mitigate it.\n",
    "\n",
    "2. **Fairness is not automatic** - Removing protected attributes from features doesn't make models fair.\n",
    "\n",
    "3. **Trade-offs are real** - You typically sacrifice some accuracy for fairness. That's usually okay.\n",
    "\n",
    "4. **No single fairness definition** - Choose the metric that matches your ethical and legal requirements.\n",
    "\n",
    "5. **Measure everything** - Track fairness metrics by subgroup. What you don't measure, you can't fix.\n",
    "\n",
    "6. **Document thoroughly** - Record data sources, model decisions, known limitations.\n",
    "\n",
    "7. **Monitor continuously** - Fairness can degrade over time. Set up production monitoring.\n",
    "\n",
    "8. **Sometimes don't build it** - Some problems are better not solved with ML.\n",
    "\n",
    "### Production Workflow\n",
    "\n",
    "```python\n",
    "# Recommended production fairness workflow\n",
    "def fair_ml_pipeline(X_train, y_train, sensitive_features, fairness_metric):\n",
    "    \"\"\"\n",
    "    Complete fair ML pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Audit data for bias\n",
    "    2. Train baseline model\n",
    "    3. Measure fairness metrics\n",
    "    4. Apply mitigation if needed\n",
    "    5. Generate fairness report\n",
    "    6. Set up monitoring\n",
    "    \"\"\"\n",
    "    # 1. Data audit\n",
    "    audit_data(X_train, y_train, sensitive_features)\n",
    "    \n",
    "    # 2. Train baseline\n",
    "    baseline = train_baseline(X_train, y_train)\n",
    "    \n",
    "    # 3. Measure fairness\n",
    "    metrics = measure_fairness(baseline, X_test, y_test, sensitive_features)\n",
    "    \n",
    "    # 4. Mitigate if needed\n",
    "    if metrics[fairness_metric] > threshold:\n",
    "        fair_model = apply_mitigation(baseline, fairness_metric)\n",
    "    else:\n",
    "        fair_model = baseline\n",
    "    \n",
    "    # 5. Generate report\n",
    "    create_fairness_report(fair_model, metrics)\n",
    "    \n",
    "    # 6. Set up monitoring\n",
    "    setup_fairness_monitoring(fair_model, sensitive_features)\n",
    "    \n",
    "    return fair_model\n",
    "```\n",
    "\n",
    "### Resources for Continued Learning\n",
    "\n",
    "**Books**:\n",
    "- *Weapons of Math Destruction* by Cathy O'Neil\n",
    "- *Race After Technology* by Ruha Benjamin\n",
    "- *Fairness and Machine Learning* by Barocas, Hardt, Narayanan (free online)\n",
    "\n",
    "**Tools**:\n",
    "- Fairlearn: https://fairlearn.org/\n",
    "- AIF360: https://aif360.mybluemix.net/\n",
    "- What-If Tool: https://pair-code.github.io/what-if-tool/\n",
    "\n",
    "**Papers**:\n",
    "- *Fairness Definitions Explained* (Verma & Rubin, 2018)\n",
    "- *A Survey on Bias and Fairness in Machine Learning* (Mehrabi et al., 2021)\n",
    "\n",
    "**Organizations**:\n",
    "- Partnership on AI: https://partnershiponai.org/\n",
    "- AI Now Institute: https://ainowinstitute.org/\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "**Ethics is not optional in 2025.**\n",
    "\n",
    "Between legal requirements (EU AI Act, anti-discrimination laws), reputational risks, and moral imperatives, you MUST build fair AI systems.\n",
    "\n",
    "The techniques in this notebook - fairness metrics, bias detection, mitigation algorithms - are your tools. But tools alone aren't enough. You need:\n",
    "\n",
    "- **Critical thinking**: Question whether the system should exist\n",
    "- **Stakeholder engagement**: Listen to affected communities  \n",
    "- **Humility**: Acknowledge what you don't know\n",
    "- **Courage**: Speak up when you see harmful systems\n",
    "\n",
    "**You are responsible for the systems you build.**\n",
    "\n",
    "Use your skills wisely. Build AI that helps people, not harms them. And when in doubt, ask: \"Would I want this algorithm making decisions about my life?\"\n",
    "\n",
    "If the answer is no, don't build it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
