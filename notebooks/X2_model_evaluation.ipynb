{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X2: Model Evaluation & Selection - Measuring What MattersGuide to evaluating and comparing machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction",
    "",
    "Accuracy alone is misleading. A model that predicts \"no cancer\" for everyone achieves 99% accuracy if only 1% have cancer - but it's useless!",
    "",
    "This guide covers all evaluation metrics, when to use each, and how to properly compare models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents",
    "1. Classification Metrics",
    "2. Regression Metrics",
    "3. Cross-Validation",
    "4. ROC Curves & AUC",
    "5. Precision-Recall Curves",
    "6. Confusion Matrix Deep Dive",
    "7. Statistical Significance Testing",
    "8. Model Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, classification_report",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold",
    "from sklearn.datasets import load_breast_cancer",
    "from sklearn.ensemble import RandomForestClassifier",
    "from sklearn.linear_model import LogisticRegression",
    "from sklearn.model_selection import train_test_split",
    "import seaborn as sns",
    "np.random.seed(42)",
    "print('\u2705 Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification Metrics",
    "",
    "**Accuracy:** Overall correctness",
    "- $\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$",
    "- \u26a0\ufe0f Misleading for imbalanced data",
    "",
    "**Precision:** When you predict positive, how often are you right?",
    "- $\\text{Precision} = \\frac{TP}{TP + FP}$",
    "- Use when: False positives are costly (spam detection)",
    "",
    "**Recall (Sensitivity):** Of all actual positives, how many did you find?",
    "- $\\text{Recall} = \\frac{TP}{TP + FN}$",
    "- Use when: False negatives are costly (cancer detection)",
    "",
    "**F1-Score:** Harmonic mean of precision and recall",
    "- $F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$",
    "- Use when: Need balance",
    "",
    "**ROC-AUC:** Area under ROC curve (0.5 = random, 1.0 = perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and train model",
    "data = load_breast_cancer()",
    "X, y = data.data, data.target",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)",
    "model.fit(X_train, y_train)",
    "y_pred = model.predict(X_test)",
    "y_proba = model.predict_proba(X_test)[:, 1]",
    "",
    "print('Classification Metrics:')",
    "print(f'Accuracy:  {accuracy_score(y_test, y_pred):.3f}')",
    "print(f'Precision: {precision_score(y_test, y_pred):.3f}')",
    "print(f'Recall:    {recall_score(y_test, y_pred):.3f}')",
    "print(f'F1-Score:  {f1_score(y_test, y_pred):.3f}')",
    "print(f'ROC-AUC:   {roc_auc_score(y_test, y_proba):.3f}')",
    "",
    "print('\\nFull Classification Report:')",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation",
    "",
    "**Why?** Single train/test split is noisy - results vary based on split.",
    "",
    "**K-Fold Cross-Validation:**",
    "1. Split data into K folds",
    "2. Train on K-1 folds, test on 1",
    "3. Repeat K times",
    "4. Average results",
    "",
    "**Stratified K-Fold:** Maintains class proportions (use for classification)",
    "",
    "**Time Series Split:** Respects temporal order (never test on past!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross-Validation",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold",
    "",
    "# Standard cross-validation",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')",
    "print(f'5-Fold CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})')",
    "print(f'Individual fold scores: {cv_scores}')",
    "",
    "# Cross-validate multiple metrics",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']",
    "cv_results = cross_validate(model, X, y, cv=5, scoring=scoring)",
    "",
    "print('\\nCross-Validation Results:')",
    "for metric in scoring:",
    "    scores = cv_results[f'test_{metric}']",
    "    print(f'{metric:12s}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROC Curves & AUC",
    "",
    "**ROC Curve:** Plot of True Positive Rate vs False Positive Rate at different thresholds",
    "",
    "**Interpretation:**",
    "- Diagonal line = random classifier (AUC = 0.5)",
    "- Perfect classifier touches top-left corner (AUC = 1.0)",
    "- Higher AUC = better overall performance",
    "",
    "**When to use:**",
    "- \u2705 Balanced classes",
    "- \u2705 Care about ranking",
    "- \u2705 Will tune threshold later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)",
    "auc = roc_auc_score(y_test, y_proba)",
    "",
    "plt.figure(figsize=(10, 6))",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')",
    "plt.xlim([0.0, 1.0])",
    "plt.ylim([0.0, 1.05])",
    "plt.xlabel('False Positive Rate', fontsize=12)",
    "plt.ylabel('True Positive Rate', fontsize=12)",
    "plt.title('ROC Curve', fontsize=14, fontweight='bold')",
    "plt.legend(loc='lower right', fontsize=11)",
    "plt.grid(alpha=0.3)",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Precision-Recall Curves",
    "",
    "**Better than ROC for imbalanced data!**",
    "",
    "**Why?** ROC can be optimistic when negatives >> positives",
    "",
    "**Interpretation:**",
    "- High precision + high recall = excellent",
    "- High precision, low recall = conservative (few predictions)",
    "- Low precision, high recall = aggressive (many predictions)",
    "",
    "**Use when:** Imbalanced classes (fraud, disease detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)",
    "",
    "plt.figure(figsize=(10, 6))",
    "plt.plot(recall, precision, linewidth=2)",
    "plt.xlabel('Recall', fontsize=12)",
    "plt.ylabel('Precision', fontsize=12)",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')",
    "plt.grid(alpha=0.3)",
    "plt.show()",
    "",
    "print(f'Average Precision Score: {precision.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix Deep Dive",
    "",
    "**2\u00d72 Matrix for Binary Classification:**",
    "",
    "|               | Predicted Negative | Predicted Positive |",
    "|---------------|--------------------|-----------------|",
    "| Actual Negative | TN | FP (Type I Error) |",
    "| Actual Positive | FN (Type II Error) | TP |",
    "",
    "**Cost-Sensitive Learning:**",
    "- Medical: FN (missed cancer) >> FP (false alarm)",
    "- Spam: FP (blocking real email) > FN (spam in inbox)",
    "- Set threshold based on costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix",
    "cm = confusion_matrix(y_test, y_pred)",
    "",
    "plt.figure(figsize=(8, 6))",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ",
    "            xticklabels=data.target_names, yticklabels=data.target_names)",
    "plt.ylabel('True Label', fontsize=12)",
    "plt.xlabel('Predicted Label', fontsize=12)",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')",
    "plt.show()",
    "",
    "tn, fp, fn, tp = cm.ravel()",
    "print(f'True Negatives:  {tn}')",
    "print(f'False Positives: {fp} (Type I Error)')",
    "print(f'False Negatives: {fn} (Type II Error)')",
    "print(f'True Positives:  {tp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "**Metric Selection Guide:**",
    "",
    "**Classification:**",
    "- Balanced data, general use: **Accuracy, F1-Score**",
    "- Imbalanced data: **Precision-Recall AUC, F1-Score**",
    "- Ranking important: **ROC-AUC**",
    "- False negatives costly (medical): **Recall**",
    "- False positives costly (spam): **Precision**",
    "",
    "**Always:**",
    "- \u2705 Use cross-validation",
    "- \u2705 Look at confusion matrix",
    "- \u2705 Plot ROC/PR curves",
    "- \u2705 Report confidence intervals",
    "- \u2705 Test on held-out data",
    "",
    "**Never:**",
    "- \u274c Tune on test set",
    "- \u274c Use accuracy alone for imbalanced data",
    "- \u274c Compare without statistical testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}