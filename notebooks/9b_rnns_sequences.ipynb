{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9b: Recurrent Neural Networks & Sequence Processing\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental architecture and mechanics of Recurrent Neural Networks (RNNs)\n",
    "- Learn the limitations of vanilla RNNs and how LSTM and GRU architectures solve them\n",
    "- Master sequence-to-sequence modeling for various tasks\n",
    "- Implement bidirectional RNNs for improved context understanding\n",
    "- Apply RNNs to time series forecasting and natural language processing\n",
    "- Understand when to use RNNs versus Transformers in modern applications\n",
    "\n",
    "**Prerequisites:**\n",
    "- Understanding of basic neural networks (Lesson 3a, 3b)\n",
    "- Familiarity with backpropagation and gradient descent\n",
    "- Basic knowledge of CNNs (Lesson 9a) is helpful but not required\n",
    "\n",
    "**Why RNNs Matter in 2025:**\n",
    "While Transformers dominate many NLP tasks, RNNs remain critical for time series analysis, real-time streaming data, resource-constrained environments, and scenarios requiring sequential memory. Understanding RNNs provides essential foundations for comprehending modern sequence models and helps you choose the right architecture for specific constraints.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "This notebook requires TensorFlow/Keras for implementing RNN architectures. We'll automatically install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip if not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package_name.split('[')[0])\n",
    "        print(f\"✓ {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name])\n",
    "        print(f\"✓ {package_name} installed successfully\")\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    'tensorflow>=2.13.0',\n",
    "    'numpy>=1.24.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'scikit-learn>=1.3.0',\n",
    "    'pandas>=2.0.0',\n",
    "    'seaborn>=0.12.0'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n✓ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. RNN Fundamentals: Understanding Sequential Processing\n",
    "\n",
    "### 2.1 Why Do We Need RNNs?\n",
    "\n",
    "Traditional neural networks (feedforward, CNNs) have a critical limitation: they cannot handle **variable-length sequences** or maintain **temporal dependencies**. They treat each input independently.\n",
    "\n",
    "**Examples where this fails:**\n",
    "- Language: \"The cat sat on the mat\" vs \"The cats sat on the mat\" (different lengths)\n",
    "- Time series: Stock prices where past values influence future predictions\n",
    "- Video: Frame sequences where temporal order matters\n",
    "\n",
    "### 2.2 RNN Core Concept\n",
    "\n",
    "RNNs solve this by maintaining **hidden state** (memory) that gets updated at each time step:\n",
    "\n",
    "```\n",
    "h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
    "y_t = W_hy * h_t + b_y\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `h_t`: Hidden state at time t (memory)\n",
    "- `x_t`: Input at time t\n",
    "- `y_t`: Output at time t\n",
    "- `W_*`: Weight matrices (learned parameters)\n",
    "\n",
    "**Key innovation**: The hidden state `h_t` depends on both the current input `x_t` AND the previous hidden state `h_{t-1}`, creating a memory chain.\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple vanilla RNN from scratch for educational purposes\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"Simple RNN implementation from scratch for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights randomly (small values)\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of input vectors, one per time step\n",
    "        \n",
    "        Returns:\n",
    "            outputs: List of output vectors\n",
    "            hidden_states: List of hidden states (for visualization)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))  # Initial hidden state\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = [h.copy()]\n",
    "        \n",
    "        for x in inputs:\n",
    "            # Reshape input to column vector\n",
    "            x = x.reshape(-1, 1)\n",
    "            \n",
    "            # Update hidden state\n",
    "            # h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
    "            h = np.tanh(np.dot(self.W_hh, h) + np.dot(self.W_xh, x) + self.b_h)\n",
    "            \n",
    "            # Compute output\n",
    "            # y_t = W_hy * h_t + b_y\n",
    "            y = np.dot(self.W_hy, h) + self.b_y\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# Demonstrate RNN on a simple sequence\n",
    "print(\"Demonstrating Vanilla RNN on Simple Sequence:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple RNN\n",
    "rnn = SimpleRNN(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# Create a simple input sequence (4 time steps, 3 features each)\n",
    "input_sequence = [\n",
    "    np.array([1.0, 0.0, 0.0]),\n",
    "    np.array([0.0, 1.0, 0.0]),\n",
    "    np.array([0.0, 0.0, 1.0]),\n",
    "    np.array([1.0, 1.0, 0.0])\n",
    "]\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = rnn.forward(input_sequence)\n",
    "\n",
    "print(f\"Input sequence length: {len(input_sequence)} time steps\")\n",
    "print(f\"Input dimension: {input_sequence[0].shape[0]} features\")\n",
    "print(f\"Hidden state dimension: {rnn.hidden_size}\")\n",
    "print(f\"Output dimension: {outputs[0].shape[0]}\")\n",
    "print(\"\\nKey Insight: The hidden state acts as memory, carrying information\")\n",
    "print(\"from previous time steps to influence current and future predictions.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize hidden state evolution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot hidden states over time\n",
    "hidden_array = np.hstack(hidden_states).T\n",
    "im1 = ax1.imshow(hidden_array, cmap='RdBu', aspect='auto')\n",
    "ax1.set_xlabel('Hidden Units', fontsize=11)\n",
    "ax1.set_ylabel('Time Step', fontsize=11)\n",
    "ax1.set_title('Hidden State Evolution Over Time', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Plot output evolution\n",
    "output_array = np.hstack(outputs).T\n",
    "ax2.plot(output_array[:, 0], marker='o', label='Output Dim 1', linewidth=2)\n",
    "ax2.plot(output_array[:, 1], marker='s', label='Output Dim 2', linewidth=2)\n",
    "ax2.set_xlabel('Time Step', fontsize=11)\n",
    "ax2.set_ylabel('Output Value', fontsize=11)\n",
    "ax2.set_title('RNN Output Over Time', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rnn_hidden_state_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Vanishing Gradient Problem\n",
    "\n",
    "**Critical limitation of vanilla RNNs:**\n",
    "\n",
    "During backpropagation through time (BPTT), gradients are multiplied repeatedly by the weight matrix `W_hh`. This causes:\n",
    "\n",
    "1. **Vanishing gradients**: If eigenvalues of `W_hh` < 1, gradients shrink exponentially → RNN cannot learn long-term dependencies\n",
    "2. **Exploding gradients**: If eigenvalues of `W_hh` > 1, gradients grow exponentially → training becomes unstable\n",
    "\n",
    "**Practical impact**: Vanilla RNNs struggle to remember information beyond 10-20 time steps.\n",
    "\n",
    "**Solution**: LSTM and GRU architectures with gating mechanisms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LSTM: Long Short-Term Memory Networks\n",
    "\n",
    "### 3.1 LSTM Architecture\n",
    "\n",
    "LSTMs solve the vanishing gradient problem through a sophisticated **gating mechanism** that controls information flow:\n",
    "\n",
    "**Three gates:**\n",
    "1. **Forget gate** (`f_t`): Decides what information to discard from cell state\n",
    "2. **Input gate** (`i_t`): Decides what new information to store in cell state\n",
    "3. **Output gate** (`o_t`): Decides what information to output from cell state\n",
    "\n",
    "**Key equations:**\n",
    "```\n",
    "f_t = σ(W_f · [h_{t-1}, x_t] + b_f)      # Forget gate\n",
    "i_t = σ(W_i · [h_{t-1}, x_t] + b_i)      # Input gate\n",
    "C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)  # Candidate values\n",
    "C_t = f_t * C_{t-1} + i_t * C̃_t          # Update cell state\n",
    "o_t = σ(W_o · [h_{t-1}, x_t] + b_o)      # Output gate\n",
    "h_t = o_t * tanh(C_t)                    # Hidden state output\n",
    "```\n",
    "\n",
    "Where `σ` is sigmoid function (outputs 0-1, perfect for gating).\n",
    "\n",
    "**Genius of LSTM**: The cell state `C_t` can flow through time with minimal modification, allowing gradients to flow more easily during backpropagation.\n",
    "\n",
    "Let's implement an LSTM for sequence prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data for demonstration\n",
    "\n",
    "def generate_sine_wave_data(n_samples=1000, sequence_length=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic sine wave data for time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Total number of time points\n",
    "        sequence_length: Length of each input sequence\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences of shape (num_sequences, sequence_length, 1)\n",
    "        y: Target values (next value in sequence)\n",
    "    \"\"\"\n",
    "    # Generate sine wave with some noise\n",
    "    time = np.linspace(0, 100, n_samples)\n",
    "    signal = np.sin(time) + 0.5 * np.sin(3 * time) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(signal) - sequence_length):\n",
    "        X.append(signal[i:i + sequence_length])\n",
    "        y.append(signal[i + sequence_length])\n",
    "    \n",
    "    X = np.array(X).reshape(-1, sequence_length, 1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, signal\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating synthetic time series data...\\n\")\n",
    "X_seq, y_seq, full_signal = generate_sine_wave_data(n_samples=1000, sequence_length=50)\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(0.7 * len(X_seq))\n",
    "val_size = int(0.15 * len(X_seq))\n",
    "\n",
    "X_train = X_seq[:train_size]\n",
    "y_train = y_seq[:train_size]\n",
    "\n",
    "X_val = X_seq[train_size:train_size + val_size]\n",
    "y_val = y_seq[train_size:train_size + val_size]\n",
    "\n",
    "X_test = X_seq[train_size + val_size:]\n",
    "y_test = y_seq[train_size + val_size:]\n",
    "\n",
    "print(f\"Data splits:\")\n",
    "print(f\"Train: {X_train.shape}, Target: {y_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}, Target: {y_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}, Target: {y_test.shape}\")\n",
    "\n",
    "# Visualize the time series\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(full_signal[:300], linewidth=1.5, alpha=0.8)\n",
    "plt.axvline(x=train_size, color='red', linestyle='--', label='Train/Val Split', linewidth=2)\n",
    "plt.axvline(x=train_size + val_size, color='orange', linestyle='--', \n",
    "           label='Val/Test Split', linewidth=2)\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title('Synthetic Time Series Data (First 300 Points)', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_series_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model for time series forecasting\n",
    "\n",
    "def build_lstm_model(sequence_length=50, lstm_units=64, num_layers=2):\n",
    "    \"\"\"\n",
    "    Build LSTM model for time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input sequences\n",
    "        lstm_units: Number of LSTM units per layer\n",
    "        num_layers: Number of stacked LSTM layers\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First LSTM layer (return sequences for stacking)\n",
    "    model.add(layers.LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=(num_layers > 1),\n",
    "        input_shape=(sequence_length, 1),\n",
    "        name='lstm_1'\n",
    "    ))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Additional LSTM layers\n",
    "    for i in range(1, num_layers):\n",
    "        return_seq = (i < num_layers - 1)\n",
    "        model.add(layers.LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=return_seq,\n",
    "            name=f'lstm_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(32, activation='relu', name='dense_1'))\n",
    "    model.add(layers.Dense(1, name='output'))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display model\n",
    "lstm_model = build_lstm_model(sequence_length=50, lstm_units=64, num_layers=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LSTM MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "lstm_model.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "\n",
    "print(\"\\nTraining LSTM model...\\n\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ LSTM training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM performance\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = lstm_model.predict(X_train, verbose=0).flatten()\n",
    "y_pred_val = lstm_model.predict(X_val, verbose=0).flatten()\n",
    "y_pred_test = lstm_model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MSE: {train_mse:.6f}\")\n",
    "print(f\"  MAE: {train_mae:.6f}\")\n",
    "print(f\"  R²:  {train_r2:.6f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MSE: {test_mse:.6f}\")\n",
    "print(f\"  MAE: {test_mae:.6f}\")\n",
    "print(f\"  R²:  {test_r2:.6f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Training history\n",
    "axes[0].plot(history_lstm.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_lstm.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=11)\n",
    "axes[0].set_title('LSTM Training History', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs actual\n",
    "test_range = range(len(y_test))\n",
    "axes[1].plot(test_range, y_test, label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[1].plot(test_range, y_pred_test, label='LSTM Predictions', \n",
    "            linewidth=2, alpha=0.7, linestyle='--')\n",
    "axes[1].set_xlabel('Time Step', fontsize=11)\n",
    "axes[1].set_ylabel('Value', fontsize=11)\n",
    "axes[1].set_title('LSTM Predictions vs Actual (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GRU: Gated Recurrent Units\n",
    "\n",
    "### 4.1 GRU Architecture\n",
    "\n",
    "GRUs are a simpler alternative to LSTMs, introduced in 2014. They combine the forget and input gates into a single **update gate** and merge the cell state and hidden state.\n",
    "\n",
    "**Two gates instead of three:**\n",
    "1. **Update gate** (`z_t`): Decides how much past information to keep\n",
    "2. **Reset gate** (`r_t`): Decides how much past information to forget\n",
    "\n",
    "**Key equations:**\n",
    "```\n",
    "z_t = σ(W_z · [h_{t-1}, x_t])           # Update gate\n",
    "r_t = σ(W_r · [h_{t-1}, x_t])           # Reset gate\n",
    "h̃_t = tanh(W · [r_t * h_{t-1}, x_t])   # Candidate activation\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t  # Final hidden state\n",
    "```\n",
    "\n",
    "**GRU vs LSTM:**\n",
    "- **GRU advantages**: Fewer parameters (~25% less), faster training, works well on smaller datasets\n",
    "- **LSTM advantages**: More expressive, better on complex tasks with large datasets\n",
    "- **In practice (2025)**: Try both! GRU often performs similarly with less computation.\n",
    "\n",
    "Let's compare GRU vs LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GRU model for comparison\n",
    "\n",
    "def build_gru_model(sequence_length=50, gru_units=64, num_layers=2):\n",
    "    \"\"\"\n",
    "    Build GRU model for time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input sequences\n",
    "        gru_units: Number of GRU units per layer\n",
    "        num_layers: Number of stacked GRU layers\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First GRU layer\n",
    "    model.add(layers.GRU(\n",
    "        gru_units,\n",
    "        return_sequences=(num_layers > 1),\n",
    "        input_shape=(sequence_length, 1),\n",
    "        name='gru_1'\n",
    "    ))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Additional GRU layers\n",
    "    for i in range(1, num_layers):\n",
    "        return_seq = (i < num_layers - 1)\n",
    "        model.add(layers.GRU(\n",
    "            gru_units,\n",
    "            return_sequences=return_seq,\n",
    "            name=f'gru_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(32, activation='relu', name='dense_1'))\n",
    "    model.add(layers.Dense(1, name='output'))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build GRU model\n",
    "gru_model = build_gru_model(sequence_length=50, gru_units=64, num_layers=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GRU MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "gru_model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare parameter counts\n",
    "lstm_params = lstm_model.count_params()\n",
    "gru_params = gru_model.count_params()\n",
    "\n",
    "print(f\"\\nParameter Comparison:\")\n",
    "print(f\"LSTM parameters: {lstm_params:,}\")\n",
    "print(f\"GRU parameters:  {gru_params:,}\")\n",
    "print(f\"Reduction:       {((lstm_params - gru_params) / lstm_params * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model\n",
    "\n",
    "print(\"\\nTraining GRU model...\\n\")\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ GRU training completed!\")\n",
    "\n",
    "# Evaluate GRU\n",
    "y_pred_gru = gru_model.predict(X_test, verbose=0).flatten()\n",
    "gru_mse = mean_squared_error(y_test, y_pred_gru)\n",
    "gru_mae = mean_absolute_error(y_test, y_pred_gru)\n",
    "gru_r2 = r2_score(y_test, y_pred_gru)\n",
    "\n",
    "# Compare LSTM vs GRU\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LSTM vs GRU COMPARISON (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<15} {'LSTM':<20} {'GRU':<20}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'MSE':<15} {test_mse:<20.6f} {gru_mse:<20.6f}\")\n",
    "print(f\"{'MAE':<15} {test_mae:<20.6f} {gru_mae:<20.6f}\")\n",
    "print(f\"{'R²':<15} {test_r2:<20.6f} {gru_r2:<20.6f}\")\n",
    "print(f\"{'Parameters':<15} {lstm_params:<20,} {gru_params:<20,}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Bidirectional RNNs: Context from Both Directions\n",
    "\n",
    "### 5.1 The Bidirectional Concept\n",
    "\n",
    "Standard RNNs process sequences in one direction (left-to-right). But for many tasks, **future context is also valuable**:\n",
    "\n",
    "**Example**: \"The animal didn't cross the street because it was too **tired**\"\n",
    "- Understanding \"it\" requires context from BOTH before (\"animal\") and after (\"tired\")\n",
    "\n",
    "**Bidirectional RNNs** solve this by:\n",
    "1. Running one RNN forward (left-to-right)\n",
    "2. Running another RNN backward (right-to-left)\n",
    "3. Concatenating both hidden states at each time step\n",
    "\n",
    "**When to use bidirectional:**\n",
    "- ✅ Text classification, named entity recognition, sentiment analysis\n",
    "- ✅ Speech recognition\n",
    "- ✅ Any task where full sequence is available at inference\n",
    "- ❌ Real-time prediction (no future context available)\n",
    "- ❌ Time series forecasting (cannot see the future!)\n",
    "\n",
    "Let's demonstrate with sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic sentiment analysis dataset\n",
    "\n",
    "# Sample movie reviews (simplified for demonstration)\n",
    "positive_reviews = [\n",
    "    \"This movie was absolutely amazing and wonderful\",\n",
    "    \"I loved every minute of this film\",\n",
    "    \"Outstanding performance by all actors\",\n",
    "    \"A masterpiece of modern cinema\",\n",
    "    \"Brilliant storytelling and direction\",\n",
    "    \"Highly recommend this fantastic movie\",\n",
    "    \"One of the best films I have ever seen\",\n",
    "    \"Exceptional quality and entertainment\",\n",
    "    \"Thoroughly enjoyed this incredible film\",\n",
    "    \"Amazing visuals and great plot\"\n",
    "] * 10  # Repeat for more samples\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This movie was terrible and boring\",\n",
    "    \"I hated every minute of this film\",\n",
    "    \"Poor performance by all actors\",\n",
    "    \"A disaster of modern cinema\",\n",
    "    \"Awful storytelling and direction\",\n",
    "    \"Do not recommend this horrible movie\",\n",
    "    \"One of the worst films I have ever seen\",\n",
    "    \"Terrible quality and no entertainment\",\n",
    "    \"Could not stand this dreadful film\",\n",
    "    \"Bad visuals and weak plot\"\n",
    "] * 10  # Repeat for more samples\n",
    "\n",
    "# Combine and create labels\n",
    "all_reviews = positive_reviews + negative_reviews\n",
    "all_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(all_reviews))\n",
    "all_reviews = [all_reviews[i] for i in indices]\n",
    "all_labels = [all_labels[i] for i in indices]\n",
    "\n",
    "print(f\"Total reviews: {len(all_reviews)}\")\n",
    "print(f\"Positive: {sum(all_labels)}, Negative: {len(all_labels) - sum(all_labels)}\")\n",
    "print(\"\\nSample reviews:\")\n",
    "for i in range(3):\n",
    "    label = \"Positive\" if all_labels[i] == 1 else \"Negative\"\n",
    "    print(f\"  [{label}] {all_reviews[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and prepare sequences\n",
    "\n",
    "# Create tokenizer\n",
    "max_words = 1000\n",
    "max_len = 20\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "\n",
    "# Convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(all_reviews)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to array\n",
    "labels_array = np.array(all_labels)\n",
    "\n",
    "# Train/val/test split\n",
    "X_temp, X_test_nlp, y_temp, y_test_nlp = train_test_split(\n",
    "    padded_sequences, labels_array, test_size=0.2, random_state=42, stratify=labels_array\n",
    ")\n",
    "X_train_nlp, X_val_nlp, y_train_nlp, y_val_nlp = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nNLP Dataset splits:\")\n",
    "print(f\"Train: {X_train_nlp.shape}\")\n",
    "print(f\"Validation: {X_val_nlp.shape}\")\n",
    "print(f\"Test: {X_test_nlp.shape}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bidirectional LSTM model for sentiment analysis\n",
    "\n",
    "def build_bidirectional_lstm(vocab_size, embedding_dim=64, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Build bidirectional LSTM for text classification.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        embedding_dim: Dimension of word embeddings\n",
    "        lstm_units: Number of LSTM units\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        layers.Embedding(vocab_size, embedding_dim, name='embedding'),\n",
    "        \n",
    "        # Bidirectional LSTM: processes sequence in both directions\n",
    "        layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True), \n",
    "                           name='bidirectional_lstm_1'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Second bidirectional layer\n",
    "        layers.Bidirectional(layers.LSTM(lstm_units), name='bidirectional_lstm_2'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(32, activation='relu', name='dense'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build bidirectional model\n",
    "bi_lstm = build_bidirectional_lstm(vocab_size=max_words, embedding_dim=64, lstm_units=64)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BIDIRECTIONAL LSTM MODEL\")\n",
    "print(\"=\"*70)\n",
    "bi_lstm.summary()\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey: Bidirectional layers process sequences in BOTH directions,\")\n",
    "print(\"     doubling the hidden state size (forward + backward).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train bidirectional LSTM\n",
    "\n",
    "print(\"\\nTraining Bidirectional LSTM...\\n\")\n",
    "\n",
    "history_bi = bi_lstm.fit(\n",
    "    X_train_nlp, y_train_nlp,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val_nlp, y_val_nlp),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = bi_lstm.evaluate(X_test_nlp, y_test_nlp, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BIDIRECTIONAL LSTM RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history_bi.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history_bi.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Accuracy', fontsize=11)\n",
    "ax1.set_title('Bidirectional LSTM Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history_bi.history['loss'], label='Training', linewidth=2)\n",
    "ax2.plot(history_bi.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Loss', fontsize=11)\n",
    "ax2.set_title('Bidirectional LSTM Loss', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bidirectional_lstm_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Test on custom examples\n",
    "test_sentences = [\n",
    "    \"This movie was absolutely amazing\",\n",
    "    \"I hated this terrible film\",\n",
    "    \"Not bad but could be better\"\n",
    "]\n",
    "\n",
    "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seqs, maxlen=max_len, padding='post')\n",
    "predictions = bi_lstm.predict(test_padded, verbose=0)\n",
    "\n",
    "print(\"\\nCustom Predictions:\")\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    sentiment = \"Positive\" if pred[0] > 0.5 else \"Negative\"\n",
    "    print(f\"  '{sent}'\")\n",
    "    print(f\"  → {sentiment} (confidence: {pred[0]:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Sequence-to-Sequence Models\n",
    "\n",
    "### 6.1 Encoder-Decoder Architecture\n",
    "\n",
    "Many tasks require mapping one sequence to another sequence of **different length**:\n",
    "- Machine translation: \"Hello\" → \"Bonjour\"\n",
    "- Text summarization: Long article → Short summary\n",
    "- Question answering: Question + Context → Answer\n",
    "\n",
    "**Sequence-to-Sequence (Seq2Seq) architecture:**\n",
    "\n",
    "1. **Encoder**: Processes input sequence, compresses into context vector (final hidden state)\n",
    "2. **Decoder**: Generates output sequence from context vector\n",
    "\n",
    "**Example flow:**\n",
    "```\n",
    "Input:  \"How are you?\"\n",
    "Encoder: [h1, h2, h3, h4] → context_vector = h4\n",
    "Decoder: context_vector → [\"Comment\", \"allez\", \"vous\", \"?\"]\n",
    "Output: \"Comment allez vous?\"\n",
    "```\n",
    "\n",
    "### 6.2 Attention Mechanism (Preview)\n",
    "\n",
    "**Problem with basic seq2seq**: The context vector is a bottleneck - it must encode the entire input sequence!\n",
    "\n",
    "**Attention mechanism** (introduced 2015) solves this by:\n",
    "- Letting decoder \"attend\" to different parts of input at each decoding step\n",
    "- Computing weighted combination of all encoder hidden states\n",
    "\n",
    "**Impact**: Attention revolutionized NLP and led directly to Transformers (covered in Lesson 9c)!\n",
    "\n",
    "We'll demonstrate a simple seq2seq model for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sequence-to-sequence example: Reversing sequences\n",
    "\n",
    "def generate_reverse_sequences(num_samples=1000, seq_length=10, vocab_size=20):\n",
    "    \"\"\"\n",
    "    Generate sequences and their reverses for seq2seq training.\n",
    "    \n",
    "    Task: Learn to reverse a sequence of integers.\n",
    "    Example: [3, 7, 2, 9] → [9, 2, 7, 3]\n",
    "    \"\"\"\n",
    "    X = np.random.randint(1, vocab_size, size=(num_samples, seq_length))\n",
    "    y = np.flip(X, axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "seq_X, seq_y = generate_reverse_sequences(num_samples=5000, seq_length=10, vocab_size=20)\n",
    "\n",
    "# Split\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    seq_X, seq_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Sequence Reversal Task:\")\n",
    "print(f\"Training samples: {len(X_train_seq)}\")\n",
    "print(f\"Test samples: {len(X_test_seq)}\")\n",
    "print(f\"Sequence length: {seq_X.shape[1]}\")\n",
    "print(f\"Vocabulary size: 20\\n\")\n",
    "\n",
    "print(\"Example sequences:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Input:  {X_train_seq[i].tolist()}\")\n",
    "    print(f\"  Target: {y_train_seq[i].tolist()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simple seq2seq model\n",
    "\n",
    "def build_seq2seq_model(vocab_size=20, seq_length=10, embedding_dim=32, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Build simple sequence-to-sequence model.\n",
    "    \n",
    "    Note: This is a simplified version for demonstration.\n",
    "    Production seq2seq uses encoder-decoder with separate models.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size + 1, embedding_dim, input_length=seq_length),\n",
    "        layers.LSTM(lstm_units, return_sequences=True),\n",
    "        layers.LSTM(lstm_units, return_sequences=True),\n",
    "        layers.TimeDistributed(layers.Dense(vocab_size + 1, activation='softmax'))\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "seq2seq_model = build_seq2seq_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEQUENCE-TO-SEQUENCE MODEL\")\n",
    "print(\"=\"*70)\n",
    "seq2seq_model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining seq2seq model...\\n\")\n",
    "\n",
    "history_seq2seq = seq2seq_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = seq2seq_model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Seq2Seq Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Test predictions\n",
    "predictions = seq2seq_model.predict(X_test_seq[:5], verbose=0)\n",
    "pred_sequences = np.argmax(predictions, axis=-1)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Input:     {X_test_seq[i].tolist()}\")\n",
    "    print(f\"  Predicted: {pred_sequences[i].tolist()}\")\n",
    "    print(f\"  True:      {y_test_seq[i].tolist()}\")\n",
    "    print(f\"  Match:     {np.array_equal(pred_sequences[i], y_test_seq[i])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Production Best Practices for RNNs\n",
    "\n",
    "### 7.1 When to Use RNNs (2025 Guidance)\n",
    "\n",
    "**✅ Use RNNs/LSTMs/GRUs when:**\n",
    "- Working with time series data (stock prices, sensor data, energy consumption)\n",
    "- Resource constraints make Transformers impractical\n",
    "- Sequential processing is needed in real-time streaming scenarios\n",
    "- Sequence length is moderate (< 500 tokens)\n",
    "- You need online/incremental learning\n",
    "\n",
    "**⚠️ Consider Transformers instead when:**\n",
    "- Working with long sequences (> 500 tokens)\n",
    "- Parallelization is critical for training speed\n",
    "- You have large datasets and computational resources\n",
    "- Working on NLP tasks (text classification, translation, QA)\n",
    "\n",
    "### 7.2 Training Best Practices\n",
    "\n",
    "**Gradient Clipping:**\n",
    "Essential for preventing exploding gradients in RNNs:\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)  # Clip by norm\n",
    "# or\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=0.5)  # Clip by value\n",
    "```\n",
    "\n",
    "**Sequence Padding:**\n",
    "- Pad sequences to same length for batch processing\n",
    "- Use `padding='post'` for better performance\n",
    "- Use masking to ignore padded values\n",
    "\n",
    "**Stateful RNNs:**\n",
    "- For very long sequences, use stateful RNNs to maintain state across batches\n",
    "- Manually reset states between independent sequences\n",
    "\n",
    "### 7.3 Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready RNN pipeline\n",
    "\n",
    "class RNNProductionPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready RNN training pipeline with best practices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rnn_type='LSTM'):\n",
    "        self.rnn_type = rnn_type\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def build_model(self, input_shape, output_dim, units=64, num_layers=2):\n",
    "        \"\"\"\n",
    "        Build RNN model with best practices.\n",
    "        \n",
    "        Best practices included:\n",
    "        - Layer normalization\n",
    "        - Gradient clipping\n",
    "        - Dropout regularization\n",
    "        - Proper activation functions\n",
    "        \"\"\"\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Choose RNN type\n",
    "        RNN_Layer = layers.LSTM if self.rnn_type == 'LSTM' else layers.GRU\n",
    "        \n",
    "        # Build stacked RNN\n",
    "        for i in range(num_layers):\n",
    "            return_seq = (i < num_layers - 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                model.add(RNN_Layer(\n",
    "                    units,\n",
    "                    return_sequences=return_seq,\n",
    "                    input_shape=input_shape,\n",
    "                    recurrent_dropout=0.2,  # Recurrent dropout\n",
    "                    name=f'{self.rnn_type.lower()}_{i+1}'\n",
    "                ))\n",
    "            else:\n",
    "                model.add(RNN_Layer(\n",
    "                    units,\n",
    "                    return_sequences=return_seq,\n",
    "                    recurrent_dropout=0.2,\n",
    "                    name=f'{self.rnn_type.lower()}_{i+1}'\n",
    "                ))\n",
    "            \n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        # Output layers\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "        model.add(layers.Dense(output_dim))\n",
    "        \n",
    "        # Compile with gradient clipping\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001, clipnorm=1.0),  # Gradient clipping\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def get_callbacks(self):\n",
    "        \"\"\"Production callbacks.\"\"\"\n",
    "        return [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=7,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                'best_rnn_model.h5',\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def prepare_sequences(self, data, sequence_length, target_column=-1):\n",
    "        \"\"\"\n",
    "        Prepare sequences from time series data.\n",
    "        \n",
    "        Args:\n",
    "            data: Time series data (1D or 2D array)\n",
    "            sequence_length: Length of input sequences\n",
    "            target_column: Which column to predict (-1 for last)\n",
    "        \n",
    "        Returns:\n",
    "            X, y: Prepared sequences\n",
    "        \"\"\"\n",
    "        # Normalize data\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "        \n",
    "        data_normalized = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(data_normalized) - sequence_length):\n",
    "            X.append(data_normalized[i:i + sequence_length])\n",
    "            y.append(data_normalized[i + sequence_length, target_column])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "        \"\"\"Train with all best practices.\"\"\"\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=self.get_callbacks(),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Demonstrate production pipeline\n",
    "print(\"\\nProduction RNN Pipeline:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pipeline = RNNProductionPipeline(rnn_type='LSTM')\n",
    "model_prod = pipeline.build_model(input_shape=(50, 1), output_dim=1, units=64, num_layers=2)\n",
    "\n",
    "print(\"\\n✓ Production pipeline initialized with:\")\n",
    "print(\"  • Gradient clipping (clipnorm=1.0)\")\n",
    "print(\"  • Batch normalization\")\n",
    "print(\"  • Dropout regularization\")\n",
    "print(\"  • Recurrent dropout\")\n",
    "print(\"  • Early stopping\")\n",
    "print(\"  • Learning rate scheduling\")\n",
    "print(\"  • Model checkpointing\")\n",
    "print(\"  • Data normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "**RNN Fundamentals:**\n",
    "- RNNs maintain hidden state (memory) to process sequences\n",
    "- Vanilla RNNs suffer from vanishing/exploding gradients\n",
    "- Cannot effectively learn dependencies beyond 10-20 time steps\n",
    "\n",
    "**LSTM & GRU:**\n",
    "- LSTM uses three gates (forget, input, output) to control information flow\n",
    "- GRU simplifies to two gates (update, reset) with comparable performance\n",
    "- Both solve vanishing gradient problem and handle long-term dependencies\n",
    "- GRU has fewer parameters (~25% less) and trains faster\n",
    "\n",
    "**Bidirectional RNNs:**\n",
    "- Process sequences in both forward and backward directions\n",
    "- Essential for tasks where full sequence context is available\n",
    "- Double the hidden state size (forward + backward concatenated)\n",
    "- Not suitable for real-time prediction or time series forecasting\n",
    "\n",
    "**Sequence-to-Sequence:**\n",
    "- Encoder-decoder architecture for variable-length input/output\n",
    "- Attention mechanism revolutionized seq2seq models\n",
    "- Foundation for modern Transformers (Lesson 9c)\n",
    "\n",
    "**Production Best Practices:**\n",
    "- Always use gradient clipping to prevent exploding gradients\n",
    "- Normalize/scale input data for stable training\n",
    "- Apply dropout and recurrent dropout for regularization\n",
    "- Use proper sequence padding and masking\n",
    "- Choose LSTM for complex tasks, GRU for efficiency\n",
    "\n",
    "### RNNs vs Transformers (2025):\n",
    "\n",
    "**Use RNNs when:**\n",
    "- Time series forecasting\n",
    "- Streaming/online learning\n",
    "- Resource-constrained environments\n",
    "- Moderate sequence lengths (< 500)\n",
    "\n",
    "**Use Transformers when:**\n",
    "- NLP tasks (classification, translation, QA)\n",
    "- Long sequences (> 500 tokens)\n",
    "- Large datasets and compute available\n",
    "- Parallelization is critical\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Lesson 9c**: Transformers & Attention - Master the architecture dominating AI in 2025\n",
    "- **Advanced RNN topics**: Attention mechanisms, teacher forcing, beam search\n",
    "- **Applications**: Speech recognition, video analysis, anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Exercises & Further Exploration\n",
    "\n",
    "### Exercise 1: Stock Price Prediction\n",
    "Download real stock price data and build an LSTM model to predict future prices:\n",
    "- Use multiple features (open, high, low, close, volume)\n",
    "- Experiment with different sequence lengths\n",
    "- Compare LSTM vs GRU performance\n",
    "\n",
    "### Exercise 2: Text Generation\n",
    "Train a character-level LSTM to generate text:\n",
    "- Use a corpus (Shakespeare, news articles, code)\n",
    "- Implement temperature sampling for diversity\n",
    "- Visualize what the model learns at different layers\n",
    "\n",
    "### Exercise 3: Bidirectional vs Unidirectional\n",
    "Compare bidirectional and unidirectional models:\n",
    "- Use the same sentiment analysis task\n",
    "- Measure accuracy difference\n",
    "- Analyze which performs better and why\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- **Original Papers**:\n",
    "  - LSTM: \"Long Short-Term Memory\" (Hochreiter & Schmidhuber, 1997)\n",
    "  - GRU: \"Learning Phrase Representations\" (Cho et al., 2014)\n",
    "  - Seq2Seq: \"Sequence to Sequence Learning\" (Sutskever et al., 2014)\n",
    "  - Attention: \"Neural Machine Translation by Jointly Learning to Align\" (Bahdanau et al., 2015)\n",
    "\n",
    "- **Resources**:\n",
    "  - Stanford CS224n: Natural Language Processing with Deep Learning\n",
    "  - \"Understanding LSTM Networks\" by Christopher Olah\n",
    "  - TensorFlow Time Series Tutorial\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand RNNs, LSTMs, GRUs, and sequence modeling. These architectures remain essential for time series and streaming applications in 2025.\n",
    "\n",
    "Continue to **Lesson 9c: Transformers & Attention** to learn the architecture that revolutionized AI! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
