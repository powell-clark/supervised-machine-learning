{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9a: Convolutional Neural Networks & Transfer Learning\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental components of Convolutional Neural Networks (CNNs)\n",
    "- Learn how convolution, pooling, and activation layers work together\n",
    "- Build CNNs from scratch for image classification tasks\n",
    "- Master transfer learning techniques using pre-trained models\n",
    "- Apply data augmentation strategies to improve model generalization\n",
    "- Implement fine-tuning strategies for domain adaptation\n",
    "\n",
    "**Prerequisites:**\n",
    "- Understanding of basic neural networks (Lesson 3a, 3b)\n",
    "- Familiarity with gradient descent and backpropagation\n",
    "- Basic knowledge of Python and NumPy\n",
    "\n",
    "**Why CNNs Matter in 2025:**\n",
    "Convolutional Neural Networks revolutionized computer vision and remain the foundation for modern visual AI systems. They are essential for image classification, object detection, medical imaging, autonomous vehicles, and countless production applications. Understanding CNNs is critical for any machine learning practitioner working with visual or spatial data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "This notebook requires TensorFlow/Keras for deep learning implementations. We'll automatically install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip if not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package_name.split('[')[0])\n",
    "        print(f\"\u2713 {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name])\n",
    "        print(f\"\u2713 {package_name} installed successfully\")\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    'tensorflow>=2.13.0',\n",
    "    'numpy>=1.24.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'scikit-learn>=1.3.0',\n",
    "    'pillow>=10.0.0',\n",
    "    'seaborn>=0.12.0'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n\u2713 All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. CNN Fundamentals: Understanding the Building Blocks\n",
    "\n",
    "### 2.1 What Makes CNNs Special?\n",
    "\n",
    "Unlike fully connected neural networks, CNNs leverage three key architectural innovations:\n",
    "\n",
    "1. **Local Connectivity**: Each neuron connects only to a small region of the input, capturing local patterns\n",
    "2. **Parameter Sharing**: The same filter is applied across the entire image, drastically reducing parameters\n",
    "3. **Translation Invariance**: Features detected anywhere in the image are recognized equally well\n",
    "\n",
    "### 2.2 Core Components\n",
    "\n",
    "**Convolutional Layers:**\n",
    "- Apply learnable filters (kernels) to detect features like edges, textures, and patterns\n",
    "- Each filter slides across the input to produce a feature map\n",
    "- Mathematical operation: element-wise multiplication and sum\n",
    "\n",
    "**Pooling Layers:**\n",
    "- Reduce spatial dimensions while retaining important information\n",
    "- Max pooling: takes maximum value in each window (most common)\n",
    "- Average pooling: takes average value in each window\n",
    "- Benefits: reduces computation, provides translation invariance, prevents overfitting\n",
    "\n",
    "**Activation Functions:**\n",
    "- ReLU (Rectified Linear Unit): most common, f(x) = max(0, x)\n",
    "- Introduces non-linearity, enabling complex pattern recognition\n",
    "\n",
    "Let's visualize these operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_convolution_operation():\n",
    "    \"\"\"Demonstrate how convolution works with a simple example.\"\"\"\n",
    "    \n",
    "    # Create a simple 6x6 input image with a vertical edge\n",
    "    input_image = np.array([\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Vertical edge detection filter (Sobel operator)\n",
    "    vertical_filter = np.array([\n",
    "        [-1, 0, 1],\n",
    "        [-1, 0, 1],\n",
    "        [-1, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Horizontal edge detection filter\n",
    "    horizontal_filter = np.array([\n",
    "        [-1, -1, -1],\n",
    "        [0, 0, 0],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Manual convolution operation\n",
    "    def convolve2d(image, kernel):\n",
    "        \"\"\"Simple 2D convolution without padding.\"\"\"\n",
    "        output_size = image.shape[0] - kernel.shape[0] + 1\n",
    "        output = np.zeros((output_size, output_size))\n",
    "        \n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                region = image[i:i+kernel.shape[0], j:j+kernel.shape[1]]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    vertical_output = convolve2d(input_image, vertical_filter)\n",
    "    horizontal_output = convolve2d(input_image, horizontal_filter)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Row 1: Vertical edge detection\n",
    "    axes[0, 0].imshow(input_image, cmap='gray')\n",
    "    axes[0, 0].set_title('Input Image\\n(Vertical Edge)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(vertical_filter, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, 1].set_title('Vertical Edge Filter\\n(Sobel)', fontsize=12, fontweight='bold')\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[0, 1].text(j, i, f'{vertical_filter[i, j]:.0f}', \n",
    "                          ha='center', va='center', fontsize=10)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(vertical_output, cmap='hot')\n",
    "    axes[0, 2].set_title('Feature Map\\n(Strong Response!)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Row 2: Horizontal edge detection\n",
    "    axes[1, 0].imshow(input_image, cmap='gray')\n",
    "    axes[1, 0].set_title('Same Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(horizontal_filter, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[1, 1].set_title('Horizontal Edge Filter', fontsize=12, fontweight='bold')\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[1, 1].text(j, i, f'{horizontal_filter[i, j]:.0f}', \n",
    "                          ha='center', va='center', fontsize=10)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(horizontal_output, cmap='hot')\n",
    "    axes[1, 2].set_title('Feature Map\\n(Weak Response)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn_convolution_demo.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Insight:\")\n",
    "    print(\"The vertical edge filter strongly responds to vertical edges in the image.\")\n",
    "    print(\"The horizontal edge filter shows weak response to the same vertical edge.\")\n",
    "    print(\"This demonstrates how different filters detect different features!\")\n",
    "\n",
    "visualize_convolution_operation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_pooling():\n",
    "    \"\"\"Visualize max pooling and average pooling operations.\"\"\"\n",
    "    \n",
    "    # Create a sample feature map\n",
    "    feature_map = np.array([\n",
    "        [1, 3, 2, 4],\n",
    "        [5, 6, 1, 3],\n",
    "        [2, 4, 8, 2],\n",
    "        [1, 3, 5, 7]\n",
    "    ])\n",
    "    \n",
    "    # Max pooling 2x2\n",
    "    max_pooled = np.array([\n",
    "        [np.max(feature_map[0:2, 0:2]), np.max(feature_map[0:2, 2:4])],\n",
    "        [np.max(feature_map[2:4, 0:2]), np.max(feature_map[2:4, 2:4])]\n",
    "    ])\n",
    "    \n",
    "    # Average pooling 2x2\n",
    "    avg_pooled = np.array([\n",
    "        [np.mean(feature_map[0:2, 0:2]), np.mean(feature_map[0:2, 2:4])],\n",
    "        [np.mean(feature_map[2:4, 0:2]), np.mean(feature_map[2:4, 2:4])]\n",
    "    ])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original feature map\n",
    "    im1 = axes[0].imshow(feature_map, cmap='viridis', vmin=0, vmax=8)\n",
    "    axes[0].set_title('Original Feature Map\\n(4\u00d74)', fontsize=12, fontweight='bold')\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[0].text(j, i, f'{feature_map[i, j]:.0f}', \n",
    "                        ha='center', va='center', fontsize=14, color='white')\n",
    "    axes[0].set_xticks([])\n",
    "    axes[0].set_yticks([])\n",
    "    \n",
    "    # Max pooled\n",
    "    im2 = axes[1].imshow(max_pooled, cmap='viridis', vmin=0, vmax=8)\n",
    "    axes[1].set_title('Max Pooling (2\u00d72)\\nTakes Maximum', fontsize=12, fontweight='bold')\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1].text(j, i, f'{max_pooled[i, j]:.0f}', \n",
    "                        ha='center', va='center', fontsize=14, color='white')\n",
    "    axes[1].set_xticks([])\n",
    "    axes[1].set_yticks([])\n",
    "    \n",
    "    # Average pooled\n",
    "    im3 = axes[2].imshow(avg_pooled, cmap='viridis', vmin=0, vmax=8)\n",
    "    axes[2].set_title('Average Pooling (2\u00d72)\\nTakes Average', fontsize=12, fontweight='bold')\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[2].text(j, i, f'{avg_pooled[i, j]:.1f}', \n",
    "                        ha='center', va='center', fontsize=14, color='white')\n",
    "    axes[2].set_xticks([])\n",
    "    axes[2].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn_pooling_demo.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPooling Effects:\")\n",
    "    print(f\"Original size: {feature_map.shape} \u2192 Pooled size: {max_pooled.shape}\")\n",
    "    print(f\"Parameters reduced: {feature_map.size} \u2192 {max_pooled.size} (75% reduction)\")\n",
    "    print(\"\\nMax pooling preserves strongest activations (most common choice).\")\n",
    "    print(\"Average pooling preserves overall information (useful for certain architectures).\")\n",
    "\n",
    "demonstrate_pooling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Building Your First CNN from Scratch\n",
    "\n",
    "We'll build a CNN for the classic MNIST handwritten digit classification task. This demonstrates all core CNN concepts in a working implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape to add channel dimension (28, 28) \u2192 (28, 28, 1)\n",
    "X_train_full = X_train_full.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Create validation split\n",
    "X_train, X_val = X_train_full[:-10000], X_train_full[-10000:]\n",
    "y_train, y_val = y_train_full[:-10000], y_train_full[-10000:]\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"Training: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}, Labels: {y_test.shape}\")\n",
    "\n",
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn(input_shape=(28, 28, 1), num_classes=10):\n",
    "    \"\"\"\n",
    "    Build a simple CNN for MNIST classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv2D(32 filters, 3x3) + ReLU + MaxPooling(2x2)\n",
    "    - Conv2D(64 filters, 3x3) + ReLU + MaxPooling(2x2)\n",
    "    - Flatten\n",
    "    - Dense(128) + ReLU + Dropout(0.5)\n",
    "    - Dense(num_classes) + Softmax\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', \n",
    "                     input_shape=input_shape, name='conv1'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), name='pool1'),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', name='conv2'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), name='pool2'),\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(128, activation='relu', name='fc1'),\n",
    "        layers.Dropout(0.5, name='dropout'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_simple_cnn()\n",
    "\n",
    "# Compile with optimizer, loss, and metrics\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CNN ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining CNN on MNIST...\\n\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn_training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and confusion matrix\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - MNIST Classification', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_classes, \n",
    "                          target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Transfer Learning: Leveraging Pre-trained Models\n",
    "\n",
    "### Why Transfer Learning?\n",
    "\n",
    "Training deep CNNs from scratch requires:\n",
    "- Massive datasets (millions of images)\n",
    "- Extensive computational resources (GPUs/TPUs)\n",
    "- Days or weeks of training time\n",
    "\n",
    "**Transfer learning solves this by:**\n",
    "1. Using models pre-trained on large datasets (ImageNet: 14M images, 1000 classes)\n",
    "2. Fine-tuning these models for your specific task\n",
    "3. Achieving excellent results with small datasets and limited compute\n",
    "\n",
    "### Common Pre-trained Architectures (2025):\n",
    "\n",
    "- **VGG16/VGG19**: Simple, deep architectures (16-19 layers)\n",
    "- **ResNet50/ResNet101**: Residual connections enable very deep networks (50-152 layers)\n",
    "- **MobileNetV2/V3**: Lightweight models for mobile/edge deployment\n",
    "- **EfficientNet**: State-of-the-art accuracy with compound scaling\n",
    "- **Vision Transformers (ViT)**: Transformer-based architectures (covered in Lesson 9c)\n",
    "\n",
    "We'll demonstrate transfer learning using a small custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic image classification dataset\n",
    "# In practice, you would load your own images\n",
    "\n",
    "def create_synthetic_image_dataset(num_samples=1000, img_size=224):\n",
    "    \"\"\"\n",
    "    Create synthetic image dataset for demonstration.\n",
    "    In real applications, use your own image data.\n",
    "    \n",
    "    Returns:\n",
    "        X: Images of shape (num_samples, img_size, img_size, 3)\n",
    "        y: Labels (0 or 1 for binary classification)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Class 0: Images with more blue channel\n",
    "        if i < num_samples // 2:\n",
    "            img = np.random.rand(img_size, img_size, 3)\n",
    "            img[:, :, 2] += 0.3  # Boost blue channel\n",
    "            img = np.clip(img, 0, 1)\n",
    "            y.append(0)\n",
    "        # Class 1: Images with more red channel\n",
    "        else:\n",
    "            img = np.random.rand(img_size, img_size, 3)\n",
    "            img[:, :, 0] += 0.3  # Boost red channel\n",
    "            img = np.clip(img, 0, 1)\n",
    "            y.append(1)\n",
    "        \n",
    "        X.append(img)\n",
    "    \n",
    "    return np.array(X, dtype='float32'), np.array(y)\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating synthetic image dataset...\")\n",
    "X_images, y_images = create_synthetic_image_dataset(num_samples=1000, img_size=224)\n",
    "\n",
    "# Split into train/validation/test\n",
    "X_temp, X_test_tl, y_temp, y_test_tl = train_test_split(\n",
    "    X_images, y_images, test_size=0.2, random_state=42, stratify=y_images\n",
    ")\n",
    "X_train_tl, X_val_tl, y_train_tl, y_val_tl = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"Train: {X_train_tl.shape}, Labels: {y_train_tl.shape}\")\n",
    "print(f\"Validation: {X_val_tl.shape}, Labels: {y_val_tl.shape}\")\n",
    "print(f\"Test: {X_test_tl.shape}, Labels: {y_test_tl.shape}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(X_train_tl[i])\n",
    "    axes[0, i].set_title(f'Class {y_train_tl[i]} (Blue-ish)', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(X_train_tl[500 + i])\n",
    "    axes[1, i].set_title(f'Class {y_train_tl[500 + i]} (Red-ish)', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Synthetic Dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model(base_model_name='ResNet50', trainable_layers=0):\n",
    "    \"\"\"\n",
    "    Build a transfer learning model using a pre-trained base.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: 'VGG16', 'ResNet50', or 'MobileNetV2'\n",
    "        trainable_layers: Number of top layers to make trainable (0 = freeze all)\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained base model\n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                          input_shape=(224, 224, 3))\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, \n",
    "                             input_shape=(224, 224, 3))\n",
    "    elif base_model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, \n",
    "                                input_shape=(224, 224, 3))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown base model: {base_model_name}\")\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Optionally unfreeze top layers for fine-tuning\n",
    "    if trainable_layers > 0:\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:-trainable_layers]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # Build complete model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build transfer learning model using ResNet50\n",
    "print(\"Building transfer learning model with ResNet50...\\n\")\n",
    "tl_model = build_transfer_learning_model(base_model_name='ResNet50', trainable_layers=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSFER LEARNING MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "tl_model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count trainable vs non-trainable parameters\n",
    "trainable_params = sum([np.prod(v.shape) for v in tl_model.trainable_weights])\n",
    "non_trainable_params = sum([np.prod(v.shape) for v in tl_model.non_trainable_weights])\n",
    "\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "print(f\"Total parameters: {trainable_params + non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for better generalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Train the transfer learning model\n",
    "print(\"\\nTraining transfer learning model...\\n\")\n",
    "\n",
    "history_tl = tl_model.fit(\n",
    "    datagen.flow(X_train_tl, y_train_tl, batch_size=32),\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_tl, y_val_tl),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Transfer learning training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate transfer learning model\n",
    "test_loss_tl, test_acc_tl = tl_model.evaluate(X_test_tl, y_test_tl, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRANSFER LEARNING TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_acc_tl*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss_tl:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fine-Tuning Strategy\n",
    "\n",
    "### Two-Stage Fine-Tuning Approach:\n",
    "\n",
    "**Stage 1: Feature Extraction**\n",
    "- Freeze all pre-trained layers\n",
    "- Train only the new top layers\n",
    "- Fast training, prevents catastrophic forgetting\n",
    "\n",
    "**Stage 2: Fine-Tuning**\n",
    "- Unfreeze top layers of base model\n",
    "- Train with very low learning rate\n",
    "- Adapts features to your specific domain\n",
    "\n",
    "Let's demonstrate this strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Fine-tuning - Unfreeze top layers\n",
    "print(\"Stage 2: Fine-tuning top layers...\\n\")\n",
    "\n",
    "# Rebuild model with some trainable layers\n",
    "tl_model_finetuned = build_transfer_learning_model(\n",
    "    base_model_name='ResNet50', \n",
    "    trainable_layers=10  # Unfreeze last 10 layers\n",
    ")\n",
    "\n",
    "# Use much lower learning rate for fine-tuning\n",
    "tl_model_finetuned.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),  # 100x smaller learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning with unfrozen top layers...\\n\")\n",
    "\n",
    "history_ft = tl_model_finetuned.fit(\n",
    "    datagen.flow(X_train_tl, y_train_tl, batch_size=32),\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_tl, y_val_tl),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "test_loss_ft, test_acc_ft = tl_model_finetuned.evaluate(X_test_tl, y_test_tl, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINE-TUNED MODEL TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_acc_ft*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss_ft:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nImprovement from fine-tuning: {(test_acc_ft - test_acc_tl)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Augmentation Techniques\n",
    "\n",
    "Data augmentation artificially expands your training dataset by applying transformations. This is critical for preventing overfitting when training with limited data.\n",
    "\n",
    "### Common Augmentation Techniques:\n",
    "\n",
    "1. **Geometric Transformations**: Rotation, flipping, shifting, zooming\n",
    "2. **Color Transformations**: Brightness, contrast, saturation adjustments\n",
    "3. **Advanced Techniques**: Cutout, mixup, CutMix (advanced)\n",
    "\n",
    "Let's visualize different augmentations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate various data augmentation techniques\n",
    "sample_image = X_train_tl[0:1]  # Take first image\n",
    "\n",
    "# Define different augmentation strategies\n",
    "augmentation_configs = [\n",
    "    ('Original', ImageDataGenerator()),\n",
    "    ('Rotation (30\u00b0)', ImageDataGenerator(rotation_range=30)),\n",
    "    ('Horizontal Flip', ImageDataGenerator(horizontal_flip=True)),\n",
    "    ('Width Shift (20%)', ImageDataGenerator(width_shift_range=0.2)),\n",
    "    ('Height Shift (20%)', ImageDataGenerator(height_shift_range=0.2)),\n",
    "    ('Zoom (20%)', ImageDataGenerator(zoom_range=0.2)),\n",
    "    ('Brightness (\u00b130%)', ImageDataGenerator(brightness_range=[0.7, 1.3])),\n",
    "    ('Combined', ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        brightness_range=[0.8, 1.2]\n",
    "    ))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, datagen) in enumerate(augmentation_configs):\n",
    "    datagen.fit(sample_image)\n",
    "    aug_iter = datagen.flow(sample_image, batch_size=1)\n",
    "    aug_image = next(aug_iter)[0]\n",
    "    \n",
    "    axes[idx].imshow(aug_image)\n",
    "    axes[idx].set_title(name, fontsize=11, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Techniques', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_augmentation_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData Augmentation Best Practices:\")\n",
    "print(\"1. Apply augmentation ONLY to training data, never to validation/test\")\n",
    "print(\"2. Choose augmentations that match your domain (e.g., no vertical flip for text)\")\n",
    "print(\"3. Combine multiple techniques for stronger regularization\")\n",
    "print(\"4. Monitor validation performance to avoid excessive augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Modern CNN Architectures Overview\n",
    "\n",
    "### Evolution of CNN Architectures (2012-2025):\n",
    "\n",
    "**AlexNet (2012)**\n",
    "- 8 layers, 60M parameters\n",
    "- First to use ReLU and dropout\n",
    "- Won ImageNet by huge margin\n",
    "\n",
    "**VGGNet (2014)**\n",
    "- Very deep (16-19 layers)\n",
    "- Simple architecture: 3\u00d73 convs only\n",
    "- 138M parameters (VGG16)\n",
    "\n",
    "**ResNet (2015)**\n",
    "- Introduced skip connections (residual learning)\n",
    "- Enabled training of very deep networks (50-152 layers)\n",
    "- Solved vanishing gradient problem\n",
    "\n",
    "**Inception/GoogLeNet (2015)**\n",
    "- Multi-scale processing with inception modules\n",
    "- Efficient with fewer parameters\n",
    "\n",
    "**MobileNet (2017-2019)**\n",
    "- Designed for mobile and edge devices\n",
    "- Depthwise separable convolutions\n",
    "- 10-100\u00d7 fewer parameters than ResNet\n",
    "\n",
    "**EfficientNet (2019)**\n",
    "- Compound scaling (depth, width, resolution)\n",
    "- State-of-the-art accuracy-to-efficiency ratio\n",
    "\n",
    "**Vision Transformers (2020-2025)**\n",
    "- Apply transformer architecture to images\n",
    "- Can outperform CNNs with sufficient data\n",
    "- Covered in detail in Lesson 9c\n",
    "\n",
    "Let's compare some key architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different pre-trained architectures\n",
    "architectures = [\n",
    "    ('VGG16', VGG16),\n",
    "    ('ResNet50', ResNet50),\n",
    "    ('MobileNetV2', MobileNetV2)\n",
    "]\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CNN ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Architecture':<20} {'Parameters':<15} {'Layers':<10} {'Top-1 Acc*':<15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, arch_class in architectures:\n",
    "    model = arch_class(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "    params = model.count_params()\n",
    "    num_layers = len(model.layers)\n",
    "    \n",
    "    # Approximate ImageNet top-1 accuracy (from literature)\n",
    "    accuracies = {'VGG16': 71.3, 'ResNet50': 76.1, 'MobileNetV2': 71.8}\n",
    "    accuracy = accuracies.get(name, 'N/A')\n",
    "    \n",
    "    print(f\"{name:<20} {params:>13,}  {num_layers:<10} {accuracy}%\")\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'name': name,\n",
    "        'params': params / 1e6,  # Convert to millions\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"* ImageNet Top-1 Accuracy (approximate)\\n\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = [d['name'] for d in comparison_data]\n",
    "params = [d['params'] for d in comparison_data]\n",
    "accs = [d['accuracy'] for d in comparison_data]\n",
    "\n",
    "# Parameters comparison\n",
    "ax1.bar(names, params, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax1.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax1.set_title('Model Size Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "ax2.bar(names, accs, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax2.set_ylabel('Top-1 Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('ImageNet Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim([65, 80])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('architecture_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"\u2022 ResNet50: Best accuracy but moderate size\")\n",
    "print(\"\u2022 MobileNetV2: Smallest size, good for mobile deployment\")\n",
    "print(\"\u2022 VGG16: Largest size, simple architecture, decent accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Production Best Practices\n",
    "\n",
    "### Key Considerations for Deploying CNNs:\n",
    "\n",
    "**1. Model Selection:**\n",
    "- Choose architecture based on deployment constraints (mobile, cloud, edge)\n",
    "- Balance accuracy vs. inference speed vs. model size\n",
    "- Consider MobileNet/EfficientNet for resource-constrained environments\n",
    "\n",
    "**2. Optimization Techniques:**\n",
    "- Quantization: Convert FP32 to INT8 (4\u00d7 smaller, faster inference)\n",
    "- Pruning: Remove redundant weights\n",
    "- Knowledge distillation: Train smaller model to mimic larger one\n",
    "\n",
    "**3. Training Best Practices:**\n",
    "- Always use data augmentation to prevent overfitting\n",
    "- Implement early stopping and learning rate scheduling\n",
    "- Monitor validation metrics closely\n",
    "- Use proper train/val/test splits (no data leakage!)\n",
    "\n",
    "**4. Transfer Learning Guidelines:**\n",
    "- Start with pre-trained weights when possible\n",
    "- Freeze base layers initially, fine-tune later\n",
    "- Use very low learning rates for fine-tuning (1e-5 to 1e-6)\n",
    "- Match input preprocessing to pre-training (ImageNet normalization)\n",
    "\n",
    "**5. Monitoring and Debugging:**\n",
    "- Track training curves (loss, accuracy)\n",
    "- Visualize predictions and errors\n",
    "- Use confusion matrices for classification tasks\n",
    "- Check class balance and distribution shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready training pipeline example\n",
    "\n",
    "class CNNProductionPipeline:\n",
    "    \"\"\"Production-ready CNN training pipeline with best practices.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model='ResNet50', num_classes=10):\n",
    "        self.base_model_name = base_model\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def build_model(self, input_shape=(224, 224, 3), trainable_base=False):\n",
    "        \"\"\"Build transfer learning model with custom top layers.\"\"\"\n",
    "        \n",
    "        # Load pre-trained base\n",
    "        base_models = {\n",
    "            'VGG16': VGG16,\n",
    "            'ResNet50': ResNet50,\n",
    "            'MobileNetV2': MobileNetV2\n",
    "        }\n",
    "        \n",
    "        base = base_models[self.base_model_name](\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "        \n",
    "        base.trainable = trainable_base\n",
    "        \n",
    "        # Build complete model\n",
    "        self.model = models.Sequential([\n",
    "            base,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        \"\"\"Compile model with optimizer and metrics.\"\"\"\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy', \n",
    "                    keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    "        )\n",
    "    \n",
    "    def get_callbacks(self, checkpoint_path='best_model.h5'):\n",
    "        \"\"\"Create production callbacks for training.\"\"\"\n",
    "        \n",
    "        return [\n",
    "            # Save best model\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                checkpoint_path,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Early stopping\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Learning rate reduction\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # TensorBoard logging\n",
    "            keras.callbacks.TensorBoard(\n",
    "                log_dir='./logs',\n",
    "                histogram_freq=1\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def create_data_augmentation(self):\n",
    "        \"\"\"Create data augmentation pipeline.\"\"\"\n",
    "        \n",
    "        return ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.2,\n",
    "            shear_range=0.15,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, \n",
    "             epochs=50, batch_size=32, use_augmentation=True):\n",
    "        \"\"\"Train the model with all best practices.\"\"\"\n",
    "        \n",
    "        if use_augmentation:\n",
    "            datagen = self.create_data_augmentation()\n",
    "            train_data = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "        else:\n",
    "            train_data = (X_train, y_train)\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            train_data,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=self.get_callbacks(),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model on test set.\"\"\"\n",
    "        \n",
    "        results = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        metrics = dict(zip(self.model.metrics_names, results))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST SET EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        for metric, value in metrics.items():\n",
    "            if 'acc' in metric:\n",
    "                print(f\"{metric}: {value*100:.2f}%\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Demonstrate usage\n",
    "print(\"\\nProduction Pipeline Example:\")\n",
    "print(\"=\"*60)\n",
    "pipeline = CNNProductionPipeline(base_model='MobileNetV2', num_classes=10)\n",
    "model_prod = pipeline.build_model()\n",
    "pipeline.compile_model(learning_rate=0.001)\n",
    "\n",
    "print(\"\\n\u2713 Production pipeline initialized\")\n",
    "print(\"\\nPipeline includes:\")\n",
    "print(\"  \u2022 Transfer learning with pre-trained weights\")\n",
    "print(\"  \u2022 Data augmentation\")\n",
    "print(\"  \u2022 Model checkpointing\")\n",
    "print(\"  \u2022 Early stopping\")\n",
    "print(\"  \u2022 Learning rate scheduling\")\n",
    "print(\"  \u2022 TensorBoard logging\")\n",
    "print(\"  \u2022 Multiple evaluation metrics (accuracy, top-5 accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "**CNN Fundamentals:**\n",
    "- Convolutional layers detect local patterns using learnable filters\n",
    "- Pooling layers reduce spatial dimensions and provide translation invariance\n",
    "- CNNs leverage local connectivity and parameter sharing for efficiency\n",
    "\n",
    "**Building CNNs:**\n",
    "- Start simple, add complexity as needed\n",
    "- Stack conv-pool blocks followed by dense layers\n",
    "- Use ReLU activation and dropout for regularization\n",
    "\n",
    "**Transfer Learning:**\n",
    "- Pre-trained models save massive computational resources\n",
    "- Two-stage approach: feature extraction \u2192 fine-tuning\n",
    "- Critical for small datasets and limited compute\n",
    "\n",
    "**Production Best Practices:**\n",
    "- Always use data augmentation for better generalization\n",
    "- Implement proper callbacks (early stopping, LR scheduling)\n",
    "- Monitor multiple metrics and validate on held-out data\n",
    "- Choose architecture based on deployment constraints\n",
    "\n",
    "### When to Use CNNs (2025):\n",
    "\n",
    "**\u2705 Excellent for:**\n",
    "- Image classification and object detection\n",
    "- Medical imaging analysis\n",
    "- Video analysis and action recognition\n",
    "- Any task with spatial or grid-like data\n",
    "\n",
    "**\u26a0\ufe0f Consider alternatives:**\n",
    "- Vision Transformers for very large datasets (>14M images)\n",
    "- Graph Neural Networks for non-grid structured data\n",
    "- RNNs/Transformers for sequential data (time series, text)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Lesson 9b**: RNNs & Sequences - Learn about sequential data processing\n",
    "- **Lesson 9c**: Transformers & Attention - Master the architecture dominating AI in 2025\n",
    "- **Advanced topics**: Object detection (YOLO, Faster R-CNN), semantic segmentation (U-Net), GANs\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Exercises & Further Exploration\n",
    "\n",
    "### Exercise 1: Build a Custom CNN\n",
    "Modify the simple CNN architecture to improve MNIST accuracy beyond 99%. Try:\n",
    "- Adding more convolutional layers\n",
    "- Using batch normalization\n",
    "- Experimenting with different filter sizes\n",
    "\n",
    "### Exercise 2: Transfer Learning on Your Data\n",
    "Apply transfer learning to a real-world dataset:\n",
    "- Download a small image dataset (e.g., Cats vs Dogs, Flowers)\n",
    "- Use different pre-trained models (VGG16, ResNet50, EfficientNet)\n",
    "- Compare results and computational requirements\n",
    "\n",
    "### Exercise 3: Data Augmentation Impact\n",
    "Train the same model with and without data augmentation:\n",
    "- Measure accuracy difference\n",
    "- Visualize training curves\n",
    "- Analyze overfitting behavior\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- **Original Papers**:\n",
    "  - AlexNet: \"ImageNet Classification with Deep CNNs\" (Krizhevsky et al., 2012)\n",
    "  - ResNet: \"Deep Residual Learning\" (He et al., 2015)\n",
    "  - EfficientNet: \"Rethinking Model Scaling\" (Tan & Le, 2019)\n",
    "\n",
    "- **Resources**:\n",
    "  - Stanford CS231n: Convolutional Neural Networks\n",
    "  - fast.ai Practical Deep Learning course\n",
    "  - TensorFlow/Keras documentation and tutorials\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand the fundamentals of CNNs and transfer learning. These skills are essential for computer vision tasks and form the foundation for many modern AI applications.\n",
    "\n",
    "Continue to **Lesson 9b: RNNs & Sequences** to learn about processing sequential data! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}