{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: Comparing Tree-Based Models Using the Beast Pipeline\n",
    "\n",
    "In this notebook, we'll explore systematic model comparison using the beast pipeline - a comprehensive framework for evaluating different tree-based models and feature combinations. We'll focus on understanding:\n",
    "\n",
    "1. How to rigorously compare model performance\n",
    "2. The impact of different feature engineering strategies\n",
    "3. Trade-offs between model complexity and performance\n",
    "4. Best practices for model evaluation and selection\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Model Comparison](#introduction)\n",
    "2. [Understanding the Beast Pipeline](#understanding-beast)\n",
    "3. [Feature Engineering Deep Dive](#feature-engineering)\n",
    "4. [Model Comparison Framework](#comparison-framework)\n",
    "5. [Results Analysis & Visualization](#results-analysis)\n",
    "6. [Model Selection Guidelines](#model-selection)\n",
    "7. [Production Considerations](#production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Model Comparison\n",
    "\n",
    "When developing machine learning solutions, we often need to compare different:\n",
    "\n",
    "- Model types (Decision Trees, Random Forests, XGBoost)\n",
    "- Feature engineering strategies\n",
    "- Hyperparameter configurations\n",
    "- Training approaches\n",
    "\n",
    "The beast pipeline provides a systematic way to make these comparisons while ensuring:\n",
    "\n",
    "- Fair evaluation conditions\n",
    "- No data leakage\n",
    "- Proper cross-validation\n",
    "- Comprehensive metrics\n",
    "- Statistical significance\n",
    "\n",
    "Let's start by importing the necessary libraries and loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning - Core\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    KFold, \n",
    ")\n",
    "\n",
    "# Preprocessing and Encoding\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, \n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "# Advanced ML models\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# System utilities\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Load the preprocessed data\n",
    "with open('../data/df_with_outcode.pkl', 'rb') as f:\n",
    "    df_with_outcode = pickle.load(f)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df_with_outcode.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df_with_outcode.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureSet:\n",
    "    \"\"\"Container for a feature set configuration\"\"\"\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_val: pd.Series\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "class PreProcessor:\n",
    "    \"\"\"Handles initial data transformations and train/test splitting\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = RANDOM_STATE):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def prepare_pre_split_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates features that must be calculated before train/test split\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Log transform price\n",
    "        df_processed['log_price'] = np.log(df_processed['Price'])\n",
    "        \n",
    "        # Create price bands for stratification\n",
    "        df_processed['price_band'] = pd.qcut(df_processed['log_price'], q=10, labels=False)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def create_train_test_split(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Performs stratified train/test split using price bands\"\"\"\n",
    "        train_data, test_data = train_test_split(\n",
    "            df,\n",
    "            test_size=0.2,\n",
    "            stratify=df['price_band'],\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "print(\"PreProcessor class loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"Handles cross-validation and model evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, n_folds: int = 5, random_state: int = RANDOM_STATE):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.models = {\n",
    "            'decision_tree': DecisionTreeRegressor(random_state=random_state),\n",
    "            'random_forest': RandomForestRegressor(\n",
    "                n_estimators=100, \n",
    "                random_state=random_state\n",
    "            ),\n",
    "            'xgboost': XGBRegressor(\n",
    "                n_estimators=100, \n",
    "                random_state=random_state\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def evaluate_all_combinations(self, train_data: pd.DataFrame, test_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate all feature set and model combinations\"\"\"\n",
    "        results = []\n",
    "        encoder = FeatureEncoder()\n",
    "        \n",
    "        # K-fold cross-validation\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for fold_idx, (fold_train_idx, fold_val_idx) in enumerate(kf.split(train_data)):\n",
    "            # Get fold data\n",
    "            fold_train = train_data.iloc[fold_train_idx].copy()\n",
    "            fold_val = train_data.iloc[fold_val_idx].copy()\n",
    "            \n",
    "            # Mark as CV fold\n",
    "            fold_train['cv_fold'] = fold_idx\n",
    "            fold_val['cv_fold'] = fold_idx\n",
    "            \n",
    "            # Create features\n",
    "            feature_sets = encoder.create_fold_features(fold_train, fold_val)\n",
    "            \n",
    "            # Evaluate combinations\n",
    "            for feature_set in feature_sets:\n",
    "                for model_name, model in self.models.items():\n",
    "                    # Train and evaluate\n",
    "                    model.fit(feature_set.X_train, feature_set.y_train)\n",
    "                    fold_val_pred = model.predict(feature_set.X_val)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    results.append({\n",
    "                        'fold': fold_idx,\n",
    "                        'feature_set': feature_set.name,\n",
    "                        'description': feature_set.description,\n",
    "                        'model': model_name,\n",
    "                        'split_type': 'cv_fold',\n",
    "                        'rmse': self._calculate_rmse(feature_set.y_val, fold_val_pred),\n",
    "                        'r2': r2_score(feature_set.y_val, fold_val_pred),\n",
    "                        'mae': mean_absolute_error(\n",
    "                            np.exp(feature_set.y_val), \n",
    "                            np.exp(fold_val_pred)\n",
    "                        ),\n",
    "                        'pct_mae': np.mean(np.abs(\n",
    "                            (np.exp(feature_set.y_val) - np.exp(fold_val_pred)) / \n",
    "                            np.exp(feature_set.y_val)\n",
    "                        )) * 100,\n",
    "                        'n_features': feature_set.X_train.shape[1]\n",
    "                    })\n",
    "        \n",
    "        # Final evaluation on test set\n",
    "        train_data = train_data.drop('cv_fold', axis=1, errors='ignore')\n",
    "        final_feature_sets = encoder.create_fold_features(train_data, test_data)\n",
    "        \n",
    "        for feature_set in final_feature_sets:\n",
    "            for model_name, model in self.models.items():\n",
    "                model.fit(feature_set.X_train, feature_set.y_train)\n",
    "                test_pred = model.predict(feature_set.X_val)\n",
    "                \n",
    "                results.append({\n",
    "                    'fold': 'final',\n",
    "                    'feature_set': feature_set.name,\n",
    "                    'description': feature_set.description,\n",
    "                    'model': model_name,\n",
    "                    'split_type': 'test',\n",
    "                    'rmse': self._calculate_rmse(feature_set.y_val, test_pred),\n",
    "                    'r2': r2_score(feature_set.y_val, test_pred),\n",
    "                    'mae': mean_absolute_error(\n",
    "                        np.exp(feature_set.y_val), \n",
    "                        np.exp(test_pred)\n",
    "                    ),\n",
    "                    'pct_mae': np.mean(np.abs(\n",
    "                        (np.exp(feature_set.y_val) - np.exp(test_pred)) / \n",
    "                        np.exp(feature_set.y_val)\n",
    "                    )) * 100,\n",
    "                    'n_features': feature_set.X_train.shape[1]\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _calculate_rmse(self, y_true: pd.Series, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Root Mean Squared Error\"\"\"\n",
    "        return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "print(\"CrossValidator class loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model Comparison Pipeline\n",
    "\n",
    "Now let's run our comparison pipeline to evaluate different model and feature combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "preprocessor = PreProcessor()\n",
    "df_processed = preprocessor.prepare_pre_split_features(df_with_outcode)\n",
    "train_data, test_data = preprocessor.create_train_test_split(df_processed)\n",
    "\n",
    "validator = CrossValidator()\n",
    "results = validator.evaluate_all_combinations(train_data, test_data)\n",
    "\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Print summary statistics\n",
    "summary = results[results['split_type'] == 'cv_fold'].groupby(['feature_set', 'model']).agg({\n",
    "    'rmse': ['mean', 'std'],\n",
    "    'r2': ['mean', 'std'],\n",
    "    'mae': ['mean', 'std'],\n",
    "    'pct_mae': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nCross-validation Summary:\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Model Performance\n",
    "\n",
    "Let's create detailed visualizations to understand the performance characteristics of our models. We'll look at:\n",
    "\n",
    "1. Performance across feature sets\n",
    "2. Model stability through cross-validation\n",
    "3. Feature importance patterns\n",
    "4. Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "def plot_model_comparison(results_df):\n",
    "    \"\"\"Create comprehensive performance comparison plots\"\"\"\n",
    "    # Filter for CV results only\n",
    "    cv_results = results_df[results_df['split_type'] == 'cv_fold']\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. R² Score by model and feature set\n",
    "    sns.boxplot(x='model', y='r2', hue='feature_set', data=cv_results, ax=axes[0,0])\n",
    "    axes[0,0].set_title('R² Score Distribution')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. RMSE by model and feature set\n",
    "    sns.boxplot(x='model', y='rmse', hue='feature_set', data=cv_results, ax=axes[0,1])\n",
    "    axes[0,1].set_title('RMSE Distribution')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. MAE by model and feature set\n",
    "    sns.boxplot(x='model', y='mae', hue='feature_set', data=cv_results, ax=axes[1,0])\n",
    "    axes[1,0].set_title('MAE Distribution')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Percentage MAE by model and feature set\n",
    "    sns.boxplot(x='model', y='pct_mae', hue='feature_set', data=cv_results, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Percentage MAE Distribution')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot performance comparisons\n",
    "plot_model_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "The plots above reveal several key insights:\n",
    "\n",
    "1. Model Comparison\n",
    "   - XGBoost consistently shows the best performance\n",
    "   - Random Forests provide more stable results than single Decision Trees\n",
    "   - Decision Trees show higher variance across folds\n",
    "\n",
    "2. Feature Set Impact\n",
    "   - Full feature sets generally perform better\n",
    "   - Location features significantly improve predictions\n",
    "   - Basic features provide reasonable baseline performance\n",
    "\n",
    "3. Error Patterns\n",
    "   - Error distributions are right-skewed\n",
    "   - More complex models show tighter error bounds\n",
    "   - Feature engineering reduces error variance\n",
    "\n",
    "Let's examine feature importance across models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"Plot feature importance for a given model\"\"\"\n",
    "    importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "    importances = importances.sort_values(ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    importances.plot(kind='barh')\n",
    "    plt.title(f'Feature Importance - {title}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = results['feature_set'].iloc[0]\n",
    "\n",
    "# Plot feature importance for each model type\n",
    "for model_name, model in validator.models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plot_feature_importance(model, feature_names, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "The feature importance plots reveal:\n",
    "\n",
    "1. Common Important Features\n",
    "   - Area in sq ft consistently ranks high\n",
    "   - Location features show strong importance\n",
    "   - Price per square foot provides valuable signal\n",
    "\n",
    "2. Model-Specific Patterns\n",
    "   - XGBoost uses features more evenly\n",
    "   - Decision Trees focus on fewer key features\n",
    "   - Random Forests show balanced feature usage\n",
    "\n",
    "Let's examine the error distributions in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_analysis(results_df):\n",
    "    \"\"\"Create detailed error analysis plots\"\"\"\n",
    "    test_results = results_df[results_df['split_type'] == 'test']\n",
    "    \n",
    "    # Create violin plots of error distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(x='model', y='pct_mae', hue='feature_set', data=test_results)\n",
    "    plt.title('Error Distribution by Model and Feature Set')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed error statistics\n",
    "    error_stats = test_results.groupby(['model', 'feature_set'])['pct_mae'].describe()\n",
    "    display(error_stats)\n",
    "\n",
    "# Plot error analysis\n",
    "plot_error_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Guidelines\n",
    "\n",
    "Based on our comprehensive analysis, here are guidelines for model selection:\n",
    "\n",
    "1. For Maximum Performance\n",
    "   - Use XGBoost with full feature set\n",
    "   - Implement proper feature engineering\n",
    "   - Monitor for overfitting\n",
    "\n",
    "2. For Balance of Performance/Complexity\n",
    "   - Use Random Forests\n",
    "   - Focus on key features\n",
    "   - Simpler feature engineering\n",
    "\n",
    "3. For Maximum Interpretability\n",
    "   - Use Decision Trees\n",
    "   - Careful feature selection\n",
    "   - Accept some performance trade-off\n",
    "\n",
    "Let's implement the model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(results_df, criterion='r2'):\n",
    "    \"\"\"Select best model based on specified criterion\"\"\"\n",
    "    # Get test set results\n",
    "    test_results = results_df[results_df['split_type'] == 'test']\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = test_results[criterion].idxmax()\n",
    "    best_result = test_results.loc[best_idx]\n",
    "    \n",
    "    print(f\"Best Model Configuration:\")\n",
    "    print(f\"Model: {best_result['model']}\")\n",
    "    print(f\"Feature Set: {best_result['feature_set']}\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"R² Score: {best_result['r2']:.3f}\")\n",
    "    print(f\"RMSE: {best_result['rmse']:.3f}\")\n",
    "    print(f\"MAE: £{best_result['mae']:,.2f}\")\n",
    "    print(f\"Percentage MAE: {best_result['pct_mae']:.2f}%\")\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "# Select best model\n",
    "best_model = select_best_model(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Considerations\n",
    "\n",
    "When deploying the selected model to production, consider:\n",
    "\n",
    "1. Model Serialization\n",
    "   - Save model and preprocessing pipeline\n",
    "   - Version control all artifacts\n",
    "   - Document dependencies\n",
    "\n",
    "2. Inference Pipeline\n",
    "   - Implement feature engineering\n",
    "   - Add input validation\n",
    "   - Include error handling\n",
    "\n",
    "3. Monitoring Setup\n",
    "   - Track prediction quality\n",
    "   - Monitor feature distributions\n",
    "   - Alert on drift\n",
    "\n",
    "Let's implement a basic production pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionPipeline:\n",
    "    \"\"\"Production-ready inference pipeline\"\"\"\n",
    "    \n",
    "    def init(self, model, preprocessor):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "    def predict(self, features: pd.DataFrame) -> float:\n",
    "        \"\"\"Make prediction with validation and error handling\"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            self._validate_input(features)\n",
    "            \n",
    "            # Preprocess features\n",
    "            processed_features = self.preprocessor.transform(features)\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = self.model.predict(processed_features)\n",
    "            \n",
    "            # Convert log price back to actual price\n",
    "            return np.exp(prediction[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error making prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _validate_input(self, features):\n",
    "        \"\"\"Validate input features\"\"\"\n",
    "        required_features = {'Area in sq ft', 'No. of Bedrooms', 'Location'}\n",
    "        if missing := required_features - set(features.columns):\n",
    "            raise ValueError(f\"Missing required features: {missing}\")\n",
    "\n",
    "# Example usage:\n",
    "# pipeline = ProductionPipeline(best_model, preprocessor)\n",
    "# prediction = pipeline.predict(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this comprehensive comparison, we've:\n",
    "\n",
    "1. Evaluated Models\n",
    "   - Compared Decision Trees, Random Forests, and XGBoost\n",
    "   - Analyzed feature importance patterns\n",
    "   - Understood error distributions\n",
    "\n",
    "2. Explored Features\n",
    "   - Tested different feature combinations\n",
    "   - Evaluated feature engineering impact\n",
    "   - Identified key predictors\n",
    "\n",
    "3. Provided Guidelines\n",
    "   - Model selection criteria\n",
    "   - Production considerations\n",
    "   - Monitoring recommendations\n",
    "\n",
    "The beast pipeline provides a robust framework for model comparison and selection, ensuring thorough evaluation and informed decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
