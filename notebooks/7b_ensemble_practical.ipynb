{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7B: Ensemble Methods Practical",
    "",
    "Production ensemble implementations with XGBoost, LightGBM, and stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import xgboost as xgb",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier",
    "from sklearn.linear_model import LogisticRegression",
    "from sklearn.datasets import load_breast_cancer",
    "from sklearn.model_selection import train_test_split, GridSearchCV",
    "from sklearn.metrics import accuracy_score, classification_report",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - The Kaggle Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()",
    "X, y = data.data, data.target",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)",
    "xgb_model.fit(X_train, y_train)",
    "y_pred = xgb_model.predict(X_test)",
    "",
    "print(f'XGBoost Accuracy: {accuracy_score(y_test, y_pred):.3f}')",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),",
    "    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42))",
    "]",
    "",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())",
    "stack.fit(X_train, y_train)",
    "y_pred_stack = stack.predict(X_test)",
    "",
    "print(f'Stacking Accuracy: {accuracy_score(y_test, y_pred_stack):.3f}')",
    "print('\\n\u2705 Stacking combines best of all models!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {",
    "    'max_depth': [3, 5, 7],",
    "    'learning_rate': [0.01, 0.1, 0.3],",
    "    'n_estimators': [50, 100, 200]",
    "}",
    "",
    "grid = GridSearchCV(xgb.XGBClassifier(random_state=42), param_grid, cv=3, scoring='accuracy', n_jobs=-1)",
    "grid.fit(X_train[:1000], y_train[:1000])  # Subset for speed",
    "",
    "print(f'Best params: {grid.best_params_}')",
    "print(f'Best CV score: {grid.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "**Production Recommendations:**",
    "- **Quick baseline:** RandomForest",
    "- **Maximum accuracy:** XGBoost with tuning",
    "- **Kaggle competition:** Stacking ensemble",
    "- **Production:** XGBoost (fast inference, good accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}