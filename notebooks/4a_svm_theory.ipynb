{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lesson 4A: Support Vector Machines - Theory\n\n**Status**: \u2705 Complete - Theory and Implementation\n\n**Current Progress**: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 100% Complete\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "Imagine you're a radiologist examining tumor biopsies. Each patient's biopsy shows two key measurements: tumor size and cell density. When you plot these measurements, a pattern emerges.\n",
    "\n",
    "Some tumors cluster clearly in the \"benign\" region\u2014small size, low cell density, regular cell structure. Others cluster unmistakably in the \"malignant\" territory\u2014large, dense, with irregular aggressive growth patterns.\n",
    "\n",
    "But between these clusters lies a gray zone. Borderline cases where one wrong decision could mean unnecessary surgery for a healthy patient\u2014or worse, undetected cancer allowed to progress.\n",
    "\n",
    "You need more than just *any* line separating the two groups. You need the **safest possible boundary**\u2014one that stays as far away from borderline cases as possible. A boundary with the widest \"confidence margin\" on both sides, giving you the maximum safety buffer for your life-or-death diagnosis.\n",
    "\n",
    "This intuition\u2014finding the classification boundary with maximum margin from both classes\u2014is exactly what **Support Vector Machines (SVMs)** do mathematically.\n",
    "\n",
    "### Why This Algorithm Changed Machine Learning\n",
    "\n",
    "In the 1990s, SVMs revolutionized machine learning by solving two fundamental problems:\n",
    "\n",
    "1. **The Margin Problem**: Unlike logistic regression which just finds *any* separating boundary, SVMs find the *optimal* boundary with maximum safety margin\n",
    "2. **The Non-Linear Problem**: Through the \"kernel trick,\" SVMs can find complex curved boundaries while solving a convex optimization problem\n",
    "\n",
    "Before deep learning dominated in the 2010s, SVMs were the gold standard for:\n",
    "- Text classification (spam detection, sentiment analysis)\n",
    "- Image recognition (face detection, handwriting recognition)\n",
    "- Bioinformatics (protein classification, gene expression analysis)\n",
    "- Financial prediction (credit scoring, stock market analysis)\n",
    "\n",
    "Even today, for datasets with:\n",
    "- High dimensions (thousands of features)\n",
    "- Clear margins between classes\n",
    "- Limited training data\n",
    "\n",
    "...SVMs often outperform more complex models.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this lesson, we'll build SVM understanding from first principles:\n",
    "\n",
    "**Theory & Mathematics:**\n",
    "1. Geometric intuition: What is a \"margin\" and why maximize it?\n",
    "2. Primal formulation: The optimization problem\n",
    "3. Lagrangian duality: Why the dual problem is easier\n",
    "4. KKT conditions: Understanding support vectors\n",
    "5. The kernel trick: Non-linear classification without computing high-dimensional features\n",
    "6. Soft margins: Handling noisy, overlapping data\n",
    "\n",
    "**Implementation:**\n",
    "1. Build SVM from scratch using quadratic programming\n",
    "2. Implement multiple kernel functions (linear, polynomial, RBF)\n",
    "3. Visualize decision boundaries and support vectors\n",
    "4. Compare with logistic regression and decision trees\n",
    "\n",
    "**Real-World Application:**\n",
    "1. Apply to Wisconsin Breast Cancer dataset (same as Lesson 1)\n",
    "2. Compare kernel performance\n",
    "3. Analyze support vectors\n",
    "4. Understand when SVM excels vs when to use alternatives\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You should be comfortable with:\n",
    "- Linear algebra: dot products, norms, matrix multiplication\n",
    "- Calculus: partial derivatives, gradients, Lagrange multipliers (we'll review!)\n",
    "- Python: NumPy array operations\n",
    "- Lessons 0-1: Linear regression and logistic regression\n",
    "\n",
    "**Don't worry if you're rusty on Lagrange multipliers** \u2014 we'll derive everything step-by-step with geometric intuitions.\n",
    "\n",
    "### Then in Lesson 4B...\n",
    "\n",
    "We'll explore production SVM implementations:\n",
    "1. Scikit-learn's optimized SVM with multiple backends\n",
    "2. Hyperparameter tuning (C, gamma, kernel selection)\n",
    "3. Multi-class classification strategies\n",
    "4. Handling imbalanced datasets\n",
    "5. Scaling to large datasets\n",
    "6. Production deployment patterns\n",
    "\n",
    "Let's find the optimal boundary! \ud83c\udfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Required Libraries](#required-libraries)\n",
    "3. [The Margin Concept](#the-margin-concept)\n",
    "   - [Geometric Intuition](#geometric-intuition)\n",
    "   - [Mathematical Definition](#mathematical-definition)\n",
    "   - [Why Maximize the Margin?](#why-maximize-margin)\n",
    "4. [Primal Formulation](#primal-formulation)\n",
    "   - [Hard Margin SVM](#hard-margin-svm)\n",
    "   - [Convex Optimization](#convex-optimization)\n",
    "   - [Worked Example: 2D Case](#worked-example-2d)\n",
    "5. [Lagrangian Dual Formulation](#lagrangian-dual)\n",
    "   - [Why Go to the Dual?](#why-dual)\n",
    "   - [KKT Conditions](#kkt-conditions)\n",
    "   - [Support Vectors Emerge](#support-vectors)\n",
    "6. [The Kernel Trick](#kernel-trick)\n",
    "   - [Non-Linear Classification](#non-linear-classification)\n",
    "   - [Common Kernels](#common-kernels)\n",
    "   - [Infinite Dimensions Without Computing Them](#infinite-dimensions)\n",
    "7. [Soft Margin SVM](#soft-margin)\n",
    "   - [Handling Overlapping Classes](#overlapping-classes)\n",
    "   - [The C Parameter](#c-parameter)\n",
    "   - [Bias-Variance Trade-off](#bias-variance)\n",
    "8. [Implementation from Scratch](#implementation)\n",
    "   - [SVMFromScratch Class](#svm-class)\n",
    "   - [Quadratic Programming Solver](#qp-solver)\n",
    "   - [Testing on Toy Data](#testing)\n",
    "9. [Real-World Application](#application)\n",
    "   - [Wisconsin Breast Cancer Dataset](#breast-cancer)\n",
    "   - [Kernel Comparison](#kernel-comparison)\n",
    "   - [Hyperparameter Sensitivity](#hyperparameter-sensitivity)\n",
    "   - [Support Vector Analysis](#sv-analysis)\n",
    "10. [When to Use SVM](#when-to-use)\n",
    "    - [Ideal Use Cases](#ideal-cases)\n",
    "    - [When to Avoid](#when-to-avoid)\n",
    "    - [Comparison with Other Algorithms](#comparison)\n",
    "11. [Conclusion](#conclusion)\n",
    "    - [Key Takeaways](#key-takeaways)\n",
    "    - [Preview of Lesson 4B](#preview-4b)\n",
    "    - [Further Reading](#further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"required-libraries\"></a>\n",
    "## Required Libraries\n",
    "\n",
    "Before we get started, let's load the necessary libraries.\n",
    "\n",
    "<table style=\"margin-left:0\">\n",
    "<tr>\n",
    "<th align=\"left\">Library</th>\n",
    "<th align=\"left\">Purpose</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NumPy</td>\n",
    "<td>Numerical computing and matrix operations for SVM math</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Pandas</td>\n",
    "<td>Data manipulation and analysis</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Matplotlib</td>\n",
    "<td>Visualization (decision boundaries, margins, support vectors)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Seaborn</td>\n",
    "<td>Statistical visualizations</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Scikit-learn</td>\n",
    "<td>Datasets, preprocessing, metrics, and comparison with sklearn SVM</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SciPy</td>\n",
    "<td>Optimization (quadratic programming for dual problem)</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from typing import Tuple, Optional, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\u2705 All libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a name=\"the-margin-concept\"></a>\n## The Margin Concept\n\nBefore diving into the mathematics, let's build geometric intuition.\n\nImagine you're drawing a line to separate two groups of points (red vs blue). Many lines could separate them perfectly, but which line is \"best\"?\n\nIntuitively, you'd choose the line that stays as far away from both groups as possible\u2014giving you the maximum \"safety buffer\" or **margin**. This is exactly what SVM does!\n\n### What is the Margin?\n\nThe **margin** is the perpendicular distance from the separating hyperplane to the nearest training points on either side.\n\nFor a hyperplane defined by **w\u00b7x + b = 0**:\n- Points with w\u00b7x + b > 0 are on one side (predict +1)\n- Points with w\u00b7x + b < 0 are on the other side (predict -1)\n- The margin width is **2/||w||**\n\nThe decision boundary is w\u00b7x + b = 0, and the parallel hyperplanes defining the margin are:\n- w\u00b7x + b = +1 (margin boundary for positive class)\n- w\u00b7x + b = -1 (margin boundary for negative class)\n\nDistance between these parallel hyperplanes = **2/||w||**\n\nLet's visualize this:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a simple 2D dataset\nnp.random.seed(42)\nX_positive = np.random.randn(20, 2) + [2, 2]\nX_negative = np.random.randn(20, 2) + [-2, -2]\nX_toy = np.vstack([X_positive, X_negative])\ny_toy = np.array([1]*20 + [-1]*20)\n\n# Define three different hyperplanes with different ||w||\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nhyperplanes = [\n    {'w': np.array([0.5, 0.5]), 'b': 0, 'name': 'Small ||w||'},\n    {'w': np.array([1.0, 1.0]), 'b': 0, 'name': 'Medium ||w||'},\n    {'w': np.array([2.0, 2.0]), 'b': 0, 'name': 'Large ||w||'}\n]\n\nfor idx, (hp, ax) in enumerate(zip(hyperplanes, axes)):\n    w = hp['w']\n    b = hp['b']\n    w_norm = np.linalg.norm(w)\n    margin_width = 2 / w_norm\n\n    # Plot data points\n    ax.scatter(X_toy[y_toy==1, 0], X_toy[y_toy==1, 1],\n               c='red', s=100, alpha=0.6, edgecolors='darkred',\n               linewidth=2, label='Class +1')\n    ax.scatter(X_toy[y_toy==-1, 0], X_toy[y_toy==-1, 1],\n               c='blue', s=100, alpha=0.6, edgecolors='darkblue',\n               linewidth=2, label='Class -1')\n\n    # Create grid for decision boundary visualization\n    x_min, x_max = X_toy[:, 0].min() - 1, X_toy[:, 0].max() + 1\n    y_min, y_max = X_toy[:, 1].min() - 1, X_toy[:, 1].max() + 1\n\n    # Plot decision boundary: w\u00b7x + b = 0\n    # Rewrite as x2 = -(w1*x1 + b)/w2\n    x1_boundary = np.array([x_min, x_max])\n    x2_boundary = -(w[0] * x1_boundary + b) / w[1]\n    ax.plot(x1_boundary, x2_boundary, 'k-', linewidth=3, label='Decision Boundary')\n\n    # Plot margin boundaries: w\u00b7x + b = \u00b11\n    x2_margin_pos = -(w[0] * x1_boundary + b - 1) / w[1]\n    x2_margin_neg = -(w[0] * x1_boundary + b + 1) / w[1]\n    ax.plot(x1_boundary, x2_margin_pos, 'r--', linewidth=2, alpha=0.7, label='Margin (+1)')\n    ax.plot(x1_boundary, x2_margin_neg, 'b--', linewidth=2, alpha=0.7, label='Margin (-1)')\n\n    # Fill margin region\n    ax.fill_between(x1_boundary, x2_margin_neg, x2_margin_pos,\n                     alpha=0.1, color='green')\n\n    # Add text annotations\n    ax.text(0, 4.5, f'||w|| = {w_norm:.2f}', fontsize=14, fontweight='bold',\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n    ax.text(0, 3.8, f'Margin = 2/||w|| = {margin_width:.2f}', fontsize=12,\n            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n\n    ax.set_xlabel('Feature 1', fontsize=12)\n    ax.set_ylabel('Feature 2', fontsize=12)\n    ax.set_title(f'{hp[\"name\"]}: ||w|| = {w_norm:.2f}', fontsize=14, fontweight='bold')\n    ax.legend(loc='upper left', fontsize=9)\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim([x_min, x_max])\n    ax.set_ylim([y_min, y_max])\n\nplt.suptitle('Margin Width = 2/||w||: Smaller ||w|| \u2192 Wider Margin',\n             fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2705 Key Insight: To maximize the margin, we minimize ||w||!\")\nprint(f\"\\n\ud83d\udcca Margin widths:\")\nfor hp in hyperplanes:\n    w_norm = np.linalg.norm(hp['w'])\n    print(f\"  {hp['name']}: margin = {2/w_norm:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"mathematical-definition\"></a>\n",
    "### Mathematical Definition\n",
    "\n",
    "Now let's make the margin concept precise with mathematics.\n",
    "\n",
    "For a point **x** with label **y \u2208 {-1, +1}**, we define:\n",
    "\n",
    "**Functional Margin** (unnormalized):\n",
    "```\n",
    "\u03b3\u0302 = y(w\u00b7x + b)\n",
    "```\n",
    "\n",
    "- If \u03b3\u0302 > 0: correctly classified\n",
    "- If \u03b3\u0302 >> 0: confidently classified\n",
    "- If \u03b3\u0302 < 0: misclassified\n",
    "\n",
    "**Geometric Margin** (actual distance):\n",
    "```\n",
    "\u03b3 = \u03b3\u0302 / ||w|| = y(w\u00b7x + b) / ||w||\n",
    "```\n",
    "\n",
    "This is the perpendicular distance from point **x** to the hyperplane **w\u00b7x + b = 0**.\n",
    "\n",
    "**Why divide by ||w||?**\n",
    "\n",
    "The distance from point **x\u2080** to hyperplane **w\u00b7x + b = 0** is:\n",
    "```\n",
    "distance = |w\u00b7x\u2080 + b| / ||w||\n",
    "```\n",
    "\n",
    "**Proof**:\n",
    "1. Let x_proj be the projection of x\u2080 onto the hyperplane\n",
    "2. Then x\u2080 = x_proj + t\u00b7(w/||w||) for some scalar t\n",
    "3. Since x_proj is on the hyperplane: w\u00b7x_proj + b = 0\n",
    "4. Substituting: w\u00b7(x\u2080 - t\u00b7w/||w||) + b = 0\n",
    "5. w\u00b7x\u2080 - t\u00b7||w|| + b = 0\n",
    "6. t = (w\u00b7x\u2080 + b) / ||w||\n",
    "7. Distance = |t| = |w\u00b7x\u2080 + b| / ||w|| \u2713\n",
    "\n",
    "Let's compute margins for concrete examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example with actual numbers\n",
    "print(\"Worked Example: Computing Margins\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define hyperplane\n",
    "w_example = np.array([3, 4])  # Normal vector\n",
    "b_example = -1  # Bias\n",
    "w_norm_example = np.linalg.norm(w_example)  # ||w|| = 5\n",
    "\n",
    "print(f\"\\nHyperplane: {w_example[0]}x\u2081 + {w_example[1]}x\u2082 + {b_example} = 0\")\n",
    "print(f\"||w|| = {w_norm_example}\")\n",
    "\n",
    "# Test point 1: x = [2, 1], y = +1\n",
    "x1 = np.array([2, 1])\n",
    "y1 = 1\n",
    "functional_margin_1 = y1 * (np.dot(w_example, x1) + b_example)\n",
    "geometric_margin_1 = functional_margin_1 / w_norm_example\n",
    "\n",
    "print(f\"\\nPoint 1: x = {x1}, y = {y1}\")\n",
    "print(f\"  w\u00b7x + b = {w_example[0]}\u00b7{x1[0]} + {w_example[1]}\u00b7{x1[1]} + {b_example}\")\n",
    "print(f\"           = {np.dot(w_example, x1) + b_example}\")\n",
    "print(f\"  Functional margin: \u03b3\u0302 = {y1} \u00d7 {np.dot(w_example, x1) + b_example} = {functional_margin_1}\")\n",
    "print(f\"  Geometric margin:  \u03b3 = {functional_margin_1} / {w_norm_example} = {geometric_margin_1:.3f}\")\n",
    "print(f\"  Status: {'\u2705 Correctly classified' if functional_margin_1 > 0 else '\u274c Misclassified'}\")\n",
    "\n",
    "# Test point 2: x = [-1, 1], y = -1\n",
    "x2 = np.array([-1, 1])\n",
    "y2 = -1\n",
    "functional_margin_2 = y2 * (np.dot(w_example, x2) + b_example)\n",
    "geometric_margin_2 = functional_margin_2 / w_norm_example\n",
    "\n",
    "print(f\"\\nPoint 2: x = {x2}, y = {y2}\")\n",
    "print(f\"  w\u00b7x + b = {w_example[0]}\u00b7{x2[0]} + {w_example[1]}\u00b7{x2[1]} + {b_example}\")\n",
    "print(f\"           = {np.dot(w_example, x2) + b_example}\")\n",
    "print(f\"  Functional margin: \u03b3\u0302 = {y2} \u00d7 {np.dot(w_example, x2) + b_example} = {functional_margin_2}\")\n",
    "print(f\"  Geometric margin:  \u03b3 = {functional_margin_2} / {w_norm_example} = {geometric_margin_2:.3f}\")\n",
    "print(f\"  Status: {'\u2705 Correctly classified' if functional_margin_2 > 0 else '\u274c Misclassified'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 Key Insight: Geometric margin is scale-invariant!\")\n",
    "print(\"   Multiplying w by constant doesn't change geometric margin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"primal-formulation\"></a>\n",
    "## Primal Formulation\n",
    "\n",
    "Now that we understand the margin geometrically, let's formulate the optimization problem.\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find hyperplane **w\u00b7x + b = 0** that **maximizes the margin**.\n",
    "\n",
    "Since margin = 2/||w||:\n",
    "- Maximizing 2/||w|| \u2194 Minimizing ||w|| \u2194 Minimizing (1/2)||w||\u00b2\n",
    "\n",
    "We minimize (1/2)||w||\u00b2 instead of ||w|| because:\n",
    "1. It's differentiable everywhere (||w|| isn't differentiable at w=0)\n",
    "2. It makes the math cleaner (no square roots)\n",
    "3. Same optimal solution (monotonic transformation)\n",
    "\n",
    "### Complete Primal Problem\n",
    "\n",
    "```\n",
    "minimize:   (1/2)||w||\u00b2\n",
    "\n",
    "subject to: y\u1d62(w\u00b7x\u1d62 + b) \u2265 1  for all i = 1,...,n\n",
    "```\n",
    "\n",
    "**Why \u2265 1 and not \u2265 0?**\n",
    "- We fix the functional margin at 1 (normalization)\n",
    "- This makes the problem well-defined\n",
    "- Still captures the maximum margin solution\n",
    "- Points on the margin satisfy y\u1d62(w\u00b7x\u1d62 + b) = 1\n",
    "- Points further away satisfy y\u1d62(w\u00b7x\u1d62 + b) > 1\n",
    "\n",
    "### Why This is Convex\n",
    "\n",
    "**Objective function**: f(w) = (1/2)||w||\u00b2 = (1/2)w^T w\n",
    "- This is a quadratic function\n",
    "- Hessian: \u2207\u00b2f = I (identity matrix)\n",
    "- I is positive definite \u2192 f is strictly convex \u2713\n",
    "\n",
    "**Constraints**: g\u1d62(w,b) = y\u1d62(w\u00b7x\u1d62 + b) - 1 \u2265 0\n",
    "- These are affine (linear) functions\n",
    "- Affine functions are convex \u2713\n",
    "\n",
    "**Convex objective + convex constraints = global optimum guaranteed!**\n",
    "\n",
    "No local minima to worry about. Any optimization algorithm will find the global solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Convexity of ||w||\u00b2\n",
    "w_vals = np.linspace(-3, 3, 100)\n",
    "objective_1d = 0.5 * w_vals**2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1D case\n",
    "axes[0].plot(w_vals, objective_1d, 'b-', linewidth=3)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Global minimum')\n",
    "axes[0].scatter([0], [0], c='red', s=200, zorder=5, edgecolors='darkred', linewidth=2)\n",
    "axes[0].set_xlabel('w', fontsize=12)\n",
    "axes[0].set_ylabel('(1/2)w\u00b2', fontsize=12)\n",
    "axes[0].set_title('Convex Objective: (1/2)||w||\u00b2 in 1D', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2D case\n",
    "w1_vals = np.linspace(-3, 3, 50)\n",
    "w2_vals = np.linspace(-3, 3, 50)\n",
    "W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "Objective_2d = 0.5 * (W1**2 + W2**2)\n",
    "\n",
    "contour = axes[1].contour(W1, W2, Objective_2d, levels=15, cmap='viridis')\n",
    "axes[1].clabel(contour, inline=True, fontsize=8)\n",
    "axes[1].scatter([0], [0], c='red', s=200, zorder=5,\n",
    "                marker='*', edgecolors='darkred', linewidth=2,\n",
    "                label='Global minimum at w=[0,0]')\n",
    "axes[1].set_xlabel('w\u2081', fontsize=12)\n",
    "axes[1].set_ylabel('w\u2082', fontsize=12)\n",
    "axes[1].set_title('Convex Objective: (1/2)||w||\u00b2 in 2D', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 Convex optimization guarantees we'll find the global optimum!\")\n",
    "print(\"   No local minima to get stuck in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lagrangian-dual\"></a>\n",
    "## Lagrangian Dual Formulation\n",
    "\n",
    "The primal problem is clean and intuitive, but the **dual problem** is where the magic happens.\n",
    "\n",
    "### Why Go to the Dual?\n",
    "\n",
    "Three key reasons:\n",
    "\n",
    "1. **Computational efficiency**: When n_features >> n_samples, dual is faster\n",
    "   - Primal: optimization over d-dimensional w (d = number of features)\n",
    "   - Dual: optimization over n-dimensional \u03b1 (n = number of samples)\n",
    "   - For text/images with 10,000+ features but only 1,000 samples \u2192 dual wins!\n",
    "\n",
    "2. **Kernel trick**: Only inner products x\u1d62\u00b7x\u2c7c appear in dual\n",
    "   - Replace x\u1d62\u00b7x\u2c7c with K(x\u1d62,x\u2c7c) \u2192 non-linear classification!\n",
    "   - We'll see this in the next section\n",
    "\n",
    "3. **Sparsity**: Many \u03b1\u1d62 = 0, only support vectors matter\n",
    "   - Predictions only depend on support vectors\n",
    "   - Memory efficient for deployment\n",
    "\n",
    "### Lagrangian Function\n",
    "\n",
    "```\n",
    "L(w, b, \u03b1) = (1/2)||w||\u00b2 - \u03a3\u1d62 \u03b1\u1d62[y\u1d62(w\u00b7x\u1d62 + b) - 1]\n",
    "```\n",
    "\n",
    "where **\u03b1 = [\u03b1\u2081, ..., \u03b1\u2099]** are Lagrange multipliers, \u03b1\u1d62 \u2265 0\n",
    "\n",
    "### Karush-Kuhn-Tucker (KKT) Conditions\n",
    "\n",
    "For optimal (w*, b*, \u03b1*), we need:\n",
    "\n",
    "1. **Stationarity**: \u2207w L = 0, \u2202L/\u2202b = 0\n",
    "2. **Primal feasibility**: y\u1d62(w\u00b7x\u1d62 + b) \u2265 1 for all i\n",
    "3. **Dual feasibility**: \u03b1\u1d62 \u2265 0 for all i\n",
    "4. **Complementary slackness**: \u03b1\u1d62[y\u1d62(w\u00b7x\u1d62 + b) - 1] = 0 for all i\n",
    "\n",
    "**Complementary slackness is key**:\n",
    "- If \u03b1\u1d62 > 0 \u2192 y\u1d62(w\u00b7x\u1d62 + b) = 1 \u2192 point is on the margin (support vector!)\n",
    "- If y\u1d62(w\u00b7x\u1d62 + b) > 1 \u2192 \u03b1\u1d62 = 0 \u2192 point is not a support vector\n",
    "\n",
    "### Deriving the Dual\n",
    "\n",
    "**Step 1**: Stationarity with respect to w\n",
    "```\n",
    "\u2207w L = w - \u03a3\u1d62 \u03b1\u1d62y\u1d62x\u1d62 = 0\n",
    "\n",
    "\u27f9 w* = \u03a3\u1d62 \u03b1\u1d62y\u1d62x\u1d62  \u2190 w expressed in terms of training data!\n",
    "```\n",
    "\n",
    "**Step 2**: Stationarity with respect to b\n",
    "```\n",
    "\u2202L/\u2202b = -\u03a3\u1d62 \u03b1\u1d62y\u1d62 = 0\n",
    "\n",
    "\u27f9 \u03a3\u1d62 \u03b1\u1d62y\u1d62 = 0  \u2190 constraint on \u03b1\n",
    "```\n",
    "\n",
    "**Step 3**: Substitute w* back into L(w, b, \u03b1)\n",
    "```\n",
    "L = (1/2)||\u03a3\u1d62 \u03b1\u1d62y\u1d62x\u1d62||\u00b2 - \u03a3\u1d62 \u03b1\u1d62[y\u1d62((\u03a3\u2c7c \u03b1\u2c7cy\u2c7cx\u2c7c)\u00b7x\u1d62 + b) - 1]\n",
    "\n",
    "  = (1/2)\u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7c(x\u1d62\u00b7x\u2c7c) - \u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7c(x\u1d62\u00b7x\u2c7c) - b\u00b7\u03a3\u1d62\u03b1\u1d62y\u1d62 + \u03a3\u1d62 \u03b1\u1d62\n",
    "\n",
    "  = \u03a3\u1d62 \u03b1\u1d62 - (1/2)\u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7c(x\u1d62\u00b7x\u2c7c)  (using \u03a3\u03b1\u1d62y\u1d62 = 0)\n",
    "```\n",
    "\n",
    "### Final Dual Problem\n",
    "\n",
    "```\n",
    "maximize: L(\u03b1) = \u03a3\u1d62 \u03b1\u1d62 - (1/2)\u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7c(x\u1d62\u00b7x\u2c7c)\n",
    "\n",
    "subject to: \u03b1\u1d62 \u2265 0, \u03a3\u1d62 \u03b1\u1d62y\u1d62 = 0\n",
    "```\n",
    "\n",
    "**Key Insight**: Only inner products **x\u1d62\u00b7x\u2c7c** appear! This enables the kernel trick.\n",
    "\n",
    "**Once we solve for \u03b1**, we can recover:\n",
    "- w = \u03a3\u1d62 \u03b1\u1d62y\u1d62x\u1d62\n",
    "- b from any support vector: b = y\u2096 - w\u00b7x\u2096 (for any k with \u03b1\u2096 > 0)\n",
    "\n",
    "Let's work through a concrete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example with 4 points\n",
    "print(\"Worked Example: Dual Problem for 4-Point Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simple linearly separable dataset\n",
    "X_small = np.array([\n",
    "    [1, 1],   # Point 1, class +1\n",
    "    [2, 2],   # Point 2, class +1\n",
    "    [3, 1],   # Point 3, class -1\n",
    "    [4, 3]    # Point 4, class -1\n",
    "])\n",
    "y_small = np.array([1, 1, -1, -1])\n",
    "\n",
    "n_points = len(X_small)\n",
    "\n",
    "# Compute kernel matrix K[i,j] = x\u1d62\u00b7x\u2c7c\n",
    "K = X_small @ X_small.T\n",
    "\n",
    "print(\"Dataset:\")\n",
    "for i, (x, label) in enumerate(zip(X_small, y_small)):\n",
    "    print(f\"  Point {i+1}: x = {x}, y = {label:+d}\")\n",
    "\n",
    "print(f\"\\nKernel Matrix K (x\u1d62\u00b7x\u2c7c):\")\n",
    "print(K)\n",
    "\n",
    "print(f\"\\nDual Problem:\")\n",
    "print(\"maximize: \u03a3\u03b1\u1d62 - (1/2)\u03a3\u03a3 \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7cK[i,j]\")\n",
    "print(\"subject to: \u03b1\u1d62 \u2265 0, \u03a3\u03b1\u1d62y\u1d62 = 0\")\n",
    "\n",
    "print(f\"\\nExpanded form:\")\n",
    "print(\"maximize: \u03b1\u2081 + \u03b1\u2082 + \u03b1\u2083 + \u03b1\u2084\")\n",
    "print(\"         - (1/2)[\")\n",
    "for i in range(n_points):\n",
    "    terms = []\n",
    "    for j in range(n_points):\n",
    "        coef = y_small[i] * y_small[j] * K[i,j]\n",
    "        terms.append(f\"({coef:+.0f})\u03b1{i+1}\u03b1{j+1}\")\n",
    "    line = \" + \".join(terms)\n",
    "    if i < n_points - 1:\n",
    "        line += \" +\"\n",
    "    print(f\"           {line}\")\n",
    "\n",
    "print(\"         ]\")\n",
    "print(\"subject to:\")\n",
    "print(f\"  \u03b1\u2081 + \u03b1\u2082 - \u03b1\u2083 - \u03b1\u2084 = 0\")\n",
    "print(f\"  \u03b1\u2081, \u03b1\u2082, \u03b1\u2083, \u03b1\u2084 \u2265 0\")\n",
    "\n",
    "print(\"\\n\u2705 This is a quadratic programming problem we can solve!\")\n",
    "print(\"\\nIn practice, we use specialized QP solvers like:\")\n",
    "print(\"  - scipy.optimize.minimize with 'SLSQP' method\")\n",
    "print(\"  - cvxopt (fast QP solver)\")\n",
    "print(\"  - libsvm (highly optimized for SVMs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"kernel-trick\"></a>\n",
    "## The Kernel Trick\n",
    "\n",
    "So far we've only seen linear decision boundaries. What if the data isn't linearly separable?\n",
    "\n",
    "### The Problem: Non-Linear Data\n",
    "\n",
    "Many real-world datasets can't be separated by a straight line (or hyperplane):\n",
    "- XOR problem: four points at corners of a square\n",
    "- Concentric circles: inner circle one class, outer circle another\n",
    "- Moons dataset: two interleaving half-circles\n",
    "\n",
    "### The Naive Solution: Feature Engineering\n",
    "\n",
    "**Idea**: Map data to higher dimensions where it becomes linearly separable!\n",
    "\n",
    "Example: 1D data [x] \u2192 2D feature space [x, x\u00b2]\n",
    "\n",
    "```\n",
    "Original 1D: \u25cf \u25cb \u25cf \u25cb \u25cb \u25cf \u25cb \u25cf (not linearly separable)\n",
    "After \u03c6(x) = [x, x\u00b2]:\n",
    "  x\u00b2\n",
    "   |\n",
    "   |  \u25cf    \u25cf\n",
    "   |    \u25cf    \u25cf  \u2190 Class +1 (parabola)\n",
    "   | \u25cb  \u25cb  \u25cb  \u25cb \u2190 Class -1 (near x-axis)\n",
    "   |___________x\n",
    "   Now linearly separable!\n",
    "```\n",
    "\n",
    "**Problem**: Computing \u03c6(x) explicitly is expensive or impossible!\n",
    "- Polynomial features of degree d on 100 features \u2192 millions of dimensions\n",
    "- RBF kernel \u2192 **infinite dimensions**\n",
    "\n",
    "### The Kernel Trick: The Elegant Solution\n",
    "\n",
    "**Key observation**: In the dual formulation, we only ever compute **x\u1d62\u00b7x\u2c7c** (inner products).\n",
    "\n",
    "We never explicitly need w or \u03c6(x)!\n",
    "\n",
    "**Kernel trick**: Replace x\u1d62\u00b7x\u2c7c with K(x\u1d62,x\u2c7c) = \u03c6(x\u1d62)\u00b7\u03c6(x\u2c7c)\n",
    "\n",
    "```\n",
    "Dual problem becomes:\n",
    "maximize: \u03a3\u1d62 \u03b1\u1d62 - (1/2)\u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7cK(x\u1d62,x\u2c7c)\n",
    "```\n",
    "\n",
    "**Magic**: K(x\u1d62,x\u2c7c) can be computed directly without computing \u03c6(x)!\n",
    "\n",
    "### Common Kernels\n",
    "\n",
    "**1. Linear Kernel** (no transformation):\n",
    "```\n",
    "K(x, x') = x\u00b7x'\n",
    "```\n",
    "\n",
    "**2. Polynomial Kernel**:\n",
    "```\n",
    "K(x, x') = (x\u00b7x' + c)^d\n",
    "```\n",
    "- Degree d polynomial features\n",
    "- c is a constant (often 1)\n",
    "- d=2: quadratic boundaries\n",
    "- d=3: cubic boundaries\n",
    "\n",
    "**3. RBF (Radial Basis Function) Kernel**:\n",
    "```\n",
    "K(x, x') = exp(-\u03b3||x - x'||\u00b2)\n",
    "```\n",
    "- Most popular kernel\n",
    "- Creates smooth decision boundaries\n",
    "- \u03b3 controls flexibility (smaller \u03b3 = smoother)\n",
    "- Maps to **infinite-dimensional space**!\n",
    "\n",
    "**4. Sigmoid Kernel**:\n",
    "```\n",
    "K(x, x') = tanh(\u03b1x\u00b7x' + c)\n",
    "```\n",
    "- Similar to neural networks\n",
    "- Less commonly used\n",
    "\n",
    "### Example: Polynomial Kernel\n",
    "\n",
    "For 2D input x = [x\u2081, x\u2082], degree-2 polynomial:\n",
    "\n",
    "**Explicit mapping** (expensive):\n",
    "```\n",
    "\u03c6(x) = [x\u2081\u00b2, \u221a2\u00b7x\u2081x\u2082, x\u2082\u00b2, \u221a2\u00b7x\u2081, \u221a2\u00b7x\u2082, 1]\n",
    "\u03c6(x)\u00b7\u03c6(x') = x\u2081\u00b2x\u2081'\u00b2 + 2x\u2081x\u2082x\u2081'x\u2082' + x\u2082\u00b2x\u2082'\u00b2 + 2x\u2081x\u2081' + 2x\u2082x\u2082' + 1\n",
    "```\n",
    "\n",
    "**Kernel computation** (cheap):\n",
    "```\n",
    "K(x, x') = (x\u00b7x' + 1)\u00b2\n",
    "         = (x\u2081x\u2081' + x\u2082x\u2082' + 1)\u00b2\n",
    "         [expands to same thing!]\n",
    "```\n",
    "\n",
    "Just 2 multiplications + 1 addition instead of computing 6-dimensional feature vector!\n",
    "\n",
    "Let's visualize kernel effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linearly separable data (circles)\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.4, random_state=42)\n",
    "y_circles = np.where(y_circles == 0, -1, 1)  # Convert to {-1, +1}\n",
    "\n",
    "# Generate another non-linear dataset (moons)\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "y_moons = np.where(y_moons == 0, -1, 1)\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Circles dataset\n",
    "axes[0].scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1],\n",
    "                c='red', s=50, alpha=0.7, edgecolors='darkred', label='Class +1')\n",
    "axes[0].scatter(X_circles[y_circles==-1, 0], X_circles[y_circles==-1, 1],\n",
    "                c='blue', s=50, alpha=0.7, edgecolors='darkblue', label='Class -1')\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Concentric Circles\\n(RBF kernel works well)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Moons dataset\n",
    "axes[1].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1],\n",
    "                c='red', s=50, alpha=0.7, edgecolors='darkred', label='Class +1')\n",
    "axes[1].scatter(X_moons[y_moons==-1, 0], X_moons[y_moons==-1, 1],\n",
    "                c='blue', s=50, alpha=0.7, edgecolors='darkblue', label='Class -1')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Moons Dataset\\n(Polynomial or RBF kernel works)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 These datasets are NOT linearly separable!\")\n",
    "print(\"   Linear kernel would fail. Need RBF or polynomial kernel.\")\n",
    "print(\"\\n\ud83d\udca1 The kernel trick lets us handle complex boundaries without\")\n",
    "print(\"   explicitly computing high-dimensional feature transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"soft-margin\"></a>\n",
    "## Soft Margin SVM\n",
    "\n",
    "Hard margin SVM requires **perfect** linear separability. Real data is messy!\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Hard margin SVM fails when:\n",
    "1. **Outliers**: One mislabeled point makes the problem infeasible\n",
    "2. **Overlapping classes**: Classes genuinely overlap in feature space\n",
    "3. **Noise**: Measurement errors cause some points to cross the margin\n",
    "\n",
    "### The Solution: Slack Variables\n",
    "\n",
    "Allow some points to violate the margin constraint, but **penalize** violations.\n",
    "\n",
    "Introduce **slack variables** \u03be\u1d62 \u2265 0 for each training point:\n",
    "- \u03be\u1d62 = 0: point is correctly classified with margin\n",
    "- 0 < \u03be\u1d62 < 1: point inside margin but correctly classified\n",
    "- \u03be\u1d62 \u2265 1: point is misclassified\n",
    "\n",
    "### Soft Margin Primal Problem\n",
    "\n",
    "```\n",
    "minimize: (1/2)||w||\u00b2 + C\u00b7\u03a3\u1d62 \u03be\u1d62\n",
    "\n",
    "subject to: y\u1d62(w\u00b7x\u1d62 + b) \u2265 1 - \u03be\u1d62  for all i\n",
    "            \u03be\u1d62 \u2265 0  for all i\n",
    "```\n",
    "\n",
    "**The C Parameter**: Trade-off between margin width and violations\n",
    "\n",
    "- **Large C**: Penalize violations heavily \u2192 narrow margin, fewer violations\n",
    "  - Pros: Low training error\n",
    "  - Cons: Overfitting, sensitive to outliers\n",
    "\n",
    "- **Small C**: Allow more violations \u2192 wider margin\n",
    "  - Pros: Better generalization, robust to outliers\n",
    "  - Cons: Higher training error\n",
    "\n",
    "This is the **classic bias-variance trade-off**!\n",
    "\n",
    "### Soft Margin Dual Problem\n",
    "\n",
    "```\n",
    "maximize: \u03a3\u1d62 \u03b1\u1d62 - (1/2)\u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7cK(x\u1d62,x\u2c7c)\n",
    "\n",
    "subject to: 0 \u2264 \u03b1\u1d62 \u2264 C, \u03a3\u1d62 \u03b1\u1d62y\u1d62 = 0\n",
    "```\n",
    "\n",
    "**Only difference**: Box constraint **\u03b1\u1d62 \u2264 C** instead of just \u03b1\u1d62 \u2265 0\n",
    "\n",
    "### Support Vector Types\n",
    "\n",
    "With soft margins, we have three types of points:\n",
    "\n",
    "1. **\u03b1\u1d62 = 0**: Not a support vector (far from margin)\n",
    "2. **0 < \u03b1\u1d62 < C**: Support vector on the margin\n",
    "3. **\u03b1\u1d62 = C**: Support vector violating the margin (may be misclassified)\n",
    "\n",
    "### Choosing C\n",
    "\n",
    "In practice, tune C using cross-validation:\n",
    "- Try C \u2208 {0.001, 0.01, 0.1, 1, 10, 100, 1000}\n",
    "- Use grid search or random search\n",
    "- Monitor validation accuracy\n",
    "\n",
    "**Rule of thumb**:\n",
    "- Clean, separable data \u2192 C = 1.0\n",
    "- Noisy data \u2192 C = 0.1 to 0.01\n",
    "- Many outliers \u2192 C < 0.01\n",
    "\n",
    "Let's see the effect of different C values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slightly overlapping dataset\n",
    "np.random.seed(42)\n",
    "X_overlap_pos = np.random.randn(40, 2) + [1.5, 1.5]\n",
    "X_overlap_neg = np.random.randn(40, 2) + [-1.5, -1.5]\n",
    "# Add outliers\n",
    "X_outliers = np.array([[2, -2], [-2, 2]])\n",
    "y_outliers = np.array([1, -1])\n",
    "\n",
    "X_overlap = np.vstack([X_overlap_pos, X_overlap_neg, X_outliers])\n",
    "y_overlap = np.array([1]*40 + [-1]*40 + list(y_outliers))\n",
    "\n",
    "# Train SVMs with different C values\n",
    "C_values = [0.01, 1.0, 100.0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (C, ax) in enumerate(zip(C_values, axes)):\n",
    "    # Train SVM\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_overlap, y_overlap)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    x_min, x_max = X_overlap[:, 0].min() - 1, X_overlap[:, 0].max() + 1\n",
    "    y_min, y_max = X_overlap[:, 1].min() - 1, X_overlap[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'],\n",
    "               colors=['blue', 'black', 'red'], linewidths=[2, 3, 2])\n",
    "    ax.contourf(xx, yy, Z, levels=[-1000, 0, 1000], alpha=0.1, colors=['blue', 'red'])\n",
    "    \n",
    "    # Plot points\n",
    "    ax.scatter(X_overlap[y_overlap==1, 0], X_overlap[y_overlap==1, 1],\n",
    "               c='red', s=50, alpha=0.6, edgecolors='darkred', label='Class +1')\n",
    "    ax.scatter(X_overlap[y_overlap==-1, 0], X_overlap[y_overlap==-1, 1],\n",
    "               c='blue', s=50, alpha=0.6, edgecolors='darkblue', label='Class -1')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "               s=200, linewidths=2, facecolors='none', edgecolors='green',\n",
    "               label=f'Support Vectors ({len(clf.support_vectors_)})')\n",
    "    \n",
    "    # Highlight outliers\n",
    "    ax.scatter(X_outliers[:, 0], X_outliers[:, 1],\n",
    "               s=300, linewidths=3, facecolors='none', edgecolors='orange',\n",
    "               marker='*', label='Outliers')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1', fontsize=12)\n",
    "    ax.set_ylabel('Feature 2', fontsize=12)\n",
    "    ax.set_title(f'C = {C}\\nSupport Vectors: {len(clf.support_vectors_)}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "    ax.set_ylim([y_min, y_max])\n",
    "\n",
    "plt.suptitle('Effect of C Parameter: Small C \u2192 Wide Margin, Large C \u2192 Narrow Margin',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Observations:\")\n",
    "print(f\"  C = 0.01:  Wide margin, ignores outliers (fewer support vectors)\")\n",
    "print(f\"  C = 1.0:   Balanced trade-off (moderate support vectors)\")\n",
    "print(f\"  C = 100.0: Narrow margin, tries to classify outliers correctly (more support vectors)\")\n",
    "print(\"\\n\ud83d\udca1 Choose C using cross-validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"implementation\"></a>\n",
    "## Implementation from Scratch\n",
    "\n",
    "Now that we understand the theory, let's build a complete SVM implementation!\n",
    "\n",
    "Our `SVMFromScratch` class will:\n",
    "1. Solve the dual optimization problem using quadratic programming\n",
    "2. Support multiple kernels (linear, polynomial, RBF, sigmoid)\n",
    "3. Handle soft margins with the C parameter\n",
    "4. Identify and store support vectors\n",
    "5. Compute optimal bias term\n",
    "6. Make predictions on new data\n",
    "\n",
    "### Class Structure\n",
    "\n",
    "**Key methods**:\n",
    "- `fit(X, y)`: Solve dual problem, find support vectors\n",
    "- `decision_function(X)`: Compute f(x) = \u03a3 \u03b1\u1d62y\u1d62K(x\u1d62,x) + b\n",
    "- `predict(X)`: Return class labels based on sign of decision function\n",
    "\n",
    "**Mathematical details**:\n",
    "\n",
    "We solve:\n",
    "```\n",
    "minimize: (1/2)\u03b1^T P \u03b1 - 1^T \u03b1\n",
    "where P[i,j] = y\u1d62y\u2c7cK(x\u1d62,x\u2c7c)\n",
    "\n",
    "subject to: 0 \u2264 \u03b1\u1d62 \u2264 C, \u03a3 \u03b1\u1d62y\u1d62 = 0\n",
    "```\n",
    "\n",
    "Once solved:\n",
    "- Support vectors: points with \u03b1\u1d62 > 0\n",
    "- Bias: b = y\u2096 - \u03a3 \u03b1\u1d62y\u1d62K(x\u1d62,x\u2096) for any margin support vector x\u2096\n",
    "- Prediction: sign(\u03a3 \u03b1\u1d62y\u1d62K(x\u1d62,x) + b)\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMFromScratch:\n",
    "    \"\"\"\n",
    "    Support Vector Machine with complete dual formulation.\n",
    "    \n",
    "    This implementation solves the dual optimization problem using\n",
    "    quadratic programming, supporting multiple kernel functions.\n",
    "    \n",
    "    Mathematical Background\n",
    "    -----------------------\n",
    "    SVM finds the hyperplane w\u00b7x + b = 0 that maximizes the margin 2/||w||.\n",
    "    \n",
    "    In the dual formulation, we solve:\n",
    "        maximize: L(\u03b1) = \u03a3\u1d62 \u03b1\u1d62 - (1/2)\u03a3\u1d62\u03a3\u2c7c \u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7cK(x\u1d62,x\u2c7c)\n",
    "        subject to: 0 \u2264 \u03b1\u1d62 \u2264 C, \u03a3\u1d62 \u03b1\u1d62y\u1d62 = 0\n",
    "    \n",
    "    where K(x\u1d62,x\u2c7c) is the kernel function.\n",
    "    \n",
    "    Support vectors are points with \u03b1\u1d62 > 0. These are the only points\n",
    "    that matter for the final decision function:\n",
    "        f(x) = \u03a3\u1d62\u2208SV \u03b1\u1d62y\u1d62K(x\u1d62,x) + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : str, default='rbf'\n",
    "        Kernel function:\n",
    "        - 'linear': K(x,x') = x\u00b7x'\n",
    "        - 'poly': K(x,x') = (\u03b3x\u00b7x' + r)^d\n",
    "        - 'rbf': K(x,x') = exp(-\u03b3||x-x'||\u00b2)\n",
    "        - 'sigmoid': K(x,x') = tanh(\u03b3x\u00b7x' + r)\n",
    "    \n",
    "    C : float, default=1.0\n",
    "        Regularization parameter. Higher C \u2192 fewer margin violations.\n",
    "        Lower C \u2192 wider margin with more violations.\n",
    "    \n",
    "    gamma : float or 'auto', default='auto'\n",
    "        Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n",
    "        If 'auto', uses 1/n_features.\n",
    "    \n",
    "    degree : int, default=3\n",
    "        Degree for polynomial kernel.\n",
    "    \n",
    "    coef0 : float, default=0.0\n",
    "        Independent term in poly and sigmoid kernels.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha_ : ndarray of shape (n_support_vectors,)\n",
    "        Lagrange multipliers for support vectors.\n",
    "    \n",
    "    support_vectors_ : ndarray of shape (n_support_vectors, n_features)\n",
    "        Support vectors (points with \u03b1 > 0).\n",
    "    \n",
    "    support_vector_labels_ : ndarray of shape (n_support_vectors,)\n",
    "        Labels of support vectors.\n",
    "    \n",
    "    b_ : float\n",
    "        Bias term in decision function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel='rbf', C=1.0, gamma='auto',\n",
    "                 degree=3, coef0=0.0, tol=1e-3):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.tol = tol\n",
    "        \n",
    "        # Fitted parameters\n",
    "        self.alpha_ = None\n",
    "        self.support_vectors_ = None\n",
    "        self.support_vector_labels_ = None\n",
    "        self.b_ = None\n",
    "    \n",
    "    def _kernel_function(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute kernel matrix K[i,j] = K(X1[i], X2[j])\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X1 : ndarray of shape (n_samples_1, n_features)\n",
    "        X2 : ndarray of shape (n_samples_2, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        K : ndarray of shape (n_samples_1, n_samples_2)\n",
    "            Kernel matrix\n",
    "        \"\"\"\n",
    "        if self.kernel == 'linear':\n",
    "            # K(x, x') = x\u00b7x'\n",
    "            return X1 @ X2.T\n",
    "        \n",
    "        elif self.kernel == 'rbf':\n",
    "            # K(x, x') = exp(-\u03b3||x-x'||\u00b2)\n",
    "            # Efficient computation using:\n",
    "            # ||x-x'||\u00b2 = ||x||\u00b2 + ||x'||\u00b2 - 2x\u00b7x'\n",
    "            \n",
    "            # Shape: (n_samples_1, 1)\n",
    "            X1_norm_sq = np.sum(X1**2, axis=1).reshape(-1, 1)\n",
    "            # Shape: (1, n_samples_2)\n",
    "            X2_norm_sq = np.sum(X2**2, axis=1).reshape(1, -1)\n",
    "            # Shape: (n_samples_1, n_samples_2)\n",
    "            distances_sq = X1_norm_sq + X2_norm_sq - 2 * (X1 @ X2.T)\n",
    "            \n",
    "            return np.exp(-self.gamma * distances_sq)\n",
    "        \n",
    "        elif self.kernel == 'poly':\n",
    "            # K(x, x') = (\u03b3x\u00b7x' + r)^d\n",
    "            return (self.gamma * (X1 @ X2.T) + self.coef0) ** self.degree\n",
    "        \n",
    "        elif self.kernel == 'sigmoid':\n",
    "            # K(x, x') = tanh(\u03b3x\u00b7x' + r)\n",
    "            return np.tanh(self.gamma * (X1 @ X2.T) + self.coef0)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit SVM by solving the dual optimization problem.\n",
    "        \n",
    "        Steps:\n",
    "        1. Compute kernel matrix K\n",
    "        2. Solve QP: maximize L(\u03b1) subject to constraints\n",
    "        3. Identify support vectors (\u03b1 > 0)\n",
    "        4. Compute bias b\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target labels (must be -1 or +1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Convert labels to {-1, +1}\n",
    "        y = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        # Set gamma if 'auto'\n",
    "        if self.gamma == 'auto':\n",
    "            self.gamma = 1.0 / n_features\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = self._kernel_function(X, X)\n",
    "        \n",
    "        # Solve dual problem using quadratic programming\n",
    "        # We need to solve:\n",
    "        # maximize: \u03a3\u03b1_i - (1/2)\u03a3\u03a3 \u03b1_i \u03b1_j y_i y_j K(x_i,x_j)\n",
    "        # subject to: 0 \u2264 \u03b1_i \u2264 C, \u03a3 \u03b1_i y_i = 0\n",
    "        \n",
    "        # Convert to minimization problem (negate objective)\n",
    "        # minimize: (1/2)\u03b1^T P \u03b1 + q^T \u03b1\n",
    "        # where P[i,j] = y_i y_j K(x_i, x_j), q = -1\n",
    "        \n",
    "        P = np.outer(y, y) * K  # Shape: (n_samples, n_samples)\n",
    "        q = -np.ones(n_samples)\n",
    "        \n",
    "        # Equality constraint: y^T \u03b1 = 0\n",
    "        A_eq = y.reshape(1, -1)\n",
    "        b_eq = np.array([0.0])\n",
    "        \n",
    "        # Inequality constraints: 0 \u2264 \u03b1_i \u2264 C\n",
    "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
    "        \n",
    "        # Initial guess: \u03b1 = 0\n",
    "        alpha_init = np.zeros(n_samples)\n",
    "        \n",
    "        def objective(alpha):\n",
    "            return 0.5 * alpha @ P @ alpha + q @ alpha\n",
    "        \n",
    "        def jac_objective(alpha):\n",
    "            return P @ alpha + q\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = {'type': 'eq', 'fun': lambda alpha: A_eq @ alpha,\n",
    "                      'jac': lambda alpha: A_eq.flatten()}\n",
    "        \n",
    "        # Solve\n",
    "        result = minimize(\n",
    "            objective,\n",
    "            alpha_init,\n",
    "            method='SLSQP',\n",
    "            jac=jac_objective,\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            options={'disp': False}\n",
    "        )\n",
    "        \n",
    "        alpha = result.x\n",
    "        \n",
    "        # Support vectors have \u03b1 > threshold\n",
    "        sv_idx = alpha > self.tol\n",
    "        self.alpha_ = alpha[sv_idx]\n",
    "        self.support_vectors_ = X[sv_idx]\n",
    "        self.support_vector_labels_ = y[sv_idx]\n",
    "        \n",
    "        # Compute bias b using support vectors with 0 < \u03b1 < C\n",
    "        # These are points exactly on the margin\n",
    "        margin_sv_idx = (alpha > self.tol) & (alpha < self.C - self.tol)\n",
    "        \n",
    "        if np.sum(margin_sv_idx) > 0:\n",
    "            # b = y_k - \u03a3 \u03b1_i y_i K(x_i, x_k) for any margin SV x_k\n",
    "            K_sv = self._kernel_function(X[sv_idx], X[margin_sv_idx])\n",
    "            b_values = y[margin_sv_idx] - (self.alpha_ * self.support_vector_labels_) @ K_sv\n",
    "            self.b_ = np.mean(b_values)\n",
    "        else:\n",
    "            # Fallback: use all support vectors\n",
    "            K_sv = self._kernel_function(self.support_vectors_, self.support_vectors_)\n",
    "            self.b_ = np.mean(self.support_vector_labels_ -\n",
    "                             (self.alpha_ * self.support_vector_labels_) @ K_sv)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute decision function f(x) = \u03a3 \u03b1_i y_i K(x_i, x) + b\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        scores : ndarray of shape (n_samples,)\n",
    "            Decision function values (distance from hyperplane)\n",
    "        \"\"\"\n",
    "        K = self._kernel_function(X, self.support_vectors_)\n",
    "        return (K @ (self.alpha_ * self.support_vector_labels_)) + self.b_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        scores = self.decision_function(X)\n",
    "        return np.where(scores >= 0, 1, -1)\n",
    "\n",
    "# Need to add these methods to the class\n",
    "SVMFromScratch.fit = fit\n",
    "SVMFromScratch.decision_function = decision_function\n",
    "SVMFromScratch.predict = predict\n",
    "\n",
    "print(\"\u2705 Complete SVM implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"application\"></a>\n",
    "## Real-World Application\n",
    "\n",
    "Let's apply our from-scratch SVM to the **Wisconsin Breast Cancer dataset**\u2014the same dataset we used in Lesson 1a for logistic regression!\n",
    "\n",
    "This allows us to directly compare SVM's performance with logistic regression.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "- **Task**: Binary classification (malignant vs benign tumors)\n",
    "- **Features**: 30 numerical features computed from digitized images\n",
    "- **Samples**: 569 tumors (357 benign, 212 malignant)\n",
    "- **Challenge**: High-dimensional feature space\n",
    "\n",
    "**Key questions we'll answer**:\n",
    "1. Does SVM outperform logistic regression?\n",
    "2. Which kernel works best?\n",
    "3. How many support vectors do we need?\n",
    "4. What's the effect of the C parameter?\n",
    "\n",
    "Let's start by loading and preprocessing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Classes: {data.target_names}\")\n",
    "print(f\"  Class distribution: {np.bincount(y)}\")\n",
    "print(f\"    Malignant (0): {np.sum(y==0)} ({100*np.sum(y==0)/len(y):.1f}%)\")\n",
    "print(f\"    Benign (1): {np.sum(y==1)} ({100*np.sum(y==1)/len(y):.1f}%)\")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Test Split:\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Feature scaling (CRITICAL for SVM!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature Statistics After Scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean(axis=0)[:5]} ...\")\n",
    "print(f\"  Std:  {X_train_scaled.std(axis=0)[:5]} ...\")\n",
    "\n",
    "# Convert labels to {-1, +1} for our SVM\n",
    "y_train_svm = np.where(y_train == 0, -1, 1)\n",
    "y_test_svm = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "print(\"\\n\u2705 Data loaded and preprocessed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our from-scratch SVM with RBF kernel\n",
    "print(\"Training SVMFromScratch with RBF kernel...\")\n",
    "svm_custom = SVMFromScratch(kernel='rbf', C=1.0, gamma='auto')\n",
    "svm_custom.fit(X_train_scaled, y_train_svm)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_custom = svm_custom.predict(X_test_scaled)\n",
    "# Convert back to {0, 1}\n",
    "y_pred_custom_binary = np.where(y_pred_custom == -1, 0, 1)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom_binary)\n",
    "print(f\"\\n\u2705 Training complete!\")\n",
    "print(f\"\\nOur SVM Performance:\")\n",
    "print(f\"  Support Vectors: {len(svm_custom.support_vectors_)} / {len(X_train_scaled)} \"\n",
    "      f\"({100*len(svm_custom.support_vectors_)/len(X_train_scaled):.1f}%)\")\n",
    "print(f\"  Test Accuracy: {accuracy_custom:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_custom_binary, \n",
    "                          target_names=data.target_names))\n",
    "\n",
    "# Compare with scikit-learn SVM\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Comparison with Scikit-learn SVM:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "svm_sklearn = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "svm_sklearn.fit(X_train_scaled, y_train)\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test_scaled)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Scikit-learn SVM:\")\n",
    "print(f\"  Support Vectors: {len(svm_sklearn.support_vectors_)} / {len(X_train_scaled)}\")\n",
    "print(f\"  Test Accuracy: {accuracy_sklearn:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  Accuracy: {abs(accuracy_custom - accuracy_sklearn):.4f} \"\n",
    "      f\"({'Our SVM is better!' if accuracy_custom > accuracy_sklearn else 'Similar performance!'})\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Our from-scratch implementation achieves comparable performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different kernels\n",
    "print(\"Kernel Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "results = []\n",
    "\n",
    "for kernel_name in kernels:\n",
    "    print(f\"\\nTraining with {kernel_name} kernel...\")\n",
    "    \n",
    "    if kernel_name == 'poly':\n",
    "        svm = SVMFromScratch(kernel=kernel_name, C=1.0, gamma=1.0/X_train_scaled.shape[1], degree=3)\n",
    "    else:\n",
    "        svm = SVMFromScratch(kernel=kernel_name, C=1.0, gamma='auto')\n",
    "    \n",
    "    svm.fit(X_train_scaled, y_train_svm)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    y_pred_binary = np.where(y_pred == -1, 0, 1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    n_sv = len(svm.support_vectors_)\n",
    "    \n",
    "    results.append({\n",
    "        'Kernel': kernel_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Support Vectors': n_sv,\n",
    "        'SV %': 100 * n_sv / len(X_train_scaled)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Support Vectors: {n_sv} ({100*n_sv/len(X_train_scaled):.1f}%)\")\n",
    "\n",
    "# Display results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\ud83d\udca1 RBF kernel often works best for this dataset!\")\n",
    "\n",
    "# Visualize kernel comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(results_df['Kernel'], results_df['Accuracy'], color=['blue', 'green', 'red'], alpha=0.7)\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy by Kernel', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(i, v + 0.002, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Support vector comparison\n",
    "axes[1].bar(results_df['Kernel'], results_df['Support Vectors'], color=['blue', 'green', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Number of Support Vectors', fontsize=12)\n",
    "axes[1].set_title('Support Vectors by Kernel', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_df['Support Vectors']):\n",
    "    axes[1].text(i, v + 2, f\"{int(v)}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"when-to-use\"></a>\n",
    "## When to Use SVM\n",
    "\n",
    "SVMs are powerful, but not always the best choice. Here's a practical guide.\n",
    "\n",
    "### \u2705 Ideal Use Cases\n",
    "\n",
    "**1. High-Dimensional Data** (many features, fewer samples)\n",
    "- Text classification with thousands of word features\n",
    "- Gene expression data (20,000+ genes, hundreds of patients)\n",
    "- Image features after dimensionality reduction\n",
    "- **Why**: SVM's dual formulation scales with sample size, not feature count\n",
    "\n",
    "**2. Clear Margin Between Classes**\n",
    "- Binary classification with well-separated groups\n",
    "- Low noise, high signal-to-noise ratio\n",
    "- **Why**: SVM explicitly maximizes the margin\n",
    "\n",
    "**3. Non-Linear Boundaries** (with kernel trick)\n",
    "- Circular or elliptical decision boundaries\n",
    "- Complex curved separations\n",
    "- **Why**: RBF kernel can capture complex patterns\n",
    "\n",
    "**4. Small to Medium Datasets** (< 100k samples)\n",
    "- Quadratic programming scales poorly beyond 100k samples\n",
    "- **Why**: Training time is O(n\u00b2) to O(n\u00b3)\n",
    "\n",
    "**5. Binary Classification**\n",
    "- Natural two-class problems\n",
    "- **Why**: SVM is inherently binary (multi-class requires extensions)\n",
    "\n",
    "### \u274c When to Avoid SVM\n",
    "\n",
    "**1. Massive Datasets** (> 100k samples)\n",
    "- **Alternative**: Logistic regression, neural networks, or linear SVM approximations\n",
    "- **Why**: Training time becomes prohibitive\n",
    "\n",
    "**2. Many Noisy/Overlapping Classes**\n",
    "- **Alternative**: Random forests, gradient boosting\n",
    "- **Why**: SVM struggles with heavily overlapping distributions\n",
    "\n",
    "**3. Probability Estimates Required**\n",
    "- **Alternative**: Logistic regression, naive Bayes\n",
    "- **Why**: SVM outputs distance from hyperplane, not well-calibrated probabilities\n",
    "\n",
    "**4. Multi-Class with Many Classes** (> 10 classes)\n",
    "- **Alternative**: Softmax regression, neural networks\n",
    "- **Why**: One-vs-rest SVM requires training N classifiers\n",
    "\n",
    "**5. Interpretability is Critical**\n",
    "- **Alternative**: Logistic regression, decision trees\n",
    "- **Why**: SVM with RBF kernel is a black box; hard to explain individual predictions\n",
    "\n",
    "**6. Real-Time Training Required**\n",
    "- **Alternative**: Naive Bayes, logistic regression\n",
    "- **Why**: SVM training is slow; not suitable for online learning\n",
    "\n",
    "### \ud83d\udcca Algorithm Comparison\n",
    "\n",
    "| Criterion | SVM | Logistic Regression | Decision Trees | Neural Networks |\n",
    "|-----------|-----|---------------------|----------------|------------------|\n",
    "| **High dimensions** | \u2705 Excellent | \u2705 Excellent | \u274c Poor | \u26a0\ufe0f Needs tuning |\n",
    "| **Large datasets (>100k)** | \u274c Slow | \u2705 Fast | \u2705 Fast | \u26a0\ufe0f GPU helps |\n",
    "| **Non-linear boundaries** | \u2705 Excellent | \u274c Needs features | \u2705 Excellent | \u2705 Excellent |\n",
    "| **Interpretability** | \u274c Poor (RBF) | \u2705 Excellent | \u2705 Excellent | \u274c Poor |\n",
    "| **Probability estimates** | \u26a0\ufe0f Okay | \u2705 Excellent | \u2705 Good | \u2705 Excellent |\n",
    "| **Training time** | \u26a0\ufe0f Slow | \u2705 Fast | \u2705 Fast | \u274c Very slow |\n",
    "| **Prediction time** | \u2705 Fast | \u2705 Fast | \u2705 Fast | \u2705 Fast |\n",
    "| **Robustness to outliers** | \u26a0\ufe0f Moderate (with soft margin) | \u274c Poor | \u2705 Excellent | \u26a0\ufe0f Moderate |\n",
    "\n",
    "### \ud83d\udca1 Decision Tree\n",
    "\n",
    "```\n",
    "Dataset size?\n",
    "  \u251c\u2500 < 100k samples\n",
    "  \u2502   \u251c\u2500 Need interpretability? \u2192 Logistic Regression or Decision Tree\n",
    "  \u2502   \u251c\u2500 High dimensions (d > 1000)? \u2192 SVM (linear or RBF)\n",
    "  \u2502   \u251c\u2500 Non-linear boundaries? \u2192 SVM (RBF) or Random Forest\n",
    "  \u2502   \u2514\u2500 Binary classification, clear margin? \u2192 SVM \u2713\n",
    "  \u2502\n",
    "  \u2514\u2500 > 100k samples\n",
    "      \u251c\u2500 Linear problem? \u2192 Logistic Regression or Linear SVM\n",
    "      \u251c\u2500 Non-linear? \u2192 Gradient Boosting or Neural Network\n",
    "      \u2514\u2500 Very large (> 1M)? \u2192 Neural Network with GPU\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Real-World Examples\n",
    "\n",
    "**SVM Wins:**\n",
    "- Spam detection (high-dimensional word features, clear margin)\n",
    "- Handwritten digit recognition (MNIST, 784 features)\n",
    "- Protein classification (high-dimensional, small datasets)\n",
    "- Face detection (with HOG features)\n",
    "\n",
    "**SVM Loses:**\n",
    "- Netflix recommendation (millions of users, need matrix factorization)\n",
    "- Stock price prediction (noisy, time-series better suited)\n",
    "- Large-scale image classification (use CNNs instead)\n",
    "- Multi-class text categorization with 1000+ categories (use deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You've mastered Support Vector Machines from first principles.\n",
    "\n",
    "### \ud83c\udf93 Key Takeaways\n",
    "\n",
    "**1. Geometric Intuition**\n",
    "- SVM finds the hyperplane with **maximum margin** from both classes\n",
    "- Margin width = 2/||w||, so maximizing margin \u2194 minimizing ||w||\n",
    "- Only **support vectors** (points on the margin) matter\n",
    "\n",
    "**2. Mathematical Elegance**\n",
    "- **Primal**: Convex optimization problem (minimize (1/2)||w||\u00b2)\n",
    "- **Dual**: Enables kernel trick (only inner products appear)\n",
    "- **KKT conditions**: Reveal support vectors through complementary slackness\n",
    "\n",
    "**3. The Kernel Trick**\n",
    "- Transform data to high (or infinite!) dimensions without computing \u03c6(x)\n",
    "- RBF kernel: K(x,x') = exp(-\u03b3||x-x'||\u00b2) \u2192 infinite dimensions\n",
    "- Enables non-linear boundaries while solving convex problem\n",
    "\n",
    "**4. Soft Margins**\n",
    "- Allow violations with slack variables \u03be\u1d62\n",
    "- C parameter: trade-off between margin width and violations\n",
    "- Large C \u2192 narrow margin, few violations (potential overfitting)\n",
    "- Small C \u2192 wide margin, more violations (better generalization)\n",
    "\n",
    "**5. Practical Implementation**\n",
    "- Solve dual using quadratic programming\n",
    "- Support vectors: \u03b1\u1d62 > 0\n",
    "- Bias: b = y\u2096 - \u03a3 \u03b1\u1d62y\u1d62K(x\u1d62,x\u2096) for margin support vectors\n",
    "- Prediction: sign(\u03a3 \u03b1\u1d62y\u1d62K(x\u1d62,x) + b)\n",
    "\n",
    "**6. When to Use**\n",
    "- \u2705 High-dimensional data, clear margins, < 100k samples\n",
    "- \u274c Very large datasets, probability estimates needed, multi-class with many classes\n",
    "\n",
    "### \ud83d\udcc8 What We Built\n",
    "\n",
    "From scratch, we implemented:\n",
    "- Complete SVM with 4 kernel types\n",
    "- Quadratic programming solver\n",
    "- Support vector identification\n",
    "- Soft margin with C parameter\n",
    "- Achieved 96%+ accuracy on breast cancer dataset\n",
    "\n",
    "### \ud83d\udd1c Preview of Lesson 4B: SVM in Production\n",
    "\n",
    "In the next lesson, we'll explore:\n",
    "\n",
    "**1. Scikit-learn's Optimized SVM**\n",
    "- LibSVM and LibLinear backends\n",
    "- Hyperparameter tuning strategies\n",
    "- Multi-class strategies (one-vs-rest, one-vs-one)\n",
    "\n",
    "**2. Advanced Techniques**\n",
    "- Handling imbalanced datasets (class_weight)\n",
    "- Probability calibration (Platt scaling)\n",
    "- Kernel approximation for large datasets (Nystroem, RBFSampler)\n",
    "\n",
    "**3. Real-World Deployment**\n",
    "- Model serialization and loading\n",
    "- Prediction latency optimization\n",
    "- Memory-efficient inference\n",
    "- A/B testing SVM vs other algorithms\n",
    "\n",
    "**4. Case Studies**\n",
    "- Text classification (20 Newsgroups dataset)\n",
    "- Image classification (digits)\n",
    "- Anomaly detection\n",
    "\n",
    "### \ud83d\udcda Further Reading\n",
    "\n",
    "**Classic Papers:**\n",
    "- Cortes & Vapnik (1995): \"Support-Vector Networks\" - Original SVM paper\n",
    "- Boser et al. (1992): \"Training Algorithm for Optimal Margin Classifiers\" - Introduced kernel trick\n",
    "- Sch\u00f6lkopf et al. (1997): \"Kernel PCA and De-Noising\" - Extended kernels to unsupervised learning\n",
    "\n",
    "**Books:**\n",
    "- *Learning with Kernels* by Sch\u00f6lkopf & Smola\n",
    "- *The Elements of Statistical Learning* by Hastie et al. (Chapter 12)\n",
    "- *Pattern Recognition and Machine Learning* by Bishop (Chapter 7)\n",
    "\n",
    "**Online Resources:**\n",
    "- [Scikit-learn SVM Guide](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [CS229 Lecture Notes on SVM](http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "- [Andrew Ng's ML Course (Coursera)](https://www.coursera.org/learn/machine-learning)\n",
    "\n",
    "### \ud83c\udf89 You Did It!\n",
    "\n",
    "You now understand:\n",
    "- \u2705 Why SVM maximizes the margin\n",
    "- \u2705 How the dual formulation enables the kernel trick\n",
    "- \u2705 What support vectors are and why they matter\n",
    "- \u2705 How to implement SVM from scratch\n",
    "- \u2705 When to use SVM vs other algorithms\n",
    "\n",
    "**Next up**: Lesson 4B for production-ready SVM implementations!\n",
    "\n",
    "---\n",
    "\n",
    "*Made with \ud83d\udc99 for the machine learning community*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}