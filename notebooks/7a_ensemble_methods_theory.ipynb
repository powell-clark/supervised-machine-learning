{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7A: Ensemble Methods Theory",
    "",
    "Master the art of combining models: Bagging, Boosting, and Stacking for superior performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction",
    "",
    "Imagine you're diagnosing a rare disease. Would you trust one doctor, or get opinions from multiple specialists and combine their diagnoses?",
    "",
    "Ensemble methods do exactly this with machine learning models - they combine multiple models to achieve better predictions than any single model could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents",
    "1. Why Ensembles Work",
    "2. Bagging (Bootstrap Aggregating)",
    "3. Random Forests Deep Dive",
    "4. Boosting Algorithms",
    "5. AdaBoost Theory",
    "6. Gradient Boosting Theory",
    "7. Stacking & Blending",
    "8. When to Use Each Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier",
    "from sklearn.tree import DecisionTreeClassifier",
    "from sklearn.linear_model import LogisticRegression",
    "from sklearn.datasets import load_breast_cancer, make_classification",
    "from sklearn.model_selection import train_test_split, cross_val_score",
    "from sklearn.metrics import accuracy_score, classification_report",
    "np.random.seed(42)",
    "print('\u2705 Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ensembles Work",
    "",
    "**The Wisdom of Crowds:**",
    "",
    "Mathematically, if you have N independent models each with accuracy p > 0.5, the ensemble accuracy approaches 1.0 as N increases!",
    "",
    "**Three Key Principles:**",
    "",
    "1. **Diversity:** Models should make different errors",
    "2. **Independence:** Models trained on different data/features",
    "3. **Quality:** Individual models must be better than random",
    "",
    "**Bias-Variance Tradeoff:**",
    "- **Bagging:** Reduces variance (Random Forests)",
    "- **Boosting:** Reduces bias (XGBoost, AdaBoost)",
    "- **Stacking:** Reduces both by learning optimal combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating)",
    "",
    "**Algorithm:**",
    "1. Create B bootstrap samples (sample with replacement)",
    "2. Train a model on each sample",
    "3. Average predictions (regression) or vote (classification)",
    "",
    "**Math:** For classification with B models:",
    "",
    "$\\hat{y} = \\text{mode}(h_1(x), h_2(x), ..., h_B(x))$",
    "",
    "For regression:",
    "",
    "$\\hat{y} = \\frac{1}{B}\\sum_{i=1}^{B} h_i(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate variance reduction",
    "from sklearn.tree import DecisionTreeClassifier",
    "from sklearn.ensemble import BaggingClassifier",
    "",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)",
    "",
    "# Single decision tree (high variance)",
    "tree = DecisionTreeClassifier(random_state=42)",
    "tree.fit(X_train, y_train)",
    "tree_acc = accuracy_score(y_test, tree.predict(X_test))",
    "",
    "# Bagging reduces variance",
    "bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)",
    "bagging.fit(X_train, y_train)",
    "bagging_acc = accuracy_score(y_test, bagging.predict(X_test))",
    "",
    "print(f'Single Tree Accuracy: {tree_acc:.3f}')",
    "print(f'Bagging Accuracy: {bagging_acc:.3f}')",
    "print(f'Improvement: {(bagging_acc - tree_acc)*100:.1f}%')",
    "print('\\n\u2705 Bagging reduces overfitting!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting",
    "",
    "**Key Idea:** Train models sequentially, each focusing on mistakes of previous models.",
    "",
    "**AdaBoost Algorithm:**",
    "1. Start with equal sample weights",
    "2. Train weak learner",
    "3. Increase weights of misclassified samples",
    "4. Repeat, combining with weighted voting",
    "",
    "**Gradient Boosting:**",
    "- Train each model to predict residuals (errors) of previous model",
    "- More powerful than AdaBoost",
    "- Forms basis of XGBoost, LightGBM, CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost example",
    "data = load_breast_cancer()",
    "X, y = data.data, data.target",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "# Weak learner (shallow tree)",
    "weak = DecisionTreeClassifier(max_depth=1)  # Called a 'stump'",
    "weak.fit(X_train, y_train)",
    "weak_acc = accuracy_score(y_test, weak.predict(X_test))",
    "",
    "# AdaBoost boosts the weak learner",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50)",
    "ada.fit(X_train, y_train)",
    "ada_acc = accuracy_score(y_test, ada.predict(X_test))",
    "",
    "print(f'Weak Learner (Stump): {weak_acc:.3f}')",
    "print(f'AdaBoost: {ada_acc:.3f}')",
    "print(f'Improvement: {(ada_acc - weak_acc)*100:.1f}%')",
    "print('\\n\u2705 Boosting turns weak learners into strong ones!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking",
    "",
    "**Meta-Learning Approach:**",
    "1. Train multiple diverse base models",
    "2. Use their predictions as features",
    "3. Train a meta-model to combine them optimally",
    "",
    "**Advantages:**",
    "- Learns optimal combination weights",
    "- Can combine very different model types",
    "- Often wins Kaggle competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking example",
    "from sklearn.ensemble import StackingClassifier",
    "",
    "# Diverse base models",
    "estimators = [",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),",
    "    ('ada', AdaBoostClassifier(n_estimators=50, random_state=42)),",
    "    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42))",
    "]",
    "",
    "# Meta-model",
    "stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())",
    "stacking.fit(X_train, y_train)",
    "stack_acc = accuracy_score(y_test, stacking.predict(X_test))",
    "",
    "print(f'Stacking Accuracy: {stack_acc:.3f}')",
    "print('\u2705 Stacking combines strengths of different models!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "**When to Use:**",
    "",
    "**Bagging (Random Forests):**",
    "- \u2705 High variance models (deep trees)",
    "- \u2705 Want stability and reliability",
    "- \u2705 Parallel training",
    "",
    "**Boosting (XGBoost, LightGBM):**",
    "- \u2705 Want maximum accuracy",
    "- \u2705 Have clean, well-prepared data",
    "- \u2705 Can tune hyperparameters carefully",
    "",
    "**Stacking:**",
    "- \u2705 Kaggle competitions",
    "- \u2705 Have diverse strong models",
    "- \u2705 Computational resources available",
    "",
    "**Next:** Lesson 7B - Production ensemble implementations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}