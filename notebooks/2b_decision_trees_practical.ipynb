{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Applying decision trees on London Housing Data\n",
    "\n",
    "In this practical, we will explore using decision trees to predict housing prices in London.\n",
    "\n",
    "We'll walk through the key steps of loading data, exploratory analysis, data preprocessing, model training and evaluation, and drawing insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Load imports](#load-imports)\n",
    "2. [Load data](#Load-the-dataset-and-display-the-data-table)\n",
    "3. [Exploratory data analysis](#exploratory-data-analysis)\n",
    "4. [Exploratory Data Analysis Discussion](#exploratory-data-analysis-discussion)\n",
    "5. [Data Processing Deep dive](#data-processing-deep-dive)\n",
    "   - [Data Cleaniness](#data-cleaniness)\n",
    "   - [Handling Missing Data](#handling-missing-data)\n",
    "   - [Handling Missing Categorical Data](#handling-missing-categorical-data)\n",
    "   - [Handling Missing Continuous Data](#handling-missing-continuous-data)\n",
    "   - [Features of the data](#features-of-the-data)\n",
    "   - [Feature engineering](#feature-engineering)\n",
    "   - [Preparing the data for modeling](#preparing-the-data-for-modeling)\n",
    "      - [Brief discussion on feature scaling](#feature-scaling)\n",
    "6. [Running the intial model and comparing missing data methods](#running-the-intial-model-and-comparing-missing-data-methods)\n",
    "7. [Feature selection](#feature-selection)\n",
    "8. [Comparing model types: Random Forest](#comparing-model-types-random-forest)\n",
    "9. [Comparing models: XGBoost gradient boosting](#comparing-models-xgboost-gradient-boosting)\n",
    "10. [Model persistence](#model-persistence-saving-and-loading-trained-models)\n",
    "    - [Loading saved models and making predictions](#loading-saved-models-and-making-predictions)\n",
    "11. [Limitations of Decision Trees](#limitations-of-decision-trees)\n",
    "12. [Ethical Considerations](#ethical-considerations)\n",
    "13. [Machine Learning Model Deployment: From Development to Production](#machine-learning-model-deployment-from-development-to-production)\n",
    "14. [Conclusion](#conclusion)\n",
    "    - [Further Reading](#further-reading)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Lets print the data table and have a look at the data.\n",
    "\n",
    "We'll see it has 11 columns and 3480 rows. The first column is the row number which will need to be removed. The second column is the price of each house and the remaining columns are features of each house.\n",
    "\n",
    "The initial features are:\n",
    "- Property Name\n",
    "- Price\n",
    "- House Type\n",
    "- Area in sq ft\n",
    "- No. of Bedrooms\n",
    "- No. of Bathrooms\n",
    "- No. of Receptions\n",
    "- Location\n",
    "- City/County\n",
    "- Postal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data with proper NaN handling and verification\n",
    "    \"\"\"\n",
    "    # Read CSV with na_values parameter to properly interpret NaN values\n",
    "    df = pd.read_csv(file_path, na_values=['NaN', 'nan', 'NAN', '', 'null', 'NULL'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_data(\"../data/London_Housing_Data.csv\")\n",
    "\n",
    "# Display first 10 rows with headers in a more readable format\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "print(\"\\nFirst 10 rows of the dataset with headers:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# Remove unnamed column with row numbers\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "print(\"\\nDataset shape after removing unnamed column:\", df.shape)\n",
    "display(df.head(10))\n",
    "\n",
    "# Let's verify the Location column specifically\n",
    "print(\"\\nUnique values in Location column:\")\n",
    "display(df['Location'].value_counts(dropna=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Let's explore the data to get a better understanding of it, identify any issues and get some insights that will help us prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display format to be more human readable with 2 decimal places\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "\n",
    "def explore_data(df):\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(df['Price']/1000000, kde=True)\n",
    "    plt.title('Distribution of House Prices')\n",
    "    plt.xlabel('Price (£ millions)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df['Area in sq ft'], df['Price']/1000000, alpha=0.5)\n",
    "    plt.title('Price vs. Area')\n",
    "    plt.xlabel('Area in sq ft')\n",
    "    plt.ylabel('Price (£ millions)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Normal correlation matrix (numeric variables only)\n",
    "    numeric_cols = ['Price', 'Area in sq ft', 'No. of Bedrooms', 'No. of Bathrooms', 'No. of Receptions']\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Matrix (Numeric Variables)')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nCorrelation Matrix (Numeric Variables):\")\n",
    "    print(corr_matrix)\n",
    "\n",
    "    print(\"\\nUnique Values Analysis:\")\n",
    "    categorical_and_discrete = ['House Type', 'Location', 'City/County', 'Postal Code', \n",
    "                              'No. of Bedrooms', 'No. of Bathrooms', 'No. of Receptions']\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    for col in categorical_and_discrete:\n",
    "        unique_count = df[col].nunique()\n",
    "        null_count = df[col].isnull().sum()\n",
    "        unique_ratio = (unique_count / total_rows) * 100\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"- Unique values: {unique_count}\")\n",
    "        print(f\"- Null values: {null_count}\")\n",
    "        print(f\"- Unique ratio: {unique_ratio:.2f}% of total rows\")\n",
    "        print(f\"- Most common values:\")\n",
    "        print(df[col].value_counts().head(3))\n",
    "\n",
    "    # Enhanced correlation matrix (including encoded categorical variables)\n",
    "    categorical_cols = ['House Type', 'Location', 'City/County', 'Postal Code']\n",
    "    df_encoded = df.copy()\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "\n",
    "    enhanced_corr_columns = numeric_cols + categorical_cols\n",
    "    enhanced_corr_matrix = df_encoded[enhanced_corr_columns].corr()\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(enhanced_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Enhanced Correlation Matrix (Including Encoded Categorical Variables)')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEnhanced Correlation Matrix (Including Encoded Categorical Variables):\")\n",
    "    print(enhanced_corr_matrix)\n",
    "\n",
    "    # Analyze categorical variables\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nTop 5 {col} by Average Price:\")\n",
    "        print(df.groupby(col)['Price'].mean().sort_values(ascending=False).head())\n",
    "        \n",
    "    # Additional visualizations for categorical variables\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x=col, y='Price', data=df.sort_values('Price', ascending=False).head(100))\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(f'Price Distribution by {col} (Top 100 Expensive Properties)')\n",
    "        plt.ylabel('Price (Millions £)')\n",
    "        # Convert y-axis values to millions and set fixed locator\n",
    "        ax = plt.gca()\n",
    "        yticks = ax.get_yticks()\n",
    "        ax.yaxis.set_major_locator(plt.FixedLocator(yticks))\n",
    "        ax.set_yticklabels(['{:.1f}'.format(x/1000000) for x in yticks])\n",
    "        plt.show()\n",
    "\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis Discussion\n",
    "\n",
    "Our exploratory data analysis reveals several key insights about the London housing market dataset that will influence our approach to building decision tree models.\n",
    "\n",
    "### Price Distribution and Implications\n",
    "\n",
    "The price distribution shows significant right-skew, with properties ranging from £180,000 to £39.75M. This 222x range presents both challenges and opportunities:\n",
    "\n",
    "#### Advantages for Decision Trees\n",
    "- Trees naturally handle non-normal distributions without transformation\n",
    "- Binary splits can effectively separate luxury properties from standard homes \n",
    "- No assumption of linearity required\n",
    "\n",
    "#### Considerations\n",
    "- Very expensive properties (>£20M) might need special treatment\n",
    "- When evaluating our model we should consider performance across different price ranges\n",
    "- We will want to use price bands for stratified sampling when splitting our data into training and test sets\n",
    "\n",
    "### Feature Relationships\n",
    "\n",
    "#### Strong Predictors\n",
    "\n",
    "#### Area (sq ft)\n",
    "- Strongest correlation with price (0.67)\n",
    "- Also shows right-skew distribution\n",
    "- Natural primary splitting candidate for decision trees\n",
    "\n",
    "#### Room Counts\n",
    "- Perfect correlation (1.0) between bedrooms, bathrooms, and receptions\n",
    "- Suggests potential data quality issue\n",
    "- Should select only one room count feature to avoid redundancy\n",
    "\n",
    "#### Location Features\n",
    "- 460 unique locations with 27.64% missing values\n",
    "- High cardinality - lots of unique values, shared between few samples - presents a challenge for one-hot encoding method of handling missing data\n",
    "- Other features such as Postal code can provide hierarchical geographic information yet also have a high number of unique values\n",
    "\n",
    "### Data Quality Considerations\n",
    "\n",
    "#### Missing Data\n",
    "The 27.64% missing location data requires careful handling:\n",
    "- Complete case analysis - dropping rows with missing values - would lose too much information\n",
    "- One-hot encoding with missing indicator preserves data patterns\n",
    "- Decision trees can handle missing values effectively\n",
    "\n",
    "#### Identical Room Distributions\n",
    "All room count features showing identical distributions (mean: 3.10, range: 0-10) suggests:\n",
    "- Possible data entry issues\n",
    "- Need for data validation\n",
    "- We should select only one room count feature to avoid redundancy and co-linearity issues\n",
    "\n",
    "### Recommendations for Model Development\n",
    "\n",
    "#### 1. Feature Selection Strategy\n",
    "- Use Area as primary numerical predictor\n",
    "- Use house type as categorical predictor\n",
    "- Select one room count feature\n",
    "- Parse outcode instead of full postocode to reduce the cardinality of this feature\n",
    "- Explore other location features\n",
    "- Consider adding other engineered features\n",
    "\n",
    "\n",
    "#### 2. Data Preprocessing Approach\n",
    "- Retain original price scale (no need for transformation)\n",
    "- Handle missing locations with indicator method - onehot encoding with missing category\n",
    "- Consider creating price bands for stratified sampling - when splitting the test train split this will ensure higher priced ranges are well represented in both datasets\n",
    "- Consider binning area into meaningful ranges as a new feature\n",
    "\n",
    "#### 3. Model Building Considerations\n",
    "- Set min_samples_leaf to handle price outliers - higher samples per leaf is more robust to outliers, but may miss some of the more subtle patterns in the data\n",
    "- Use cross-validation with stratification\n",
    "- Consider separate models for different price ranges\n",
    "\n",
    "#### 4. Evaluation Strategy\n",
    "- Evaluate performance across price bands\n",
    "- Use both absolute and percentage errors\n",
    "- Compare against simpler baseline models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "This analysis suggests we should focus on:\n",
    "- Handling missing categorical data effectively\n",
    "- Managing high cardinality features\n",
    "- Dealing with extreme price outliers\n",
    "- Creating meaningful geographic features\n",
    "- Evaluating model performance across price ranges\n",
    "\n",
    "These insights will help us build more robust and interpretable decision tree models for London house price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Deep dive!\n",
    "\n",
    "Before training our models, we need to prepare our data appropriately. In this lesson we will create multiple datasets to compare different approaches. \n",
    "\n",
    "We will:\n",
    "\n",
    "1. **Create and compare multiple clean Datasets that handle missing data in different ways**\n",
    "   - Dataset A: Leave missing values as is and let the decision tree model handle it\n",
    "   - Dataset B: Drop rows with missing values\n",
    "   - Dataset B: Fill missing values with 'Unknown'\n",
    "   - Dataset C: Use one-hot encoding with missing category\n",
    "\n",
    "2. **Feature Engineering and Selection**\n",
    "   - Drop columns we deem non-predictive (e.g., Property Name)\n",
    "   - Create feature sets of increasing complexity:\n",
    "     - Basic: Bedrooms, Bathrooms, Receptions\n",
    "     - Property: Basic + area in sq ft + House Type\n",
    "     - Property + Location Combinations\n",
    "     - Full: All available features including new engineered features such as postcode outcode\n",
    "\n",
    "3. **Data Preprocessing**\n",
    "   - Convert categorical variables to numerical using `LabelEncoder`\n",
    "   - Split each dataset variant into training and test sets\n",
    "   - Apply feature scaling where appropriate\n",
    "\n",
    "This structured approach will allow us to:\n",
    "- Compare the impact of different missing data handling methods\n",
    "- Using the best missing data approach then evaluate the models performance with different feature combinations\n",
    "- We will then compare the performance of the decision tree model with a linear regression and a more advanced technique such as a random forest model.\n",
    "\n",
    "The following code sections will implement these preparation steps and create our model comparison framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaniness\n",
    "\n",
    "Our data is dirty! By this we mean it contains missing values and possibly incorrect data. Not to worry, the world is messy and this is a great opportunity to practice our data cleaning and preprocessing skills.\n",
    "\n",
    "We've uncovered two issues with our data:\n",
    "- We have missing values in the 'Location' field in 27% of our data\n",
    "- We have a perfect correlation between bedroom, bathrooms and receptions which suggests a data quality issue\n",
    "\n",
    "We will need to handle both of these issues before we can train our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Our data has a significant amount of missing values, particularly in the 'Location' field where 27% of the values are NaN. \n",
    "\n",
    "While missing data can be problematic for many machine learning models, modern decision tree implementations handle missing values elegantly through advanced techniques.\n",
    "\n",
    "Some libraries use \"surrogate splits\" where alternative features that best approximate the original split are used when the primary split feature has missing values. \n",
    "\n",
    "Others, like scikit-learn, use a technique called \"fractional samples\" or \"fractional instances\" where samples with missing values are assigned fractional weights to each branch based on the proportion of non-missing samples that go to each branch.\n",
    "\n",
    "The approach to handling missing data can differ based on whether the variable is categorical or continuous.\n",
    "\n",
    "### Handling Missing Categorical Data\n",
    "\n",
    "For categorical variables like 'Location', we can consider the following methods:\n",
    "\n",
    "1. **Leaving the missing values as is and let our decision tree model handle it**\n",
    "2. **Dropping rows with missing values**\n",
    "3. **Filling NaNs with a specific value (e.g., 'Unknown')**\n",
    "3. **One-hot encoding with a separate 'Missing' category**\n",
    "\n",
    "Let's explore these methods and their potential impact on our decision tree model.\n",
    "\n",
    "#### 1. Leaving the missing values as is and letting our decision tree model handle it\n",
    "\n",
    "Scikit-learn's DecisionTreeRegressor has built-in support for handling missing values without the need for explicit imputation or removal of rows. It uses a technique called \"fractional samples\" or \"fractional instances\" to effectively work with missing data.\n",
    "\n",
    "##### Fractional Samples Technique:\n",
    "\n",
    "When considering a split for a feature with missing values, the decision tree assigns fractional weights to samples with missing values based on the proportion of non-missing samples that go to each branch.\n",
    "\n",
    "For example, if 70% of the non-missing samples go to the left branch and 30% go to the right branch, a sample with a missing value would be assigned a weight of 0.7 for the left branch and 0.3 for the right branch.\n",
    "\n",
    "The decision tree algorithm then proceeds to evaluate the split based on the weighted samples, considering both the non-missing samples and the fractional weights assigned to the missing samples.\n",
    "\n",
    "During prediction, if a new sample has a missing value for a feature used in a split, the sample follows both branches of the split, and the final prediction is a weighted average of the predictions from the leaves reached in each branch.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- No data loss: All available information is preserved, including rows with missing values.\n",
    "- Avoids introducing bias: The decision tree algorithm can learn from the patterns in the non-missing values without making assumptions about the missing data.\n",
    "- Handles missing values during prediction: The trained model can make predictions on new data that has missing location values.\n",
    "- Efficient and convenient: Eliminates the need for separate data preprocessing steps to handle missing values.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Increased complexity: The decision tree algorithm needs to handle missing values internally, which can add complexity to the model training process.\n",
    "- Potential overfitting: If the missing values have a specific pattern or meaning, the decision tree might overfit to that pattern, leading to reduced generalization performance.\n",
    "- Interpretability: The fractional samples approach may make the decision tree splits and structure less intuitive and harder to interpret compared to a clean dataset without missing values.\n",
    "\n",
    "#### 2. Dropping Rows with Missing Values\n",
    "\n",
    "The simplest approach is to drop any row that has a missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_missing(df):\n",
    "    return df.dropna()\n",
    "\n",
    "df_dropped = drop_rows_with_missing(df)\n",
    "print(f\"Rows after dropping: {len(df_dropped)}\")\n",
    "print(f\"Percentage of data lost: {(1 - len(df_dropped) / len(df)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Easy to implement\n",
    "- Results in a clean dataset without any missing values\n",
    "\n",
    "Cons:\n",
    "- We lose a significant portion of our data (27% in this case), which can reduce the representativeness of our dataset and the predictive power of our model\n",
    "- If the missing values are not Missing Completely At Random (MCAR) - where the probability of a value being missing is unrelated to any other variable, this can introduce bias\n",
    "- The model will not be able to make predictions on new data that has missing location values\n",
    "\n",
    "#### 3. Filling NaNs with a Specific Value\n",
    "\n",
    "We can fill in the missing values with a specific value that indicates missingness, such as 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_value(df, value):\n",
    "    return df.fillna(value)\n",
    "\n",
    "df_filled_unknown = fill_with_value(df, 'Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Easy to implement: It requires just a single fillna() operation in pandas, making it one of the simplest approaches to handle missing data\n",
    "- Retains all data points: Unlike dropping rows, this method preserves your entire dataset, ensuring no information is discarded\n",
    "\n",
    "Cons:\n",
    "- Problematic assumptions about missing data: When we replace NaN with 'Unknown', we're making an assumption that all missing values represent a specific category that is known to us and related to the observed data.\n",
    "\n",
    "- This assumption may not be true because:\n",
    "  - Values might be missing for various unrelated reasons (data entry errors, not collected, etc.)\n",
    "  - Missing values might not have any meaningful relationship to each other\n",
    "  - The missingness itself might not be informative\n",
    "\n",
    "- Potential bias introduction: \n",
    "  - If 'Unknown' has a specific meaning in your domain (e.g., deliberately withheld information), using it as a catch-all for missing values could confuse the model and lead to incorrect predictions\n",
    "\n",
    "#### 4. One-Hot Encoding with a Separate 'Missing' Category\n",
    "\n",
    "In this approach, we create a separate category for missing values during one-hot encoding. For example, with our 'Location' column:\n",
    "\n",
    "```code\n",
    "Original data:\n",
    "+------------+-----------+\n",
    "| Row Number | Location  |\n",
    "+------------+-----------+\n",
    "| Row 1      | Chelsea   |\n",
    "| Row 2      | NaN       |\n",
    "| Row 3      | Hackney   |\n",
    "| Row 4      | Chelsea   |\n",
    "+------------+-----------+\n",
    "\n",
    "After one-hot encoding with missing values:\n",
    "+------------+------------------+------------------+-------------+\n",
    "| Row Number | Location_Chelsea | Location_Hackney | Location_NaN|\n",
    "+------------+------------------+------------------+-------------+\n",
    "| Row 1      |        1         |        0         |      0      |\n",
    "| Row 2      |        0         |        0         |      1      |\n",
    "| Row 3      |        0         |        1         |      0      |\n",
    "| Row 4      |        1         |        0         |      0      |\n",
    "+------------+------------------+------------------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_with_missing(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encodes a column while handling missing values.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        column: Name of column to encode\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with one-hot encoded columns replacing original column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Create dummy variables including NaN values\n",
    "    dummy_cols = pd.get_dummies(df_encoded[column], dummy_na=True, prefix=column)\n",
    "    \n",
    "    # Remove original column and add dummy columns\n",
    "    df_encoded = df_encoded.drop(columns=[column])\n",
    "    df_encoded = pd.concat([df_encoded, dummy_cols], axis=1)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Create new DataFrame with encoded values\n",
    "df_onehot = onehot_encode_with_missing(df, 'Location')\n",
    "display(df_onehot.head())\n",
    "print(f\"Total columns: {len(df_onehot.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Retains all data points (3,480 rows)\n",
    "- Allows the model to treat missing values as a separate category\n",
    "- Preserves location-specific patterns in the data\n",
    "- No assumptions made about missing values\n",
    "\n",
    "Cons:\n",
    "- Dramatic increase in dimensionality:\n",
    "  - From 1 column to 656 columns (656 unique locations + 1 NaN column)\n",
    "  - Creates very sparse matrix (most values will be 0)\n",
    "  - Even common locations like Putney (96 rows) only use 2.8% of their column\n",
    "- High memory usage:\n",
    "  - Each row needs 657 boolean values instead of one categorical value\n",
    "  - Significant impact on model training time\n",
    "- Risk of overfitting:\n",
    "  - Many locations have very few examples (low signal-to-noise ratio)\n",
    "  - Decision trees might make splits based on rare locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Continuous Data\n",
    "\n",
    "Whilst our data is present for all our other variables, lets quickly discuss how we would handle missing data for a continous variable.\n",
    "\n",
    "For continuous variables like 'Area in sq ft', we can consider the following methods:\n",
    "\n",
    "1. **Dropping rows with missing values**\n",
    "2. **Imputing missing values**\n",
    "   - Using statistics like mean, median\n",
    "   - Using advanced imputation methods like KNN or MICE\n",
    "3. **Binning the continuous variable and treating missing values as a separate bin**\n",
    "\n",
    "#### Dropping Rows with Missing Values\n",
    "\n",
    "The simplest approach is to drop any row that has a missing value. \n",
    "\n",
    "Just like we did with our categorical data its has the same pros and cons.\n",
    "\n",
    "Pros:\n",
    "- Easy to implement\n",
    "- Results in a clean dataset without any missing values\n",
    "\n",
    "Cons:\n",
    "- We lose data points, which can reduce the representativeness of our dataset and the predictive power of our model\n",
    "- If the missing values are not completely at random (MCAR), this can introduce bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing Missing Values\n",
    "\n",
    "Instead of dropping rows, we can fill in (impute) the missing values using statistics like mean or median.\n",
    "\n",
    "```python\n",
    "def fill_with_stats(df, method):\n",
    "    if method == 'mean':\n",
    "        return df.fillna(df.mean())\n",
    "    elif method == 'median':\n",
    "        return df.fillna(df.median())\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "df_filled_mean = fill_with_stats(df, 'mean')\n",
    "df_filled_median = fill_with_stats(df, 'median')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Retains all data points\n",
    "- If the missingness is unrelated to the observed data - Missing At Random (MAR), then this method can give unbiased estimates\n",
    "\n",
    "Cons:\n",
    "- Imputed values are estimates, which can introduce \"noise\" - statistical variability or random fluctuations in the data that don't represent true patterns\n",
    "- If missingness is related to the unobserved data also known as Missing Not At Random (MNAR) - where missing values are related to the unobserved variable, then this method can give biased estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced imputation methods: KNN and MICE\n",
    "\n",
    "We can also use more advanced imputation methods like KNN or MICE (available in libraries like scikit-learn and impyute).\n",
    "\n",
    " Key Difference:\n",
    " - KNN looks at similar samples\n",
    " - MICE builds relationships between features\n",
    "\n",
    " ##### KNN (K-Nearest Neighbors) Imputation:\n",
    " - K represents the number of neighbours to consider (e.g., if K=5, we look at 5 similar samples)\n",
    " - For each sample with missing values:\n",
    "   1. Look at all other complete samples in the dataset\n",
    "   2. Calculate similarity between samples using features that aren't missing\n",
    "   3. Find the K most similar samples (the \"neighbours\")\n",
    "   4. Fill missing values using:\n",
    "      - For numerical: average of the K neighbors' values\n",
    "      - For categorical: most common value among K neighbors\n",
    " - Larger K values (e.g., 10-20):\n",
    "   + More stable predictions\n",
    "   - Might miss local patterns\n",
    " - Smaller K values (e.g., 3-5):\n",
    "   + Better at capturing local patterns\n",
    "   - More sensitive to noise\n",
    "\n",
    "   \n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def fill_with_knn(df, n_neighbors=5):\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    return pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df_filled_knn = fill_with_knn(df)\n",
    "```\n",
    "\n",
    "Pros:\n",
    "- Can capture more complex patterns in the data when imputing\n",
    "- Can work well if the data conforms to the assumptions of the method (e.g., data is missing at random for KNN)\n",
    "\n",
    "Cons:\n",
    "- More computationally expensive than simple methods\n",
    "- Requires careful selection of parameters (e.g., number of neighbors for KNN)\n",
    "- Can still introduce bias if the assumptions are not met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### MICE (Multiple Imputation by Chained Equations):\n",
    " A more sophisticated approach that works like this:\n",
    " 1. Initial Setup:\n",
    "    - Start with rough guesses for all missing values (e.g., mean/mode)\n",
    "    - Create multiple copies of the dataset (usually 3-5)\n",
    "\n",
    " 2. For each copy:\n",
    "   - a) For each feature with missing values:\n",
    "       - Temporarily remove the current guesses\n",
    "       - Build a regression/classification model using other features\n",
    "       - Predict new values for the missing data\n",
    "       - Update the guesses with these predictions\n",
    "   - b) Repeat this process several times (iterations)\n",
    "\n",
    " 3. Final Step:\n",
    "    - Now have multiple complete datasets\n",
    "    - Each represents a possible version of reality\n",
    "    - Combine them for final estimates, capturing uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using MICE:\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "\n",
    "def fill_with_mice(df, n_imputations=5, max_iter=10, random_state=42):\n",
    "    # Create multiple imputations\n",
    "    all_imputations = []\n",
    "    \n",
    "    for i in range(n_imputations):\n",
    "        # Initialize MICE imputer\n",
    "        mice_imputer = IterativeImputer(\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state + i\n",
    "        )\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        imputed_data = mice_imputer.fit_transform(df)\n",
    "        all_imputations.append(pd.DataFrame(imputed_data, columns=df.columns))\n",
    "    \n",
    "    # Combine imputations by taking the mean\n",
    "    final_df = pd.concat(all_imputations).groupby(level=0).mean()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Apply MICE imputation\n",
    "df_filled_mice = fill_with_mice(df)\n",
    "\n",
    "# Compare original vs imputed values for a column with missing data\n",
    "missing_col = df.columns[df.isnull().any()][0]\n",
    "print(f\"\\nComparison for {missing_col}:\")\n",
    "print(f\"Original mean: {df[missing_col].mean():.2f}\")\n",
    "print(f\"Imputed mean: {df_filled_mice[missing_col].mean():.2f}\")\n",
    "```\n",
    "\n",
    "Pros of MICE:\n",
    " - Preserves relationships between variables by using other features to predict missing values\n",
    " - Creates multiple imputations to capture uncertainty in the missing data\n",
    " - Can handle different variable types (continuous and categorical)\n",
    " - More sophisticated than simple mean/median imputation\n",
    " - Accounts for the randomness in the imputation process\n",
    "\n",
    " Cons of MICE:\n",
    " - Computationally intensive, especially with large datasets\n",
    " - Assumes missing data is MAR (Missing At Random)\n",
    " - May not perform well if relationships between variables are highly non-linear\n",
    " - Can be sensitive to the order of variables in the imputation process\n",
    " - Multiple imputations need to be combined, which adds complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Binning the Continuous Variable\n",
    "\n",
    "We can bin the continuous variable into discrete intervals and treat missing values as a separate bin.\n",
    "\n",
    "```python\n",
    "\n",
    "def bin_with_missing(df, column, n_bins=5):\n",
    "    df[column] = pd.qcut(df[column], n_bins, labels=False, duplicates='drop')\n",
    "    df[column] = df[column].astype('object')\n",
    "    df[column] = df[column].fillna('Missing')\n",
    "    return df\n",
    "\n",
    "df_binned = bin_with_missing(df, 'Area in sq ft')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Retains all data points\n",
    "- Allows the model to treat missing values as a separate category\n",
    "- Can capture non-linear relationships between the variable and the target\n",
    "\n",
    "Cons:\n",
    "- Loses some information by discretizing the continuous variable\n",
    "- The choice of the number of bins can impact model performance\n",
    "\n",
    "### Using Models that Handle Missing Data\n",
    "\n",
    "More advanced implementations using multiple trees offer sophisticated approaches to missing data:\n",
    "\n",
    "1. **Random Forests** \n",
    "   - An ensemble method that builds multiple decision trees and averages their predictions\n",
    "   - Handles missing values through proximity-based methods, which is a form of surrogate splitting at the ensemble level\n",
    "   - Instead of finding correlated features, it looks at similar samples across multiple trees to determine the best path for missing values\n",
    "\n",
    "2. **XGBoost (Extreme Gradient Boosting)**\n",
    "   - A boosting algorithm that builds trees sequentially, with each tree correcting errors from previous trees\n",
    "   - Takes a different approach by learning the optimal default direction for missing values at each split\n",
    "   - During training, it tests sending missing values both left and right, choosing the direction that minimizes error\n",
    "\n",
    "Pros:\n",
    "- No separate imputation step needed (for supported models)\n",
    "- Missing patterns can contribute to the model's learning\n",
    "- Often performs better than simple imputation methods\n",
    "\n",
    "Cons:\n",
    "- Not all implementations support these methods (e.g., scikit-learn's decision trees don't implement surrogate splits)\n",
    "- Less transparent than explicit handling methods\n",
    "- May not be suitable when understanding the missing data mechanism is important\n",
    "\n",
    "In the next section, we'll examine the features of our London housing dataset and begin building our initial model.\n",
    "\n",
    "##  Features of the data\n",
    "\n",
    "Lets have a look at our dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data types and value ranges for each column\n",
    "print(\"Data types and value ranges for each column:\\n\")\n",
    "\n",
    "for column in df.columns:\n",
    "    print(f\"\\n{column}:\")\n",
    "    if df[column].dtype in ['int64', 'float64']:\n",
    "        print(f\"Type: {df[column].dtype}\")\n",
    "        print(f\"Range: {df[column].min():,.2f} to {df[column].max():,.2f}\")\n",
    "        print(f\"Mean: {df[column].mean():,.2f}\")\n",
    "    else:\n",
    "        print(f\"Type: {df[column].dtype}\")\n",
    "        print(\"Categories:\")\n",
    "        value_counts = df[column].value_counts()\n",
    "        for value, count in value_counts.items():\n",
    "            print(f\"  - {value}: {count:,} occurrences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The dataset has 3478 properties including:\n",
    "\n",
    "**Target variable:**\n",
    "\n",
    "Price \n",
    "- dtype (int64 type)\n",
    "- Continuous variable\n",
    "- Range: £180,000 to £39,750,000\n",
    "- Mean: £1,864,173\n",
    "\n",
    "**Features:**\n",
    "\n",
    "Property Name \n",
    "  - dtype pandas object - strings, no missing values\n",
    "  - Contains 785 unique property names\n",
    "  - Categorical variable with text values\n",
    "  - Most common: \"Television Centre\" (17 occurrences)\n",
    "\n",
    "\n",
    "House Type \n",
    "- dtype pandas object - strings, no missing values\n",
    "- Categorical variable with 8 categories:\n",
    "  - Flat/Apartment (1,565)\n",
    "  - House (1,430)\n",
    "  - New development (357)\n",
    "  - Penthouse (100)\n",
    "  - Studio (10)\n",
    "  - Bungalow (9)\n",
    "  - Duplex (7)\n",
    "  - Mews (2)\n",
    "\n",
    "Area in sq ft \n",
    "- dtype int64 \n",
    "- Continuous variable\n",
    "- Range: 274 to 15,405 square feet\n",
    "- Mean: 1,713 sq ft\n",
    "\n",
    "\n",
    "Number of Bedrooms \n",
    "- dtype int64 \n",
    "- Discrete numerical variable\n",
    "- Range: 0 to 10 bedrooms\n",
    "- Mean: 3.10\n",
    "\n",
    "\n",
    "Number of Bathrooms \n",
    "- dtype int64 \n",
    "- Discrete numerical variable\n",
    "- Range: 0 to 10 bathrooms\n",
    "- Mean: 3.10\n",
    "\n",
    "\n",
    "Number of Receptions \n",
    "- dtype int64 \n",
    "- Discrete numerical variable\n",
    "- Range: 0 to 10 reception rooms\n",
    "- Mean: 3.10\n",
    "\n",
    "\n",
    "Location \n",
    "- dtype pandas object - strings, has missing values as NAN\n",
    "- Categorical variable with 460 unique locations\n",
    "- Most common: \"Putney\" (96 occurrences)\n",
    "\n",
    "\n",
    "City/County \n",
    "- dtype pandas object - string, no missing values\n",
    "- Categorical variable with 53 unique values\n",
    "- Most common: \"London\" (2,972 occurrences)\n",
    "\n",
    "\n",
    "Postal Code \n",
    "- dtype pandas object - strings, no missing values\n",
    "- Contains 1,284 unique postal codes\n",
    "- Alphanumeric string format (e.g., \"SW6 3LF\")\n",
    "- Most common: \"SW6 3LF\" (14 occurrences)\n",
    "\n",
    "We also have the Postal Code, which we can use to extract more geographical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Before training our models, we can enhance our dataset through feature engineering - the process of creating new features that might help capture important patterns in the data.\n",
    "\n",
    "One key opportunity in our London housing dataset is to extract more meaningful geographical information from the postcodes.\n",
    "\n",
    "Other ideas could include:\n",
    "- Binning the area feature into more meaningful ranges\n",
    "- Distance from city center\n",
    "- Distance from nearest tube station\n",
    "- School quality score for the area\n",
    "- Crime rate for the area\n",
    "- Green space percentage for the area\n",
    "- Number of amenities (shops, restaurants, etc.) within a radius\n",
    "\n",
    "#### Adding Postcode Outcode feature\n",
    "Currently, our dataset has 1,284 unique postcodes spread across 3,478 properties, meaning we have an average of only 2.7 properties per postcode. This sparsity could make it difficult for the model to learn meaningful patterns, as many postcodes will have just 1-2 examples.\n",
    "\n",
    "We can improve this by extracting the \"outcode\" - the first part of a UK postcode (e.g., \"SW6\" from \"SW6 3LF\"). \n",
    "\n",
    "Outcodes represent broader geographical areas and offer several advantages:\n",
    "\n",
    "1. Increased data density: Each outcode will contain more properties, giving the model more examples to learn from within each area\n",
    "\n",
    "2. Better generalization: The model can learn broader geographical patterns - a borough - rather than overfitting to specific streets\n",
    "\n",
    "3. Reduced dimensionality: Instead of 1,284 unique values, we'll have far fewer unique outcodes, making the feature space more manageable\n",
    "\n",
    "4. Statistical significance: More properties per group means more reliable average prices and trends for each area\n",
    "\n",
    "\n",
    "#### Let's add this new feature to each of our cleaned datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_outcode(postcode: str) -> str:\n",
    "    \"\"\"Extract the outcode (first part) from a postcode.\"\"\"\n",
    "    return postcode.split()[0] if isinstance(postcode, str) else None\n",
    "\n",
    "def add_outcode_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add outcode feature derived from Postal Code column.\"\"\"\n",
    "    df_with_outcode = df.assign(\n",
    "        Outcode=df['Postal Code'].map(extract_outcode)\n",
    "    )\n",
    "    \n",
    "    n_unique = df_with_outcode['Outcode'].nunique()\n",
    "    avg_properties = len(df_with_outcode) / n_unique\n",
    "    \n",
    "    print(f\"Created {n_unique} unique outcodes\")\n",
    "    print(f\"Average properties per outcode: {avg_properties:.1f}\")\n",
    "    \n",
    "    return df_with_outcode\n",
    "\n",
    "# Apply to each of our cleaned datasets\n",
    "df_original_with_outcode = add_outcode_feature(df)\n",
    "df_dropped_with_outcode = add_outcode_feature(df_dropped)\n",
    "df_filled_unknown_with_outcode = add_outcode_feature(df_filled_unknown)\n",
    "df_onehot_with_outcode = add_outcode_feature(df_onehot)\n",
    "\n",
    "display(df_original_with_outcode.head(10))\n",
    "display(df_dropped_with_outcode.head(10))\n",
    "display(df_filled_unknown_with_outcode.head(10))\n",
    "display(df_onehot_with_outcode.head(10))\n",
    "# Example analysis of how outcodes relate to price\n",
    "print(\"\\nTop 5 outcodes by average price:\")\n",
    "\n",
    "print(df_dropped_with_outcode.groupby('Outcode')['Price'].agg(['mean', 'count'])\n",
    "      .sort_values('mean', ascending=False)\n",
    "      .head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for modeling\n",
    "\n",
    "For our initial model will compare df_original to our missing data approaches df_dropped, df_filled_unknown and df_onehot:\n",
    "\n",
    "We need to prepare the data for modeling by:\n",
    "- Dropping any columns that are not useful for training our model (e.g. property name and 2 of the room count features)\n",
    "- Converting any categorical variables to numerical variables using `LabelEncoder`\n",
    "- Splitting our data into training and test sets using a 80/20 split and we'll implement a stratified split on the price variable to ensure the test and training sets have a similar distribution of prices\n",
    "\n",
    "#### Feature scaling\n",
    "\n",
    "We will not applying any scaling to the data as decision trees are not sensitive to the scale of the data. It is worth noting however, as revealed in our exploratory analysis, that the price variable has a very large range and is not normally distributed. \n",
    "\n",
    "We need to account for this when spliting our data into training and test sets and be mindful of this when interpreting our model's results - errors are likely to be larger for properties with higher prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling with stratified sampling based on price bands\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Create price bands for stratification\n",
    "    df_model['price_band'] = pd.qcut(df_model['Price'], q=10, labels=False)\n",
    "\n",
    "    # Drop unnecessary columns if present\n",
    "    unnecessary_columns = ['Property Name', 'No. of Bathrooms', 'No. of Receptions']\n",
    "    df_model = df_model.drop([col for col in unnecessary_columns if col in df_model.columns], axis=1)\n",
    "\n",
    "    # Convert categorical variables using LabelEncoder\n",
    "    categorical_cols = ['House Type', 'Location', 'City/County', 'Postal Code', 'Outcode']\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df_model.columns:\n",
    "            df_model[col] = label_encoder.fit_transform(df_model[col])\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df_model.drop(['Price', 'price_band'], axis=1)\n",
    "    y = df_model['Price']\n",
    "    \n",
    "    # Stratified split using price bands\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=df_model['price_band']  # Use price bands for stratification\n",
    "    )\n",
    "    \n",
    "    print(\"Features included:\", X.columns.tolist())\n",
    "    # print(\"Training set shape:\", X_train.shape)\n",
    "    # print(\"Test set shape:\", X_test.shape)\n",
    "    \n",
    "    # Verify stratification worked\n",
    "    train_price_stats = y_train.describe()\n",
    "    test_price_stats = y_test.describe()\n",
    "    # print(\"\\nPrice distribution comparison:\")\n",
    "    # print(\"\\nTraining set:\")\n",
    "    # print(train_price_stats)\n",
    "    # print(\"\\nTest set:\")\n",
    "    # print(test_price_stats)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "df_original_prepared = prepare_data_for_modeling(df)\n",
    "df_dropped_prepared = prepare_data_for_modeling(df_dropped)\n",
    "df_filled_unknown_prepared = prepare_data_for_modeling(df_filled_unknown)\n",
    "df_onehot_prepared = prepare_data_for_modeling(df_onehot)\n",
    "\n",
    "display(df_original_prepared[0].head(5))\n",
    "display(df_dropped_prepared[0].head(5))\n",
    "display(df_filled_unknown_prepared[0].head(5))\n",
    "display(df_onehot_prepared[0].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the intial model and comparing missing data methods\n",
    "\n",
    "Let's create and evaluate our initial model using the `df_dropped` dataset and compare the results to the other missing data approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_evaluate_model(prepared_data: tuple, model_name: str = \"Model\") -> dict:\n",
    "    \"\"\"\n",
    "    Creates and evaluates a decision tree model using prepared data tuples.\n",
    "    \n",
    "    Args:\n",
    "        prepared_data: Tuple of (X_train, X_test, y_train, y_test)\n",
    "        model_name: Name identifier for the model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing model metrics and feature importance\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Unpack the prepared data\n",
    "    X_train, X_test, y_train, y_test = prepared_data\n",
    "    \n",
    "    # Create and train model using default parameters on the training set\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'name': model_name,\n",
    "        'metrics': {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred)\n",
    "        },\n",
    "        'feature_importance': dict(zip(X_train.columns, model.feature_importances_))\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"MAE: £{results['metrics']['MAE']:,.2f}\")\n",
    "    print(f\"RMSE: £{results['metrics']['RMSE']:,.2f}\")\n",
    "    print(f\"R2 Score: {results['metrics']['R2']:.4f}\")\n",
    "    \n",
    "    # Print top 5 most important features\n",
    "    importance = results['feature_importance']\n",
    "    sorted_importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for feature, importance in sorted_importance.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    return results, model, X_test\n",
    "\n",
    "# Individual calls for each prepared dataset\n",
    "original_results, decision_tree_model, X_test = create_and_evaluate_model(df_original_prepared, \"Original\")\n",
    "dropped_results = create_and_evaluate_model(df_dropped_prepared, \"Dropped NAs\")\n",
    "unknown_results = create_and_evaluate_model(df_filled_unknown_prepared, \"Unknown Values\")\n",
    "onehot_results = create_and_evaluate_model(df_onehot_prepared, \"One-hot Encoded\")\n",
    "\n",
    "# Collect all results\n",
    "all_results = {\n",
    "    'original': original_results,\n",
    "    'dropped': dropped_results,\n",
    "    'unknown': unknown_results,\n",
    "    'onehot': onehot_results\n",
    "}\n",
    "\n",
    "# used to test saved model later in lesson\n",
    "decision_tree_test_sample = X_test.sample(10)\n",
    "# display(decision_tree_test_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! One-hot encoding is the best performing method and area in sq ft is the most important feature.\n",
    "\n",
    "We determine this because it has the lowest MAE and RMSE and the highest R2 score.\n",
    "\n",
    "Let's verify this by comparing the performance of each missing data method with our engineered feature outcode included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_original_with_outcode_prepared = prepare_data_for_modeling(df_original_with_outcode)\n",
    "df_dropped_with_outcode_prepared = prepare_data_for_modeling(df_dropped_with_outcode)\n",
    "df_filled_unknown_with_outcode_prepared = prepare_data_for_modeling(df_filled_unknown_with_outcode)\n",
    "df_onehot_with_outcode_prepared = prepare_data_for_modeling(df_onehot_with_outcode)\n",
    "\n",
    "original_with_outcode_results, decision_tree_model = create_and_evaluate_model(df_original_with_outcode_prepared, \"Original with Outcode\")\n",
    "dropped_with_outcode_results = create_and_evaluate_model(df_dropped_with_outcode_prepared, \"Dropped NAs with Outcode\")\n",
    "filled_unknown_with_outcode_results = create_and_evaluate_model(df_filled_unknown_with_outcode_prepared, \"Unknown Values with Outcode\")\n",
    "onehot_with_outcode_results = create_and_evaluate_model(df_onehot_with_outcode_prepared, \"One-hot Encoded with Outcode\")\n",
    "\n",
    "display(original_with_outcode_results)\n",
    "display(dropped_with_outcode_results)\n",
    "display(filled_unknown_with_outcode_results)\n",
    "display(onehot_with_outcode_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Changes with Outcode Addition\n",
    "\n",
    "Let's analyze the results from our second test including our engineered feature postcode outcode.\n",
    "\n",
    "First, let's recall what these metrics mean:\n",
    "- MAE (Mean Absolute Error): Average absolute difference between predicted and actual prices \n",
    "- RMSE (Root Mean Square Error): Square root of average squared differences, penalizes large errors more\n",
    "- R2 (R-squared): Proportion of variance explained by model, higher is better (max 1.0)\n",
    "\n",
    "#### Original Dataset with Outcode\n",
    "- MAE: ↓0.3% (£693,983.58)\n",
    "- RMSE: ↓1.6% (£1,884,277.57)\n",
    "- R²: ↑3.6% (0.4864)\n",
    "- Top Features:\n",
    "    1. Area in sq ft: 0.6167\n",
    "    2. Postal Code: 0.1364\n",
    "    3. City/County: 0.0796\n",
    "    4. House Type: 0.0582\n",
    "    5. No. of Bedrooms: 0.0407\n",
    "\n",
    "#### Dropped NAs Dataset with Outcode\n",
    "- MAE: ↓2.5% (£604,233.61)\n",
    "- RMSE: ↓3.3% (£1,411,814.09)\n",
    "- R²: ↑5.6% (0.5641)\n",
    "- Top Features:\n",
    "    1. Area in sq ft: 0.5567\n",
    "    2. Postal Code: 0.1117\n",
    "    3. City/County: 0.1022\n",
    "    4. Outcode: 0.0674\n",
    "    5. House Type: 0.0670\n",
    "\n",
    "#### Unknown Values Dataset with Outcode\n",
    "- MAE: ↓2.1% (£655,992.20)\n",
    "- RMSE: ↓1.7% (£1,786,127.58)\n",
    "- R²: ↑3.0% (0.5385)\n",
    "- Top Features:\n",
    "    1. Area in sq ft: 0.6202\n",
    "    2. Postal Code: 0.1216\n",
    "    3. City/County: 0.0802\n",
    "    4. House Type: 0.0581\n",
    "    5. Outcode: 0.0500\n",
    "\n",
    "#### One-hot Encoded Dataset with Outcode\n",
    "- MAE: ↑2.7% (£619,448.17)\n",
    "- RMSE: ↑3.1% (£1,629,301.54)\n",
    "- R²: ↓3.6% (0.6160)\n",
    "- Top Features:\n",
    "    1. Area in sq ft: 0.5188\n",
    "    2. Location_Mayfair: 0.1102\n",
    "    3. Postal Code: 0.0883\n",
    "    4. No. of Bedrooms: 0.0694\n",
    "    5. House Type: 0.0585\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Performance Impact**\n",
    "   - Outcode improved performance in 3 out of 4 approaches\n",
    "   - Largest improvements in Dropped NAs approach (↑5.6% R²)\n",
    "   - One-hot encoding showed slight performance degradation with outcode addition\n",
    "\n",
    "2. **Feature Importance**\n",
    "   - Area in sq ft remained dominant (51-62%) across all variations\n",
    "   - Outcode partially absorbed importance from Postal Code\n",
    "   - Location features collectively account for ~25-30%\n",
    "\n",
    "3. **Model Behavior**\n",
    "   - Simple models benefited most from outcode addition\n",
    "   - One-hot encoding showed potential feature redundancy\n",
    "   - Outcode provides meaningful signal for location-based pricing\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Model Selection**\n",
    "   - Use outcode in simpler models where dimensionality is a concern\n",
    "   - Consider dropping outcode for one-hot encoded models\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Investigate combining location features to reduce redundancy\n",
    "   - Consider creating location clusters using outcode and postal code\n",
    "   - Explore interaction terms between outcode and other features\n",
    "\n",
    "3. **Next Steps**\n",
    "   - Test reduced feature sets to optimize model complexity\n",
    "   - Evaluate performance on specific price ranges\n",
    "   - Consider ensemble approaches combining different encoding methods\n",
    "\n",
    "**Next, let's try one-hot encoding with different combinations of features to reduce dimensionality while maintaining model performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features (variables) from a larger set of features. \n",
    "\n",
    "This is important because:\n",
    "\n",
    "- It can help improve model performance by reducing overfitting and improving generalization\n",
    "- It can make the model more interpretable by focusing on the most important features\n",
    "- It can reduce computational complexity and improve training speed\n",
    "\n",
    "In this section, we'll explore different feature subsets within the one-hot encoded dataset. It's important to note that this dataset has 665 features because it is one-hot encoded on the location field where it is had NAN values. this adds an additional layer of complexity to our feature selection process. \n",
    "\n",
    "We'll try different variation of the location features including the removal of the one-shot encoded location field and the addition of the engineered outcode field.\n",
    "\n",
    " Let's try different feature combinations based on the following subsets:\n",
    " \n",
    " 1. Basic Features\n",
    "    - Bedrooms, Bathrooms\n",
    " \n",
    " 2. Minimal Features\n",
    "    - Basic + Receptions\n",
    " \n",
    " 3. Minimal Extended Features\n",
    "    3.1. Minimal + Area in sq ft\n",
    "    3.2. Minimal + House Type\n",
    " \n",
    " 4. Property Features (Core)\n",
    "    - Minimal + Area + House Type\n",
    " \n",
    " 5. Property Location Combinations\n",
    "    5.1. Property + Outcode\n",
    "    5.2. Property + Location\n",
    "    5.3. Property + City/County\n",
    "    5.4. Property + Full Postcode\n",
    " \n",
    " 6. Property Double Location Combinations\n",
    "    6.1. Property + Location + City\n",
    "    6.2. Property + City + Outcode\n",
    "    6.3. Property + City + Postcode\n",
    "    6.4. Property + Outcode + Location\n",
    "    6.5. Property + Postcode + Location\n",
    " \n",
    " 7. Property Triple Location Combinations\n",
    "    7.1. Property + Location + City + Outcode\n",
    "    7.2. Property + Location + City + Postcode\n",
    "    7.3. Property + Outcode + Postcode + Location\n",
    " \n",
    " 8. Full Property Profile\n",
    "    - All features combined\n",
    " \n",
    "The feature groupings above test combinations in three logical steps:\n",
    "\n",
    " 1. Core property metrics (bedrooms, bathrooms, receptions)\n",
    " 2. Property classification combinations (adding house type and area)\n",
    " 3. Location data at different scales (outcode, full location, city/county, postal code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This helps identify which features and geographic granularity provide the best predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define feature subsets\n",
    "feature_subsets = {\n",
    "    # Basic Features\n",
    "    'Basic': ['No. of Bedrooms', ],\n",
    "    \n",
    "    # Minimal Extended Features\n",
    "    'Minimal + Area': ['No. of Bedrooms',  'Area in sq ft'],\n",
    "    'Minimal + House Type': ['No. of Bedrooms',  'House Type'],\n",
    "    \n",
    "    # Property Features (Core)\n",
    "    'Property': ['No. of Bedrooms',  'Area in sq ft', 'House Type'],\n",
    "    \n",
    "    # Property Location Combinations\n",
    "    'Property + Outcode': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode'],\n",
    "    'Property + Location': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'Location'],\n",
    "    'Property + City': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'City/County'],\n",
    "    'Property + Postcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'Postal Code'],\n",
    "    \n",
    "    # Property Double Location Combinations\n",
    "    'Property + Location + City': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Location', 'City/County'],\n",
    "    'Property + City + Outcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'City/County', 'Outcode'],\n",
    "    'Property + City + Postcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'City/County', 'Postal Code'],\n",
    "    'Property + Outcode + Location': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode', 'Location'],\n",
    "    'Property + Postcode + Location': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Postal Code', 'Location'],\n",
    "    \n",
    "    # Property Triple Location Combinations\n",
    "    'Property + Location + City + Outcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'Location', 'City/County', 'Outcode'],\n",
    "    'Property + Location + City + Postcode': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Location', 'City/County', 'Postal Code'],\n",
    "    'Property + Outcode + Postcode + Location': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode', 'Postal Code', 'Location'],\n",
    "    \n",
    "    # Full Property Profile\n",
    "    'Full': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Location', 'City/County', 'Postal Code', 'Outcode']\n",
    "}\n",
    "\n",
    "def get_feature_columns(df, feature_subset):\n",
    "    \"\"\"\n",
    "    Gets all relevant columns including one-hot encoded ones for Location\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for feature in feature_subset:\n",
    "        if feature == 'Location':\n",
    "            # Add all Location_ columns for one-hot encoded data\n",
    "            location_cols = [col for col in df.columns if col.startswith('Location_')]\n",
    "            columns.extend(location_cols)\n",
    "        elif feature in df.columns:\n",
    "            columns.append(feature)    \n",
    "    return columns\n",
    "\n",
    "def evaluate_feature_subsets(df, feature_subsets):\n",
    "    \"\"\"\n",
    "    Evaluates model performance for different feature subsets\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for subset_name, features in feature_subsets.items():\n",
    "        print(f\"\\nEvaluating {subset_name} subset...\")\n",
    "        # Get relevant columns including one-hot encoded ones\n",
    "        selected_columns = get_feature_columns(df, features)\n",
    "        \n",
    "        # Add Price column to selected columns\n",
    "        selected_columns.append('Price')\n",
    "        \n",
    "        # Create subset of data\n",
    "        df_subset = df[selected_columns]\n",
    "        \n",
    "        # Prepare data using existing function\n",
    "        X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_subset)\n",
    "        \n",
    "        # Train model\n",
    "        model = DecisionTreeRegressor(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[subset_name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'Feature Count': X_train.shape[1]\n",
    "        }\n",
    "        \n",
    "        print(f\"Number of features: {X_train.shape[1]}\")\n",
    "        print(f\"MAE: £{mae:,.2f}\")\n",
    "        print(f\"RMSE: £{rmse:,.2f}\")\n",
    "        print(f\"R2 Score: {r2:.4f}\")\n",
    "            \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "def plot_results(results_df):\n",
    "    \"\"\"\n",
    "    Creates plots comparing model performance across feature subsets\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # R² plot\n",
    "    results_df['R2'].plot(marker='o', ax=ax1)\n",
    "    ax1.set_title('R² Score by Feature Subset')\n",
    "    ax1.set_ylabel('R² Score')\n",
    "    ax1.grid(True)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RMSE plot\n",
    "    results_df['RMSE'].plot(marker='o', ax=ax2)\n",
    "    ax2.set_title('RMSE by Feature Subset')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.grid(True)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run evaluation on one-hot encoded dataset\n",
    "print(\"\\nEvaluating one-hot encoded dataset...\")\n",
    "\n",
    "results = evaluate_feature_subsets(df_onehot_with_outcode, feature_subsets)\n",
    "display(results)\n",
    "# Plot results\n",
    "plot_results(results)\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nOne-hot Encoded Dataset Results:\")\n",
    "print(results.sort_values('R2', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Subset Performance Analysis\n",
    "\n",
    "Right! Which feature subset performs the best?\n",
    "\n",
    "##### Feature Subset Performance (Ordered by R² Score) top 5:\n",
    "\n",
    "1. **Property + Outcode + Postcode + Location** (Best)\n",
    "   - MAE: £566,853\n",
    "   - RMSE: £1,283,867\n",
    "   - R² Score: 0.7615\n",
    "   - Features: 662\n",
    "\n",
    "2. **Property + Location + Postcode**\n",
    "   - MAE: £599,924\n",
    "   - RMSE: £1,481,158\n",
    "   - R² Score: 0.6826\n",
    "   - Features: 661\n",
    "\n",
    "3. **Full Dataset**\n",
    "   - MAE: £602,994\n",
    "   - RMSE: £1,599,813\n",
    "   - R² Score: 0.6297\n",
    "   - Features: 663\n",
    "\n",
    "4. **Property + Location + City + Outcode**\n",
    "   - MAE: £593,725\n",
    "   - RMSE: £1,587,228\n",
    "   - R² Score: 0.6355\n",
    "   - Features: 662\n",
    "\n",
    "5. **Property + Location + City + Postcode**\n",
    "   - MAE: £599,672\n",
    "   - RMSE: £1,567,275\n",
    "   - R² Score: 0.6446\n",
    "   - Features: 662\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### 1. Feature Importance\n",
    "- Basic property features alone (bedrooms, area) perform poorly (R² = 0.1749)\n",
    "- Adding area improves performance significantly (R² increases to 0.2919)\n",
    "- Location features provide the biggest boost to performance\n",
    "- The combination of granular location data (Postcode + Outcode) with property features performs best\n",
    "\n",
    "#### 2. Model Evolution\n",
    "- Basic → +Area: +11.7% R² improvement\n",
    "- +House Type: +7.66% additional improvement\n",
    "- +Location features: +23.94% additional improvement\n",
    "- Fine-grained location (Postcode + Outcode): +19.36% final improvement\n",
    "\n",
    "#### 3. Optimal Feature Set\n",
    "The best performing combination includes:\n",
    "- Property characteristics (bedrooms, area, house type)\n",
    "- Outcode (broader postal area)\n",
    "- Full postal code\n",
    "- Specific location details\n",
    "- This combination achieves significantly better results than using either postcode or outcode alone\n",
    "\n",
    "### Performance Metrics Analysis\n",
    "\n",
    "#### MAE (Mean Absolute Error)\n",
    "- Best: £566,853 (Property + Outcode + Postcode + Location)\n",
    "- Worst: £1,068,158 (Basic subset)\n",
    "- Adding location features reduces MAE by approximately 47%\n",
    "\n",
    "#### RMSE (Root Mean Square Error)\n",
    "- Best: £1,283,867 (Property + Outcode + Postcode + Location)\n",
    "- Worst: £2,388,161 (Basic subset)\n",
    "- Indicates presence of some large prediction errors even in best model, most likely due to the skew and large outliers in the data.\n",
    "\n",
    "#### R² Score\n",
    "- Best: 0.7615 (Property + Outcode + Postcode + Location)\n",
    "- Worst: 0.1749 (Basic subset)\n",
    "- Shows model explains 76.15% of price variance at best\n",
    "\n",
    "### Implications & Recommendations\n",
    "\n",
    "1. **Feature Selection Strategy**\n",
    "   - Retain all granular location data\n",
    "   - Include both outcode and full postal code\n",
    "   - Keep property characteristics as baseline features\n",
    "\n",
    "2. **Model Improvements**\n",
    "   - Consider feature engineering for more location interactions\n",
    "   - Investigate non-linear relationships, especially in property features\n",
    "   - Possible benefit from ensemble methods\n",
    "\n",
    "3. **Data Collection**\n",
    "   - Focus on gathering more detailed location data\n",
    "  - Get a better dataset, we've lost the rooms data of this dataset is poor quality!\n",
    "   - Consider additional property features\n",
    "   - Potential value in temporal data (sale dates, market conditions)\n",
    "\n",
    "4. **Practical Application**\n",
    "   - At present this model is probably not suitable for initial pricing estimates.\n",
    "   - We may be able to seperate the model in two, for smaller and larger properties, giving us a more accurate model for the smaller properties.\n",
    "   - Error margins should be communicated (±£566K on average)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Error Magnitude**\n",
    "   - Even best model has significant average error (£566K)\n",
    "   - RMSE indicates some very large prediction errors\n",
    "\n",
    "2. **Feature Complexity**\n",
    "   - Large number of features (662) may lead to overfitting\n",
    "   - Sparse matrix from location encoding\n",
    "\n",
    "3. **Location Dependency**\n",
    "   - Heavy reliance on location features\n",
    "   - May perform poorly in areas with limited data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Consider implementing:\n",
    "   - Feature reduction techniques (PCA)\n",
    "   - Ensemble methods (Random Forest, XGBoost)\n",
    "   - Cross-validation for more robust evaluation\n",
    "\n",
    "2. Explore:\n",
    "   - Feature interaction effects\n",
    "   - Non-linear transformations of numeric features\n",
    "   - More sophisticated location encoding methods\n",
    "\n",
    "3. Investigate:\n",
    "   - Temporal aspects of pricing\n",
    "   - Market condition indicators\n",
    "   - Regional price trends\n",
    "\n",
    "This updated analysis shows significant improvement over previous results, particularly in the R² score, but still indicates room for improvement in prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing model types: Random Forest\n",
    "\n",
    "Random forests are an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. Each tree in the forest is trained on a random subset of the data and features, and the final prediction is made by averaging the predictions of all trees. \n",
    "\n",
    "This approach helps reduce overfitting and improves generalisation compared to a single decision tree.\n",
    "\n",
    "It's called an ensemble method because random forests exemplify the \"wisdom of crowds\" principle - where combining many simpler models (the trees) leads to better performance than any individual model alone. This makes them particularly effective for complex regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def get_feature_columns(df, feature_subset):\n",
    "    \"\"\"\n",
    "    Gets all relevant columns including one-hot encoded ones for Location\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for feature in feature_subset:\n",
    "        if feature == 'Location':\n",
    "            # Add all Location_ columns for one-hot encoded data\n",
    "            location_cols = [col for col in df.columns if col.startswith('Location_')]\n",
    "            columns.extend(location_cols)\n",
    "        elif feature in df.columns:\n",
    "            columns.append(feature)    \n",
    "    return columns\n",
    "\n",
    "def train_random_forest(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a Random Forest model\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,  # Number of trees\n",
    "        max_depth=None,    # Let trees grow fully\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Get feature importance and aggregate location features\n",
    "    feature_importance = dict(zip(X_train.columns, rf_model.feature_importances_))\n",
    "    \n",
    "    # Aggregate location feature importance if present\n",
    "    if any('Location_' in col for col in X_train.columns):\n",
    "        location_importance = sum(\n",
    "            importance for col, importance in feature_importance.items() \n",
    "            if 'Location_' in col\n",
    "        )\n",
    "        # Add aggregated location importance\n",
    "        feature_importance['Location (aggregated)'] = location_importance\n",
    "        # Remove individual location features from importance dict\n",
    "        feature_importance = {k: v for k, v in feature_importance.items() \n",
    "                            if not k.startswith('Location_')}\n",
    "    \n",
    "    top_features = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    \n",
    "    print(\"\\nRandom Forest Results:\")\n",
    "    print(f\"MAE: £{mae:,.2f}\")\n",
    "    print(f\"RMSE: £{rmse:,.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for feature, importance in top_features.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    return rf_model, mae, rmse, r2, feature_importance\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = [\n",
    "    'No. of Bedrooms',\n",
    "    'Area in sq ft', 'House Type', 'Outcode', 'Location', 'Postal Code'\n",
    "]\n",
    "\n",
    "# Get relevant columns including one-hot encoded ones\n",
    "selected_columns = get_feature_columns(df_onehot_with_outcode, selected_features)\n",
    "selected_columns.append('Price')\n",
    "\n",
    "df_subset = df_onehot_with_outcode[selected_columns]\n",
    "# Prepare the data using the existing function\n",
    "X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_subset)\n",
    "\n",
    "# Train and evaluate the random forest\n",
    "rf_model, rf_mae, rf_rmse, rf_r2, rf_importance = train_random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Compare with previous Decision Tree results\n",
    "print(\"\\nComparison with Decision Tree:\")\n",
    "print(f\"{'Metric':<20} {'Decision Tree':<15} {'Random Forest':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'MAE':<20} £{566853:<14,.0f} £{rf_mae:<14,.0f}\")\n",
    "print(f\"{'RMSE':<20} £{1283867:<14,.0f} £{rf_rmse:<14,.0f}\")\n",
    "print(f\"{'R2 Score':<20} {0.7615:<14.3f} {rf_r2:<14.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! Our random forest on the property + location + postcode + outcode one hot encoded dataset has a lower MAE, yet a higher RMSE and lower R^2 score. \n",
    "\n",
    "```code\n",
    "Comparison with Decision Tree:\n",
    "Metric               Decision Tree   Random Forest  \n",
    "--------------------------------------------------\n",
    "MAE                  £566,853        £496,928       \n",
    "RMSE                 £1,283,867      £1,424,932     \n",
    "R2 Score             0.761          0.706  \n",
    "```\n",
    "\n",
    "Lets compare our feature subsets of the one-hot encoded dataset using the random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define feature subsets\n",
    "feature_subsets = {\n",
    "    # Basic Features\n",
    "    'Basic': ['No. of Bedrooms', ],\n",
    "    \n",
    "    # Minimal Extended Features\n",
    "    'Minimal + Area': ['No. of Bedrooms',  'Area in sq ft'],\n",
    "    'Minimal + House Type': ['No. of Bedrooms',  'House Type'],\n",
    "    \n",
    "    # Property Features (Core)\n",
    "    'Property': ['No. of Bedrooms',  'Area in sq ft', 'House Type'],\n",
    "    \n",
    "    # Property Location Combinations\n",
    "    'Property + Outcode': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode'],\n",
    "    'Property + Location': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'Location'],\n",
    "    'Property + City': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'City/County'],\n",
    "    'Property + Postcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'Postal Code'],\n",
    "    \n",
    "    # Property Double Location Combinations\n",
    "    'Property + Location + City': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Location', 'City/County'],\n",
    "    'Property + City + Outcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'City/County', 'Outcode'],\n",
    "    'Property + City + Postcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'City/County', 'Postal Code'],\n",
    "    'Property + Outcode + Location': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode', 'Location'],\n",
    "    'Property + Postcode + Location': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Postal Code', 'Location'],\n",
    "    \n",
    "    # Property Triple Location Combinations\n",
    "    'Property + Location + City + Outcode': ['No. of Bedrooms',  'Area in sq ft', 'House Type', 'Location', 'City/County', 'Outcode'],\n",
    "    'Property + Location + City + Postcode': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Location', 'City/County', 'Postal Code'],\n",
    "    'Property + Outcode + Postcode + Location': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode', 'Postal Code', 'Location'],\n",
    "    \n",
    "    # Full Property Profile\n",
    "    'Full': ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Location', 'City/County', 'Postal Code', 'Outcode']\n",
    "}\n",
    "\n",
    "def get_feature_columns(df, feature_subset):\n",
    "    \"\"\"\n",
    "    Gets all relevant columns including one-hot encoded ones for Location\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for feature in feature_subset:\n",
    "        if feature == 'Location':\n",
    "            # Add all Location_ columns for one-hot encoded data\n",
    "            location_cols = [col for col in df.columns if col.startswith('Location_')]\n",
    "            columns.extend(location_cols)\n",
    "        elif feature in df.columns:\n",
    "            columns.append(feature)    \n",
    "    return columns\n",
    "\n",
    "def evaluate_feature_subsets_rf(df, feature_subsets):\n",
    "    \"\"\"\n",
    "    Evaluates Random Forest model performance for different feature subsets\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    feature_importances = {}\n",
    "    \n",
    "    for subset_name, features in feature_subsets.items():\n",
    "        print(f\"\\nEvaluating {subset_name} subset...\")\n",
    "        selected_columns = get_feature_columns(df, features)\n",
    "        selected_columns.append('Price')\n",
    "        \n",
    "        df_subset = df[selected_columns]\n",
    "        X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_subset)\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Get feature importance\n",
    "        importances = dict(zip(X_train.columns, model.feature_importances_))\n",
    "        top_features = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "        feature_importances[subset_name] = top_features\n",
    "        \n",
    "        # Store results\n",
    "        results[subset_name] = {\n",
    "            'Mean Absolute Error': mae,\n",
    "            'Root Mean Squared Error': rmse,\n",
    "            'R-squared': r2,\n",
    "            'Feature Count': X_train.shape[1]\n",
    "        }\n",
    "        \n",
    "        print(f\"Number of features: {X_train.shape[1]}\")\n",
    "        print(f\"MAE: £{mae:,.2f}\")\n",
    "        print(f\"RMSE: £{rmse:,.2f}\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "        print(\"\\nTop 5 Most Important Features:\")\n",
    "        for feature, importance in top_features.items():\n",
    "            print(f\"{feature}: {importance:.4f}\")\n",
    "            \n",
    "    return pd.DataFrame(results).T, feature_importances\n",
    "\n",
    "def plot_results(results_df):\n",
    "    \"\"\"\n",
    "    Creates plots comparing model performance across feature subsets\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # R² plot\n",
    "    results_df['R-squared'].plot(marker='o', ax=ax1)\n",
    "    ax1.set_title('R² Score by Feature Subset')\n",
    "    ax1.set_ylabel('R² Score')\n",
    "    ax1.grid(True)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RMSE plot\n",
    "    results_df['Root Mean Squared Error'].plot(marker='o', ax=ax2)\n",
    "    ax2.set_title('RMSE by Feature Subset')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.grid(True)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAE plot\n",
    "    results_df['Mean Absolute Error'].plot(marker='o', ax=ax3)\n",
    "    ax3.set_title('MAE by Feature Subset')\n",
    "    ax3.set_ylabel('MAE')\n",
    "    ax3.grid(True)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\nEvaluating Random Forest models on one-hot encoded dataset...\")\n",
    "\n",
    "results_rf, feature_importances = evaluate_feature_subsets_rf(df_onehot_with_outcode, feature_subsets)\n",
    "display(results_rf)\n",
    "plot_results(results_rf)\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nRandom Forest Results (sorted by R-squared):\")\n",
    "print(results_rf.sort_values('R-squared', ascending=False))\n",
    "\n",
    "# Print feature importances for best performing model\n",
    "best_model = results_rf.sort_values('R-squared', ascending=False).index[0]\n",
    "print(f\"\\nFeature Importances for Best Model ({best_model}):\")\n",
    "for feature, importance in feature_importances[best_model].items():\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Based on the results, the \"Property + Location + City + Outcode\" combination model is the best for the following reasons:\n",
    "\n",
    " Best Performance Metrics:\n",
    " - Strong R-squared (0.71): Explains 71% of the variance in house prices\n",
    " - Lowest Mean Absolute Error (£491,578) among all models\n",
    " - Very competitive Root Mean Squared Error (£1,408,315)\n",
    "\n",
    " Model Characteristics:\n",
    " - Uses 662 features to capture complex relationships\n",
    " - Combines property fundamentals with comprehensive location data:\n",
    "   - Basic property features (bedrooms, area)\n",
    "   - Detailed location (specific area)\n",
    "   - City/County level context\n",
    "   - Outcode for broader geographical grouping\n",
    "\n",
    " Performance Comparison:\n",
    " - Significantly outperforms simpler models:\n",
    "   - Basic: R² = 0.19, MAE = £1,059,008\n",
    "   - Property only: R² = 0.52, MAE = £808,336\n",
    " - Marginally better than other complex models:\n",
    "   - Full model (663 features): MAE = £492,159\n",
    "   - Property + City + Postcode (5 features): MAE = £529,612\n",
    "\n",
    " Let's validate this model's performance without one-hot encoding to see if we can maintain accuracy with simpler feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_feature_subsets_on_original_data_with_random_forest(df, feature_subsets):\n",
    "    \"\"\"\n",
    "    Evaluates Random Forest model performance for different feature subsets\n",
    "    \"\"\"\n",
    "\n",
    "    # selected_columns = ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'City/County', 'Postal Code', 'Location']\n",
    "    # selected_columns = ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'City/County', 'Postal Code', 'Location', \"Outcode\"]\n",
    "    selected_columns = ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Postal Code', 'Location', \"Outcode\"]\n",
    "\n",
    "    selected_columns.append('Price')\n",
    "    \n",
    "    df_subset = df[selected_columns]\n",
    "    X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_subset)\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = dict(zip(X_train.columns, model.feature_importances_))\n",
    "    top_features = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    results = {\n",
    "        'Mean Absolute Error': mae,\n",
    "        'Root Mean Squared Error': rmse,\n",
    "        'R-squared': r2,\n",
    "        'Feature Count': X_train.shape[1]\n",
    "    }\n",
    "    \n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    print(f\"MAE: £{mae:,.2f}\")\n",
    "    print(f\"RMSE: £{rmse:,.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for feature, importance in top_features.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "        \n",
    "    return results, model, X_test\n",
    "\n",
    "\n",
    "# # Run evaluation\n",
    "print(\"\\nEvaluating Random Forest models on original dataset...\")\n",
    "\n",
    "results_rf, random_forest_model, X_test = evaluate_feature_subsets_on_original_data_with_random_forest(df_original_with_outcode, feature_subsets)\n",
    "display(results_rf)\n",
    "\n",
    "# used to test saved model later in lesson\n",
    "random_forest_test_sample = X_test.sample(10)\n",
    "# display(random_forest_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random forest model performs better on the one-hot encoded dataset than the original dataset. Interestingly our original dataset has a similar (albiet slightly higher) R^2 score, yet poorer error metrics.\n",
    "\n",
    "One-hot encoded dataset - property + postcode + location + outcode:\n",
    " - Mean Absolute Error: £491,578\n",
    " - Root Mean Squared Error: £1,408,315\n",
    " - R-squared: 0.7065\n",
    "\n",
    "Original dataset - property + postcode + location + outcode:\n",
    " - Number of features: 6\n",
    " - MAE: £520,728.92\n",
    " - RMSE: £1,399,084.58\n",
    " - R² Score: 0.7168\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models: XGBoost gradient boosting\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is another powerful ensemble learning method that builds on the principles of gradient boosting. It sequentially creates decision trees where each new tree tries to correct the errors made by the previous trees.\n",
    "\n",
    "What makes XGBoost particularly effective is its use of regularisation techniques to prevent overfitting, along with optimisations that make it computationally efficient. It has become one of the most popular algorithms for structured/tabular data due to its strong predictive performance and ability to handle complex relationships in data.\n",
    "\n",
    "Like random forests, XGBoost combines multiple trees, but does so in a more focused way by giving more weight to previously misclassified examples. This often results in better performance than random forests, especially for complex regression tasks like house price prediction.\n",
    "\n",
    "Finally lets have a look at how we could implement XGBoost gradient boosting to our original and our onehot encoded dataset for feature subset: property + postcode + location + outcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def evaluate_feature_subsets_on_original_data_with_xgboost(df, feature_subsets):\n",
    "    \"\"\"\n",
    "    Evaluates XGBoost model performance for different feature subsets\n",
    "    \"\"\"\n",
    "\n",
    "    selected_columns = ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Postal Code', 'Location', \"Outcode\"]\n",
    "    selected_columns.append('Price')\n",
    "    \n",
    "    df_subset = df[selected_columns]\n",
    "    X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_subset)\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = dict(zip(X_train.columns, model.feature_importances_))\n",
    "    top_features = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'Mean Absolute Error': mae,\n",
    "        'Root Mean Squared Error': rmse,\n",
    "        'R-squared': r2,\n",
    "        'Feature Count': X_train.shape[1]\n",
    "    }\n",
    "    \n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    print(f\"MAE: £{mae:,.2f}\")\n",
    "    print(f\"RMSE: £{rmse:,.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for feature, importance in top_features.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "        \n",
    "    return results, model, X_test\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\nEvaluating XGBoost models on original dataset...\")\n",
    "\n",
    "results_xgb, xgb_original_model, xgb_x_test_sample = evaluate_feature_subsets_on_original_data_with_xgboost(df_original_with_outcode, feature_subsets)\n",
    "display(results_xgb)\n",
    "\n",
    "# used to test saved model later in lesson\n",
    "xgb_test_sample = xgb_x_test_sample.sample(10)\n",
    "display(xgb_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def get_feature_columns(df, feature_subset):\n",
    "    \"\"\"\n",
    "    Gets all relevant columns including one-hot encoded ones for Location\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for feature in feature_subset:\n",
    "        if feature == 'Location':\n",
    "            # Add all Location_ columns for one-hot encoded data\n",
    "            location_cols = [col for col in df.columns if col.startswith('Location_')]\n",
    "            columns.extend(location_cols)\n",
    "        elif feature in df.columns:\n",
    "            columns.append(feature)    \n",
    "    return columns\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost model\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Get feature importance and aggregate location features\n",
    "    feature_importance = dict(zip(X_train.columns, xgb_model.feature_importances_))\n",
    "    \n",
    "    # Aggregate location feature importance if present\n",
    "    if any('Location_' in col for col in X_train.columns):\n",
    "        location_importance = sum(\n",
    "            importance for col, importance in feature_importance.items() \n",
    "            if 'Location_' in col\n",
    "        )\n",
    "        # Add aggregated location importance\n",
    "        feature_importance['Location (aggregated)'] = location_importance\n",
    "        # Remove individual location features from importance dict\n",
    "        feature_importance = {k: v for k, v in feature_importance.items() \n",
    "                            if not k.startswith('Location_')}\n",
    "    \n",
    "    top_features = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    \n",
    "    print(\"\\nXGBoost Results:\")\n",
    "    print(f\"MAE: £{mae:,.2f}\")\n",
    "    print(f\"RMSE: £{rmse:,.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for feature, importance in top_features.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    return xgb_model, mae, rmse, r2, feature_importance\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = [\n",
    "    'No. of Bedrooms',\n",
    "    'Area in sq ft', 'House Type', 'Outcode', 'Location', 'Postal Code'\n",
    "]\n",
    "\n",
    "# Get relevant columns including one-hot encoded ones\n",
    "selected_columns = get_feature_columns(df_onehot_with_outcode, selected_features)\n",
    "selected_columns.append('Price')\n",
    "\n",
    "df_subset = df_onehot_with_outcode[selected_columns]\n",
    "# Prepare the data using the existing function\n",
    "X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_subset)\n",
    "\n",
    "# Train and evaluate XGBoost\n",
    "xgb_model, xgb_mae, xgb_rmse, xgb_r2, xgb_importance = train_xgboost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woahey! XGBoost is a powerful model that performs better on both the original and one-hot encoded datasets.\n",
    "\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "Features included: ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Postal Code', 'Location', 'Outcode']\n",
    "\n",
    "- Number of features: 6\n",
    "- MAE: £474,797.03\n",
    "- RMSE: £1,296,308.09\n",
    "- R² Score: 0.7569\n",
    "\n",
    "One-hot encoded dataset:\n",
    "\n",
    "Features included: ['No. of Bedrooms', 'Area in sq ft', 'House Type', 'Outcode', 'Location', 'Postal Code']\n",
    "\n",
    "XGBoost Results:\n",
    "- Number of features: 662\n",
    "- MAE: £472,931.22\n",
    "- RMSE: £1,301,992.45\n",
    "- R2 Score: 0.7548\n",
    "\n",
    "\n",
    "MAE, RMSE are lower for both models, but the R^2 score is higher for both models than when compared to both decision trees and random forest. \n",
    "\n",
    "The original dataset using XGBoost has the best score of all the models so far.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss interpretability, Bias-Variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cover hyperparameter tuning for decision trees and random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence (saving and loading trained models)\n",
    "\n",
    "When working with trained models, it's essential to know how to save and load them for future use. This allows you to:\n",
    "\n",
    "- Save training time by reusing trained models\n",
    "- Deploy models in production environments\n",
    "- Share models with team members\n",
    "- Version control your models\n",
    "\n",
    "Let's look at different methods for saving our trained Decision Tree and Random Forest models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import sklearn\n",
    "import xgboost\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Model Persistence Guide\n",
    "----------------------\n",
    "This script demonstrates different methods for saving and loading machine learning models.\n",
    "Key methods covered:\n",
    "1. joblib - Efficient for scikit-learn models with NumPy arrays\n",
    "2. pickle - Python's native serialization\n",
    "3. XGBoost's native format - Optimized for XGBoost models\n",
    "4. Version control and metadata management\n",
    "\n",
    "Each method has its pros and cons:\n",
    "- joblib: Best for scikit-learn models with large NumPy arrays\n",
    "- pickle: More flexible but slower for large NumPy arrays\n",
    "- XGBoost native: Most efficient for XGBoost models\n",
    "\"\"\"\n",
    "\n",
    "# # Create base directory for models\n",
    "# os.makedirs('models', exist_ok=True)\n",
    "\n",
    "###########################################\n",
    "# 1. Decision Tree Model (scikit-learn)\n",
    "###########################################\n",
    "\n",
    "# Create comprehensive metadata dictionary\n",
    "# This helps track model versions, performance, and dependencies\n",
    "dt_model_info = {\n",
    "    'model_version': '1.0',\n",
    "    'model_type': 'DecisionTreeRegressor',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'model_params': decision_tree_model.get_params(),\n",
    "    'performance': {\n",
    "        'mae': 474797.03,\n",
    "        'rmse': 1296308.09,\n",
    "        'r2': 0.7569\n",
    "    },\n",
    "    'library_versions': {\n",
    "        'sklearn': sklearn.__version__\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save using joblib - recommended for scikit-learn models\n",
    "# joblib is optimized for numpy arrays and large datasets\n",
    "joblib.dump(decision_tree_model, '../models/decision_tree_model.joblib')\n",
    "\n",
    "# Save using pickle - alternative method\n",
    "# Pickle is Python's native serialization protocol\n",
    "with open('../models/decision_tree_model.pkl', 'wb') as f:\n",
    "    pickle.dump(decision_tree_model, f)\n",
    "\n",
    "###########################################\n",
    "# 2. Random Forest Model (scikit-learn)\n",
    "###########################################\n",
    "\n",
    "rf_model_info = {\n",
    "    'model_version': '1.0',\n",
    "    'model_type': 'RandomForestRegressor',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'model_params': random_forest_model.get_params(),\n",
    "    'performance': {\n",
    "        'mae': 520728.92,\n",
    "        'rmse': 1399084.58,\n",
    "        'r2': 0.7168\n",
    "    },\n",
    "    'library_versions': {\n",
    "        'sklearn': sklearn.__version__\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save using joblib with compression\n",
    "# compress=3 provides good balance between size and speed\n",
    "joblib.dump(random_forest_model, '../models/random_forest_model.joblib', compress=3)\n",
    "\n",
    "# Save using pickle with highest protocol for better performance\n",
    "# HIGHEST_PROTOCOL is usually faster and more efficient\n",
    "with open('../models/random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(random_forest_model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "###########################################\n",
    "# 3. XGBoost Model\n",
    "###########################################\n",
    "\n",
    "xgb_model_info = {\n",
    "    'model_version': '1.0',\n",
    "    'model_type': 'XGBRegressor',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'model_params': xgb_original_model.get_params(),\n",
    "    'performance': {\n",
    "        'mae': 472931.22,\n",
    "        'rmse': 1301992.45,\n",
    "        'r2': 0.7548\n",
    "    },\n",
    "    'library_versions': {\n",
    "        'xgboost': xgboost.__version__,\n",
    "        'sklearn': sklearn.__version__\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save using XGBoost's native format\n",
    "# This is the recommended method for XGBoost models\n",
    "# Advantages: smaller file size, faster loading, better version compatibility\n",
    "xgb_original_model.save_model('../models/xgboost_model.json')\n",
    "\n",
    "# Backup save using pickle\n",
    "# It's good practice to have multiple save formats\n",
    "with open('../models/xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_original_model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save metadata for all models\n",
    "# Storing metadata separately allows easy model information lookup\n",
    "# without loading the entire model\n",
    "for model_name, info in [\n",
    "    ('decision_tree', dt_model_info),\n",
    "    ('random_forest', rf_model_info),\n",
    "    ('xgboost', xgb_model_info)\n",
    "]:\n",
    "    with open(f'../models/{model_name}_info.json', 'w') as f:\n",
    "        json.dump(info, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved models and making predictions\n",
    "\n",
    "We can load the saved models and make predictions on a sample of the test data or new data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_and_predict(model_path, model_type, X_test):\n",
    "    \"\"\"\n",
    "    Load a model and make predictions\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved model\n",
    "        model_type: Type of model ('joblib', 'pickle', or 'xgboost')\n",
    "        X_test: Test data to predict on\n",
    "        \n",
    "    Returns:\n",
    "        Model and predictions\n",
    "    \"\"\"\n",
    "    if model_type == 'joblib':\n",
    "        model = joblib.load(model_path)\n",
    "    elif model_type == 'pickle':\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    elif model_type == 'xgboost':\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.load_model(model_path)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    return model, predictions\n",
    "\n",
    "# Load each model and make predictions\n",
    "dt_model, dt_predictions = load_and_predict(\n",
    "    '../models/decision_tree_model.joblib',\n",
    "    'joblib',\n",
    "    decision_tree_test_sample\n",
    ")\n",
    "\n",
    "rf_model, rf_predictions = load_and_predict(\n",
    "    '../models/random_forest_model.joblib',\n",
    "    'joblib',\n",
    "    random_forest_test_sample\n",
    ")\n",
    "\n",
    "xgb_original_model, xgb_predictions = load_and_predict(\n",
    "    '../models/xgboost_model.json',\n",
    "    'xgboost',\n",
    "    xgb_test_sample\n",
    ")\n",
    "\n",
    "\n",
    "# # Compare predictions\n",
    "prediction_results = pd.DataFrame({\n",
    "    'Decision Tree': dt_predictions,\n",
    "    'Random Forest': rf_predictions,\n",
    "    'XGBoost': xgb_predictions\n",
    "})\n",
    "\n",
    "display(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Decision Trees\n",
    "\n",
    "While powerful, decision trees have some limitations:\n",
    "\n",
    "1. **Overfitting**: Deep trees can learn rules that are too specific to the training data.\n",
    "2. **Instability**: Small changes in the data can result in very different trees. \n",
    "3. **Bias towards features with many levels**: Trees prefer to split on features with many distinct values.\n",
    "4. **Difficulty capturing some relationships**: Trees struggle to model linear or smooth relationships.\n",
    "5. **High variance**: Predictions can vary significantly based on the specific training data used.\n",
    "\n",
    "Ensemble methods like random forests can mitigate some of these issues.\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "When using machine learning for real-world applications like house price prediction, it's important to consider the potential ethical implications:\n",
    "\n",
    "- **Bias**: If the training data contains historical biases, the model may perpetuate these biases in its predictions.\n",
    "\n",
    "- **Transparency**: If the model is used to make important decisions (like mortgage approvals), there may be a legal or moral obligation to explain how it makes predictions.\n",
    "\n",
    "- **Privacy**: The model uses detailed personal information, so it's crucial to ensure that data is collected, stored, and used responsibly.\n",
    "\n",
    "As machine learning practitioners, it's our duty to strive for models that are fair, transparent, and respectful of privacy. This may involve techniques like bias auditing, model interpretability tools, and differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Deployment: From Development to Production\n",
    "\n",
    "Moving machine learning models from development to production requires careful consideration of both software engineering and ML-specific challenges. This guide bridges the gap between theoretical understanding and practical implementation.\n",
    "\n",
    "### 1. Model Serving Architecture: Beyond Basic REST APIs\n",
    "\n",
    "The serving architecture is your model's interface with the world. While simple REST APIs work for basic use cases, production deployments need to handle concerns like:\n",
    "- Request batching for efficiency\n",
    "- Model versioning and rollbacks\n",
    "- Load balancing and scaling\n",
    "- Request prioritization\n",
    "- Warm-up strategies to avoid cold starts\n",
    "- Comprehensive error handling\n",
    "\n",
    "Here's a practical implementation that addresses these concerns:\n",
    "\n",
    "```python\n",
    "# Advanced serving implementation with pools and warm-up\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel, validator\n",
    "from typing import Optional, Dict, List\n",
    "import asyncio\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: Dict[str, float]\n",
    "    request_id: str\n",
    "    priority: Optional[int] = 1  # Support priority queuing\n",
    "\n",
    "    @validator('features')\n",
    "    def validate_features(cls, v):\n",
    "        # Strict validation prevents issues downstream\n",
    "        required_features = {'square_feet', 'bedrooms', 'location'}\n",
    "        if missing := required_features - v.keys():\n",
    "            raise ValueError(f\"Missing required features: {missing}\")\n",
    "        return v\n",
    "\n",
    "class ModelServer:\n",
    "    \"\"\"\n",
    "    Production-ready model server with:\n",
    "    - Model pooling for concurrent requests\n",
    "    - Version management\n",
    "    - Warm-up handling\n",
    "    - Request queuing\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_pool = {}  # Version -> List[Model]\n",
    "        self.request_queue = asyncio.Queue()\n",
    "        self.is_warm = False  # Track warm-up state\n",
    "\n",
    "    @asynccontextmanager\n",
    "    async def get_model(self, version: str = 'latest'):\n",
    "        model = await self.model_pool[version].acquire()\n",
    "        try:\n",
    "            # Ensure model is warm before first prediction\n",
    "            if not self.is_warm:\n",
    "                await model.warmup()\n",
    "                self.is_warm = True\n",
    "            yield model\n",
    "        finally:\n",
    "            await self.model_pool[version].release(model)\n",
    "```\n",
    "\n",
    "### 2. Advanced Model Optimization\n",
    "\n",
    "Model optimization isn't just about making predictions faster - it's about finding the optimal balance between:\n",
    "- Inference latency\n",
    "- Memory usage\n",
    "- Prediction accuracy\n",
    "- Resource costs\n",
    "- Maintenance complexity\n",
    "\n",
    "Different use cases will prioritize these differently. For example, edge deployment might prioritize memory usage, while a high-throughput API might focus on latency.\n",
    "\n",
    "Here's an implementation that considers these trade-offs:\n",
    "\n",
    "```python\n",
    "class ModelOptimizer:\n",
    "    \"\"\"\n",
    "    Comprehensive model optimization with validation at each step.\n",
    "    Balances multiple optimization objectives while maintaining \n",
    "    model quality.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, validation_data):\n",
    "        self.model = model\n",
    "        self.validation_data = validation_data\n",
    "        self.baseline_metrics = self.evaluate(model)\n",
    "\n",
    "    def optimize(self, target_latency_ms: float, min_accuracy_drop: float = 0.01):\n",
    "        \"\"\"\n",
    "        Multi-stage optimization pipeline that:\n",
    "        1. Optimizes model structure (pruning, compression)\n",
    "        2. Improves memory layout\n",
    "        3. Optimizes inference paths\n",
    "        While maintaining accuracy within specified bounds\n",
    "        \"\"\"\n",
    "        optimized_model = self.model\n",
    "\n",
    "        # Stage 1: Structure Optimization\n",
    "        # Prune model while monitoring accuracy impact\n",
    "        optimized_model = self.optimize_structure(optimized_model)\n",
    "        \n",
    "        # Stage 2: Memory Layout\n",
    "        # Improve cache efficiency and reduce memory footprint\n",
    "        optimized_model = self.optimize_memory_layout(optimized_model)\n",
    "        \n",
    "        # Stage 3: Inference Optimization\n",
    "        # Speed up common prediction paths\n",
    "        optimized_model = self.optimize_inference(optimized_model)\n",
    "\n",
    "        # Validate all requirements are met\n",
    "        final_metrics = self.evaluate(optimized_model)\n",
    "        if not self.meets_requirements(final_metrics, target_latency_ms, min_accuracy_drop):\n",
    "            raise OptimizationError(\"Failed to meet optimization targets\")\n",
    "\n",
    "        return optimized_model, final_metrics\n",
    "```\n",
    "\n",
    "### 3. Monitoring and Drift Detection\n",
    "\n",
    "Production ML systems need three types of monitoring:\n",
    "1. System metrics (latency, throughput, resource usage)\n",
    "2. ML metrics (accuracy, predictions distribution)\n",
    "3. Business metrics (user satisfaction, revenue impact)\n",
    "\n",
    "Additionally, drift detection is crucial for maintaining model quality over time. Common types of drift include:\n",
    "- Feature drift (input distributions change)\n",
    "- Concept drift (relationship between features and target changes)\n",
    "- Data quality drift (degradation in input quality)\n",
    "\n",
    "Here's a monitoring implementation that covers these aspects:\n",
    "\n",
    "```python\n",
    "class MLMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive monitoring system that tracks system health,\n",
    "    model performance, and data quality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Separate collectors for different metric types\n",
    "        self.metrics = MetricsCollector()\n",
    "        self.drift_detector = DriftDetector()\n",
    "        self.performance_tracker = PerformanceTracker()\n",
    "\n",
    "    async def monitor_prediction(self, features, prediction, actual=None):\n",
    "        \"\"\"\n",
    "        Holistic monitoring of each prediction, covering:\n",
    "        - System performance (latency, resource usage)\n",
    "        - Prediction quality (confidence, distributions)\n",
    "        - Data quality (missing values, ranges)\n",
    "        \"\"\"\n",
    "        await asyncio.gather(\n",
    "            self.track_prediction_metrics(features, prediction),\n",
    "            self.check_drift(features),\n",
    "            self.monitor_performance(),\n",
    "            self.log_prediction(features, prediction)\n",
    "        )\n",
    "\n",
    "    async def check_drift(self, features):\n",
    "        \"\"\"\n",
    "        Multi-faceted drift detection using statistical tests\n",
    "        and distribution monitoring. Handles different types of\n",
    "        drift with appropriate statistical methods.\n",
    "        \"\"\"\n",
    "        drift_types = {\n",
    "            'feature_drift': self.drift_detector.check_feature_drift(features),\n",
    "            'concept_drift': self.drift_detector.check_concept_drift(features),\n",
    "            'data_quality_drift': self.drift_detector.check_data_quality()\n",
    "        }\n",
    "\n",
    "        if any(drift_types.values()):\n",
    "            await self.handle_drift(drift_types)\n",
    "```\n",
    "\n",
    "### 4. Testing Framework\n",
    "\n",
    "ML testing goes beyond traditional software testing. We need to verify:\n",
    "- Statistical performance (accuracy, precision, recall)\n",
    "- System performance (latency, throughput)\n",
    "- Edge cases and failure modes\n",
    "- Model behavior under load\n",
    "- Drift detection accuracy\n",
    "\n",
    "A comprehensive testing strategy should cover all these aspects while remaining maintainable and reliable.\n",
    "\n",
    "```python\n",
    "class MLTestSuite:\n",
    "    \"\"\"\n",
    "    End-to-end testing framework for ML systems.\n",
    "    Combines traditional software tests with ML-specific validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_service):\n",
    "        self.model_service = model_service\n",
    "        self.test_cases = self.load_test_cases()\n",
    "\n",
    "    async def run_comprehensive_tests(self):\n",
    "        \"\"\"\n",
    "        Full test suite that validates:\n",
    "        - Model accuracy on test set\n",
    "        - Performance under various loads\n",
    "        - System reliability and error handling\n",
    "        - Drift detection accuracy\n",
    "        - Edge case handling\n",
    "        \"\"\"\n",
    "        results = await asyncio.gather(\n",
    "            self.test_accuracy(),      # Statistical performance\n",
    "            self.test_performance(),   # System performance\n",
    "            self.test_reliability(),   # Error handling\n",
    "            self.test_edge_cases(),    # Boundary conditions\n",
    "            self.test_drift_detection(),  # Drift handling\n",
    "            self.test_load_handling()     # Load testing\n",
    "        )\n",
    "        \n",
    "        return self.generate_test_report(results)\n",
    "\n",
    "    async def test_load_handling(self):\n",
    "        \"\"\"\n",
    "        Sophisticated load testing that simulates real-world scenarios:\n",
    "        - Steady state load\n",
    "        - Sudden spikes\n",
    "        - Gradual ramp-up\n",
    "        - Mixed load patterns\n",
    "        \"\"\"\n",
    "        async with LoadGenerator() as generator:\n",
    "            patterns = [\n",
    "                ('steady', 100, 60),    # Baseline load\n",
    "                ('spike', 500, 10),     # Traffic spike\n",
    "                ('ramp', (10, 200), 30) # Gradual increase\n",
    "            ]\n",
    "            \n",
    "            for pattern_type, load, duration in patterns:\n",
    "                metrics = await generator.run_pattern(pattern_type, load, duration)\n",
    "                await self.analyze_load_metrics(metrics)\n",
    "```\n",
    "\n",
    "### 5. Production Deployment Pipeline\n",
    "\n",
    "Deploying ML models safely requires more care than traditional software deployments. Key considerations include:\n",
    "- Model versioning and artifact management\n",
    "- Gradual rollouts with monitoring\n",
    "- Automatic rollback capabilities\n",
    "- Performance comparison with previous versions\n",
    "- Handling model warmup and cold starts\n",
    "\n",
    "Here's an implementation of a robust deployment pipeline:\n",
    "\n",
    "```python\n",
    "class ModelDeployment:\n",
    "    \"\"\"\n",
    "    Production deployment manager that handles safe rollouts\n",
    "    and monitoring of new model versions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.current_model = None\n",
    "        self.rollback_model = None  # Keep previous version for rollbacks\n",
    "\n",
    "    async def deploy_new_version(self, new_model):\n",
    "        \"\"\"\n",
    "        Careful deployment process:\n",
    "        1. Validate new model thoroughly\n",
    "        2. Deploy gradually with monitoring\n",
    "        3. Enable quick rollback if needed\n",
    "        4. Handle traffic shifting safely\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Pre-deployment validation\n",
    "            await self.health_check(self.current_model)\n",
    "            await self.validate_model(new_model)\n",
    "            \n",
    "            # Gradual rollout with monitoring\n",
    "            await self.gradual_rollout(new_model)\n",
    "            \n",
    "            # Post-deployment monitoring\n",
    "            await self.monitor_deployment(new_model)\n",
    "            \n",
    "        except DeploymentError as e:\n",
    "            await self.rollback()\n",
    "            raise DeploymentFailed(f\"Deployment failed: {str(e)}\")\n",
    "\n",
    "    async def gradual_rollout(self, new_model):\n",
    "        \"\"\"\n",
    "        Traffic shifting with health monitoring at each stage.\n",
    "        Uses increasing traffic percentages with validation\n",
    "        at each step.\n",
    "        \"\"\"\n",
    "        traffic_splits = [\n",
    "            (0.1, 300),  # Start with 10% traffic\n",
    "            (0.5, 300),  # Increase to 50%\n",
    "            (1.0, 300)   # Full deployment\n",
    "        ]\n",
    "        \n",
    "        for traffic_fraction, duration in traffic_splits:\n",
    "            await self.shift_traffic(new_model, traffic_fraction)\n",
    "            await asyncio.sleep(duration)  # Allow metrics to stabilize\n",
    "            \n",
    "            # Continuous health checking\n",
    "            if not await self.check_health_metrics():\n",
    "                raise DeploymentError(\"Health check failed during rollout\")\n",
    "```\n",
    "\n",
    "### Key Takeaways and Best Practices\n",
    "\n",
    "1. **System Design**\n",
    "   - Plan for scalability from the start\n",
    "   - Consider both ML and system metrics\n",
    "   - Build in monitoring and testing from day one\n",
    "\n",
    "2. **Model Optimization**\n",
    "   - Balance multiple performance objectives\n",
    "   - Validate optimizations thoroughly\n",
    "   - Keep optimization pipeline maintainable\n",
    "\n",
    "3. **Monitoring**\n",
    "   - Monitor both technical and ML metrics\n",
    "   - Implement comprehensive drift detection\n",
    "   - Have clear incident response procedures\n",
    "\n",
    "4. **Testing**\n",
    "   - Go beyond accuracy metrics\n",
    "   - Test under realistic conditions\n",
    "   - Include performance and reliability tests\n",
    "\n",
    "5. **Deployment**\n",
    "   - Use gradual rollouts\n",
    "   - Monitor deployments closely\n",
    "   - Maintain rollback capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lesson, we've covered:\n",
    "\n",
    "- The intuition behind decision trees and how they make predictions\n",
    "- Different splitting criteria, including MSE and MAE\n",
    "- Preprocessing data for decision tree models, handling missing values, and feature engineering\n",
    "- Training and evaluating decision trees in scikit-learn\n",
    "- The impact of different feature subsets on model performance\n",
    "- Comparing decision trees to linear regression and random forests\n",
    "- The bias-variance trade-off and how it relates to model selection\n",
    "- Interpreting decision tree models and analyzing feature importances\n",
    "- Advanced techniques like hyperparameter tuning and ensemble methods\n",
    "- The limitations of decision trees and ethical considerations in their use\n",
    "\n",
    "Decision trees are a powerful and interpretable tool for regression and classification tasks. While they have limitations, they form the foundation for more advanced methods like random forests and gradient boosting.\n",
    "\n",
    "Understanding decision trees is crucial for any machine learning practitioner. They provide a solid grounding in the core concepts of supervised learning, and their interpretability makes them invaluable for explaining predictions to stakeholders.\n",
    "\n",
    "In the next lesson, we'll dive deeper into ensemble methods with random forests, seeing how they can improve upon the performance of single decision trees.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Scikit-learn documentation on decision trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "- [An Introduction to Statistical Learning, Chapter 8: Tree-Based Methods](http://faculty.marshall.usc.edu/gareth-james/ISL/)\n",
    "- [Elements of Statistical Learning, Chapter 9: Additive Models, Trees, and Related Methods](https://web.stanford.edu/~hastie/ElemStatLearn/)\n",
    "- [Kaggle course on Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "- [Google's Machine Learning Crash Course, Descending into ML: Training and Loss](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)\n",
    "- [Interpretable Machine Learning, A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)\n",
    "\n",
    "These resources will help deepen your understanding of decision trees and their place in the broader machine learning landscape. They cover the mathematical underpinnings, practical considerations, and cutting-edge techniques in model interpretability and explainability.\n",
    "\n",
    "Machine learning is a vast and rapidly evolving field, and there's always more to learn. I encourage you to actively experiment with these models, tune their parameters, and test them on different datasets. Hands-on experience is invaluable for building intuition and understanding.\n",
    "\n",
    "As you progress in your machine learning journey, always keep the end goal in mind: creating models that are not only accurate, but also transparent, fair, and beneficial to society. The technical skills are important, but the ethical considerations are just as crucial.\n",
    "\n",
    "I hope this lesson has provided a solid foundation for your exploration of decision trees and machine learning. Feel free to reach out if you have any further questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
