{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X1: Feature Engineering - The Art of Creating Better Features",
    "",
    "Feature engineering is often the difference between mediocre and winning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction",
    "",
    "**Andrew Ng's famous quote:**",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\"",
    "",
    "Good features make simple models work. Bad features make complex models fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents",
    "1. Why Feature Engineering Matters",
    "2. Handling Missing Data",
    "3. Encoding Categorical Variables",
    "4. Numerical Transformations",
    "5. Feature Scaling & Normalization",
    "6. Creating Interaction Features",
    "7. Polynomial Features",
    "8. Time-based Features",
    "9. Text Features",
    "10. Automated Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required library if not available\n# Uncomment the line below if running locally and category-encoders is not installed\n# !pip install category-encoders\n\ntry:\n    import category_encoders\n    print(f'‚úÖ category-encoders version {category_encoders.__version__} found')\nexcept ImportError:\n    print('‚ö†Ô∏è category-encoders not found. Installing...')\n    import sys\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"category-encoders\"])\n    import category_encoders\n    print(f'‚úÖ category-encoders version {category_encoders.__version__} installed successfully!')\n\n# Import all required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import TargetEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('‚úÖ All libraries loaded successfully!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling Missing Data",
    "",
    "**Three Strategies:**",
    "",
    "**A. Deletion:**",
    "- Drop rows: When <5% missing",
    "- Drop columns: When >70% missing",
    "",
    "**B. Imputation:**",
    "- Mean/Median: Numerical features",
    "- Mode: Categorical features",
    "- Forward/Backward fill: Time series",
    "- Model-based: Predict missing values",
    "",
    "**C. Flag Creation:**",
    "- Create 'is_missing' binary feature",
    "- Often helps models learn patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values",
    "df = pd.DataFrame({",
    "    'age': [25, np.nan, 35, 45, np.nan],",
    "    'income': [50000, 60000, np.nan, 80000, 75000],",
    "    'city': ['NYC', 'LA', np.nan, 'Chicago', 'NYC']",
    "})",
    "",
    "print('Original data:')",
    "print(df)",
    "print(f'\\nMissing values:\\n{df.isnull().sum()}')",
    "",
    "# Strategy 1: Simple imputation",
    "from sklearn.impute import SimpleImputer",
    "num_imputer = SimpleImputer(strategy='mean')",
    "df['age_imputed'] = num_imputer.fit_transform(df[['age']])",
    "df['income_imputed'] = SimpleImputer(strategy='median').fit_transform(df[['income']])",
    "",
    "# Strategy 2: Create missing flag",
    "df['age_was_missing'] = df['age'].isnull().astype(int)",
    "",
    "print('\\nAfter imputation:')",
    "print(df[['age', 'age_imputed', 'age_was_missing']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding Categorical Variables",
    "",
    "**Methods:**",
    "",
    "**A. Label Encoding:** For ordinal data (low, medium, high)",
    "",
    "**B. One-Hot Encoding:** For nominal data with few categories (<10)",
    "- Creates binary column for each category",
    "- Avoids imposing false order",
    "",
    "**C. Target Encoding:** For high-cardinality features (>10 categories)",
    "- Replace category with mean target value",
    "- Risk of overfitting - use cross-validation",
    "",
    "**D. Frequency Encoding:** Replace with category frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ‚ö†Ô∏è CRITICAL: Avoiding Data Leakage in Target Encoding\n\n**Data leakage** is one of the most dangerous mistakes in machine learning. It happens when information from the test set \"leaks\" into the training process, causing models to appear much better than they actually are.\n\n### The Problem with Target Encoding\n\nWhen you replace a categorical variable with the mean of the target variable for that category, you MUST compute those means using ONLY the training data, never the full dataset.\n\n### ‚ùå WRONG (Data Leakage):\n```python\n# DON'T DO THIS!\ntarget_means = df.groupby('category')['target'].mean()  # Uses ALL data\ndf['category_encoded'] = df['category'].map(target_means)\n# THEN split train/test\nX_train, X_test, y_train, y_test = train_test_split(...)\n```\n\n**Why it's wrong:** You're using information from the test set to create features. Your model will learn patterns that include test data, making validation metrics unrealistically optimistic. In production, when you encounter truly new data, performance will drop dramatically.\n\n### ‚úÖ CORRECT (No Leakage):\n```python\n# Split FIRST\nX_train, X_test, y_train, y_test = train_test_split(...)\n\n# Compute encoding ONLY on training data\ntarget_means = X_train.groupby('category')['target'].mean()\n\n# Apply to both sets\nX_train['category_encoded'] = X_train['category'].map(target_means)\nX_test['category_encoded'] = X_test['category'].map(target_means)\n\n# Handle unseen categories in test set\nX_test['category_encoded'].fillna(y_train.mean(), inplace=True)\n```\n\n**The demonstration below shows both approaches side-by-side so you can see the difference and understand why this matters.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# One-Hot Encoding\ndf_cat = pd.DataFrame({'color': ['red', 'blue', 'green', 'red']})\ndf_onehot = pd.get_dummies(df_cat, columns=['color'], prefix='color')\n\nprint('One-Hot Encoding:')\nprint(df_onehot)\n\nprint('\\n' + '='*70)\nprint('TARGET ENCODING - Data Leakage Demonstration')\nprint('='*70)\n\n# Create sample data with train/test split\nnp.random.seed(42)\nn_samples = 100\ndf_full = pd.DataFrame({\n    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Boston'], n_samples),\n    'price': np.random.randint(200, 600, n_samples)\n})\n\n# Split into train and test\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df_full, test_size=0.3, random_state=42)\n\nprint(f'\\nDataset: {len(train_df)} train samples, {len(test_df)} test samples')\nprint('\\n‚ö†Ô∏è WARNING: WRONG APPROACH (Data Leakage) ‚ö†Ô∏è')\nprint('-' * 70)\n\n# WRONG: Computing statistics on full dataset before split\nwrong_target_means = df_full.groupby('city')['price'].mean()\ntrain_df_wrong = train_df.copy()\ntest_df_wrong = test_df.copy()\ntrain_df_wrong['city_encoded_WRONG'] = train_df_wrong['city'].map(wrong_target_means)\ntest_df_wrong['city_encoded_WRONG'] = test_df_wrong['city'].map(wrong_target_means)\n\nprint('Encoding computed on FULL dataset (includes test data!):')\nprint(wrong_target_means)\nprint('\\n‚ùå Problem: Test set statistics leaked into training!')\nprint('‚ùå Model will appear better than it actually is')\nprint('‚ùå Performance will drop significantly in production')\n\nprint('\\n‚úÖ CORRECT APPROACH (No Leakage) ‚úÖ')\nprint('-' * 70)\n\n# RIGHT: Computing statistics ONLY on training data\ncorrect_target_means = train_df.groupby('city')['price'].mean()\ntrain_df_correct = train_df.copy()\ntest_df_correct = test_df.copy()\n\n# Apply to training data\ntrain_df_correct['city_encoded'] = train_df_correct['city'].map(correct_target_means)\n\n# Apply to test data with fallback for unseen categories\nglobal_mean = train_df['price'].mean()\ntest_df_correct['city_encoded'] = test_df_correct['city'].map(correct_target_means).fillna(global_mean)\n\nprint('Encoding computed ONLY on training data:')\nprint(correct_target_means)\nprint('\\n‚úÖ Training data uses its own statistics')\nprint('‚úÖ Test data uses training statistics (as it should)')\nprint('‚úÖ Unseen categories filled with global mean')\nprint('‚úÖ No information from test set leaked!')\n\n# Show comparison\nprint('\\n' + '='*70)\nprint('COMPARISON: Wrong vs Correct Encoding Values')\nprint('='*70)\ncomparison = pd.DataFrame({\n    'City': ['NYC', 'LA', 'Chicago', 'Boston'],\n    'Wrong (full data)': [wrong_target_means.get(c, 0) for c in ['NYC', 'LA', 'Chicago', 'Boston']],\n    'Correct (train only)': [correct_target_means.get(c, 0) for c in ['NYC', 'LA', 'Chicago', 'Boston']],\n    'Difference': [abs(wrong_target_means.get(c, 0) - correct_target_means.get(c, 0)) \n                   for c in ['NYC', 'LA', 'Chicago', 'Boston']]\n})\nprint(comparison.to_string(index=False))\n\nprint('\\n' + '='*70)\nprint('BEST PRACTICE: Use sklearn TargetEncoder with proper CV')\nprint('='*70)\n\n# Using sklearn's TargetEncoder (handles cross-validation internally)\nfrom category_encoders import TargetEncoder\n\nencoder = TargetEncoder(cols=['city'], smoothing=1.0)\ntrain_df_sklearn = train_df[['city', 'price']].copy()\ntest_df_sklearn = test_df[['city', 'price']].copy()\n\n# Fit on training data only\nencoder.fit(train_df_sklearn['city'], train_df_sklearn['price'])\n\n# Transform both sets\ntrain_df_sklearn['city_encoded'] = encoder.transform(train_df_sklearn['city'])\ntest_df_sklearn['city_encoded'] = encoder.transform(test_df_sklearn['city'])\n\nprint('\\n‚úÖ TargetEncoder handles:')\nprint('  ‚Ä¢ Fitting only on training data')\nprint('  ‚Ä¢ Smoothing to prevent overfitting')\nprint('  ‚Ä¢ Unseen categories automatically')\nprint('  ‚Ä¢ Can use cross-validation to reduce overfitting')\n\nprint('\\nüéØ KEY TAKEAWAY:')\nprint('  ALWAYS fit encoders on training data ONLY!')\nprint('  NEVER use test set statistics during training!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical Transformations",
    "",
    "**Common Transformations:**",
    "",
    "**A. Log Transform:** For skewed data",
    "- $x' = \\log(x + 1)$",
    "- Makes distribution more Gaussian",
    "",
    "**B. Square Root:** For count data",
    "- $x' = \\sqrt{x}$",
    "",
    "**C. Box-Cox:** Automatic optimal transformation",
    "",
    "**D. Binning:** Convert continuous to categorical",
    "- Equal-width bins",
    "- Equal-frequency bins (quantiles)",
    "- Custom thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation for skewed data",
    "skewed_data = np.exp(np.random.normal(0, 1, 1000))  # Highly skewed",
    "",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))",
    "ax1.hist(skewed_data, bins=50)",
    "ax1.set_title('Original (Skewed)', fontweight='bold')",
    "ax1.set_xlabel('Value')",
    "",
    "log_transformed = np.log1p(skewed_data)",
    "ax2.hist(log_transformed, bins=50)",
    "ax2.set_title('Log Transformed (More Gaussian)', fontweight='bold')",
    "ax2.set_xlabel('Log(Value + 1)')",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "print(f'Original skewness: {pd.Series(skewed_data).skew():.2f}')",
    "print(f'Log-transformed skewness: {pd.Series(log_transformed).skew():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling",
    "",
    "**Why Scale?** Many algorithms (KNN, SVM, Neural Networks) are sensitive to feature magnitude.",
    "",
    "**Methods:**",
    "",
    "**A. Standardization (Z-score):**",
    "- $x' = \\frac{x - \\mu}{\\sigma}$",
    "- Mean=0, Std=1",
    "- Use when data is Gaussian",
    "",
    "**B. Min-Max Normalization:**",
    "- $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$",
    "- Range: [0, 1]",
    "- Use when need bounded range",
    "",
    "**C. Robust Scaling:**",
    "- Uses median and IQR (robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of scaling methods",
    "data = np.array([[1, 2000], [2, 3000], [3, 5000], [4, 10000], [5, 2500]])",
    "df_scale = pd.DataFrame(data, columns=['feature1', 'feature2'])",
    "",
    "print('Original data:')",
    "print(df_scale)",
    "",
    "# StandardScaler",
    "scaler_std = StandardScaler()",
    "df_std = pd.DataFrame(scaler_std.fit_transform(df_scale), columns=['f1_std', 'f2_std'])",
    "",
    "# MinMaxScaler  ",
    "scaler_mm = MinMaxScaler()",
    "df_mm = pd.DataFrame(scaler_mm.fit_transform(df_scale), columns=['f1_mm', 'f2_mm'])",
    "",
    "print('\\nStandardized:')",
    "print(df_std.head())",
    "print('\\nMin-Max Normalized:')",
    "print(df_mm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Features",
    "",
    "**Idea:** Combine features to capture relationships",
    "",
    "**Examples:**",
    "- Price √ó Quantity = Total Revenue",
    "- Height √ó Weight = BMI indicator",
    "- Day √ó Hour = Time of day patterns",
    "",
    "**Polynomial Features:**",
    "- Degree 2: $[x_1, x_2] ‚Üí [x_1, x_2, x_1^2, x_1x_2, x_2^2]$",
    "- Captures non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features",
    "X = np.array([[2, 3], [4, 5]])",
    "poly = PolynomialFeatures(degree=2, include_bias=False)",
    "X_poly = poly.fit_transform(X)",
    "",
    "print('Original features: [x1, x2]')",
    "print(X)",
    "print('\\nPolynomial features (degree=2): [x1, x2, x1¬≤, x1*x2, x2¬≤]')",
    "print(X_poly)",
    "print(f'\\nFeature names: {poly.get_feature_names_out()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time-Based Features",
    "",
    "**From timestamps, extract:**",
    "- Hour, day of week, month, quarter, year",
    "- Is_weekend, is_holiday",
    "- Days since epoch",
    "- Cyclical encoding (sin/cos for hour/month)",
    "",
    "**Cyclical Encoding:**",
    "- Hour 23 and Hour 0 are close, but numerically 23 apart",
    "- Solution: $hour\\_sin = \\sin(2\\pi \\times hour / 24)$",
    "- $hour\\_cos = \\cos(2\\pi \\times hour / 24)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based feature extraction",
    "dates = pd.date_range('2024-01-01', periods=5, freq='12H')",
    "df_time = pd.DataFrame({'timestamp': dates})",
    "",
    "# Extract features",
    "df_time['hour'] = df_time['timestamp'].dt.hour",
    "df_time['day_of_week'] = df_time['timestamp'].dt.dayofweek",
    "df_time['is_weekend'] = (df_time['day_of_week'] >= 5).astype(int)",
    "",
    "# Cyclical encoding for hour",
    "df_time['hour_sin'] = np.sin(2 * np.pi * df_time['hour'] / 24)",
    "df_time['hour_cos'] = np.cos(2 * np.pi * df_time['hour'] / 24)",
    "",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Automated Feature Engineering\n\nAutomated feature engineering tools can generate hundreds of features from your raw data, but they require careful application to avoid overfitting.\n\n### Popular Tools\n\n**Featuretools** - Deep Feature Synthesis\n- Automatically creates features from relational datasets\n- Handles multi-table databases\n- Generates aggregation and transformation features\n\n**tsfresh** - Time Series Features\n- Extracts 700+ features from time series data\n- Built-in feature selection\n- Statistical and signal processing features\n\n**AutoFeat** - Linear Models\n- Generates non-linear features for linear models\n- Automatic feature selection\n- Particularly good for regression problems\n\n### Why We Don't Cover Them In-Depth Here\n\nThe manual feature engineering techniques you've learned in this notebook (encoding, scaling, transformations, interactions, time features) form the foundation that you MUST understand before using automated tools. Automated tools should augment your domain knowledge, not replace it.\n\n### Learning Resources\n\nIf you want to explore automated feature engineering:\n\n**Featuretools:**\n- Official Tutorial: https://docs.featuretools.com/en/stable/getting_started/getting_started.html\n- GitHub: https://github.com/alteryx/featuretools\n- Best for: Relational databases, multi-table data\n\n**Example Workflow:**\n```python\n# Install: pip install featuretools\nimport featuretools as ft\n\n# Create entity set\nes = ft.EntitySet(id='my_data')\nes = es.add_dataframe(dataframe_name='transactions', \n                      dataframe=df, \n                      index='transaction_id',\n                      time_index='timestamp')\n\n# Generate features automatically\nfeature_matrix, feature_defs = ft.dfs(\n    entityset=es,\n    target_dataframe_name='transactions',\n    max_depth=2,  # How many levels of features to create\n    verbose=True\n)\n```\n\n### Best Practices for Automated Feature Engineering\n\n1. **Start Manual** - Create domain-knowledge features first\n2. **Understand Output** - Review generated features, don't blindly use all\n3. **Feature Selection** - Most generated features won't be useful\n4. **Cross-Validation** - Essential to avoid overfitting with many features\n5. **Computation Cost** - Can be slow on large datasets\n6. **Interpretability** - Complex generated features may be hard to explain\n\n### When to Use Automated Tools\n\n‚úÖ **Good for:**\n- Exploration phase of a project\n- Finding non-obvious feature interactions\n- Kaggle competitions where performance matters most\n- Large relational databases\n\n‚ùå **Not ideal for:**\n- Production systems requiring interpretability\n- When you have limited data (risk of overfitting)\n- Real-time systems (computation overhead)\n- When domain expertise is critical\n\n### The Bottom Line\n\n**Master manual feature engineering first** (this notebook), then explore automated tools to augment your work. The techniques you've learned here - handling missing data, encoding categories correctly, avoiding data leakage, scaling features, and creating interactions - are fundamental skills that will serve you in every machine learning project."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "**Feature Engineering Checklist:**",
    "",
    "**Data Cleaning:**",
    "- ‚òë Handle missing values",
    "- ‚òë Remove duplicates",
    "- ‚òë Fix data types",
    "",
    "**Encoding:**",
    "- ‚òë One-hot encode nominal categories (<10 levels)",
    "- ‚òë Target encode high-cardinality (>10 levels)",
    "- ‚òë Label encode ordinal categories",
    "",
    "**Transformations:**",
    "- ‚òë Log transform skewed features",
    "- ‚òë Scale features for distance-based models",
    "- ‚òë Create polynomial/interaction features",
    "",
    "**Domain-Specific:**",
    "- ‚òë Extract time-based features",
    "- ‚òë Create ratios and aggregations",
    "- ‚òë Add domain knowledge features",
    "",
    "**Validation:**",
    "- ‚òë Check for data leakage",
    "- ‚òë Validate with cross-validation",
    "- ‚òë Analyze feature importance",
    "",
    "**Remember:** Good features > Complex models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}