{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X1: Feature Engineering - The Art of Creating Better Features",
    "",
    "Feature engineering is often the difference between mediocre and winning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction",
    "",
    "**Andrew Ng's famous quote:**",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\"",
    "",
    "Good features make simple models work. Bad features make complex models fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents",
    "1. Why Feature Engineering Matters",
    "2. Handling Missing Data",
    "3. Encoding Categorical Variables",
    "4. Numerical Transformations",
    "5. Feature Scaling & Normalization",
    "6. Creating Interaction Features",
    "7. Polynomial Features",
    "8. Time-based Features",
    "9. Text Features",
    "10. Automated Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import TargetEncoder\nimport warnings\n\n# Filter only specific warnings to avoid hiding important issues\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nprint('âœ… Libraries loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling Missing Data",
    "",
    "**Three Strategies:**",
    "",
    "**A. Deletion:**",
    "- Drop rows: When <5% missing",
    "- Drop columns: When >70% missing",
    "",
    "**B. Imputation:**",
    "- Mean/Median: Numerical features",
    "- Mode: Categorical features",
    "- Forward/Backward fill: Time series",
    "- Model-based: Predict missing values",
    "",
    "**C. Flag Creation:**",
    "- Create 'is_missing' binary feature",
    "- Often helps models learn patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values",
    "df = pd.DataFrame({",
    "    'age': [25, np.nan, 35, 45, np.nan],",
    "    'income': [50000, 60000, np.nan, 80000, 75000],",
    "    'city': ['NYC', 'LA', np.nan, 'Chicago', 'NYC']",
    "})",
    "",
    "print('Original data:')",
    "print(df)",
    "print(f'\\nMissing values:\\n{df.isnull().sum()}')",
    "",
    "# Strategy 1: Simple imputation",
    "from sklearn.impute import SimpleImputer",
    "num_imputer = SimpleImputer(strategy='mean')",
    "df['age_imputed'] = num_imputer.fit_transform(df[['age']])",
    "df['income_imputed'] = SimpleImputer(strategy='median').fit_transform(df[['income']])",
    "",
    "# Strategy 2: Create missing flag",
    "df['age_was_missing'] = df['age'].isnull().astype(int)",
    "",
    "print('\\nAfter imputation:')",
    "print(df[['age', 'age_imputed', 'age_was_missing']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding Categorical Variables",
    "",
    "**Methods:**",
    "",
    "**A. Label Encoding:** For ordinal data (low, medium, high)",
    "",
    "**B. One-Hot Encoding:** For nominal data with few categories (<10)",
    "- Creates binary column for each category",
    "- Avoids imposing false order",
    "",
    "**C. Target Encoding:** For high-cardinality features (>10 categories)",
    "- Replace category with mean target value",
    "- Risk of overfitting - use cross-validation",
    "",
    "**D. Frequency Encoding:** Replace with category frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# One-Hot Encoding\ndf_cat = pd.DataFrame({'color': ['red', 'blue', 'green', 'red']})\ndf_onehot = pd.get_dummies(df_cat, columns=['color'], prefix='color')\nprint('One-Hot Encoding:')\nprint(df_onehot)\n\n# Target Encoding - PROPER IMPLEMENTATION (avoiding data leakage)\ndf_target = pd.DataFrame({\n    'city': ['NYC', 'LA', 'NYC', 'Chicago', 'LA', 'NYC', 'Chicago', 'LA'],\n    'price': [500, 300, 520, 400, 310, 515, 390, 305]\n})\n\n# âš ï¸ WARNING: INCORRECT (causes data leakage):\n# target_means = df_target.groupby('city')['price'].mean()\n# df_target['city_encoded'] = df_target['city'].map(target_means)\n# âŒ This uses information from the entire dataset including test set!\n\n# âœ… CORRECT: Use cross-validated target encoding\nfrom sklearn.model_selection import KFold\nfrom typing import Dict\n\ndef cross_validated_target_encode(df: pd.DataFrame, \n                                   cat_col: str, \n                                   target_col: str, \n                                   n_splits: int = 5) -> pd.Series:\n    \"\"\"\n    Perform cross-validated target encoding to prevent data leakage.\n    \n    Args:\n        df: DataFrame containing the data\n        cat_col: Name of categorical column to encode\n        target_col: Name of target column\n        n_splits: Number of CV folds\n    \n    Returns:\n        Series with encoded values\n    \"\"\"\n    encoded = pd.Series(index=df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    # Global mean for smoothing\n    global_mean = df[target_col].mean()\n    \n    for train_idx, val_idx in kf.split(df):\n        # Calculate means only on training fold\n        means = df.iloc[train_idx].groupby(cat_col)[target_col].mean()\n        \n        # Apply smoothing (blend with global mean)\n        # This helps with rare categories\n        alpha = 0.1  # Smoothing factor\n        means = means * (1 - alpha) + global_mean * alpha\n        \n        # Encode validation fold\n        encoded.iloc[val_idx] = df.iloc[val_idx][cat_col].map(means).fillna(global_mean)\n    \n    return encoded\n\n# Apply cross-validated target encoding\ndf_target['city_encoded_cv'] = cross_validated_target_encode(\n    df_target, 'city', 'price', n_splits=3\n)\n\nprint('\\nâœ… Target Encoding (Cross-Validated - NO DATA LEAKAGE):')\nprint(df_target)\nprint('\\nðŸ“Š Mean encoding per city (for reference):')\nprint(df_target.groupby('city')['price'].mean())\nprint('\\nâš ï¸  Note: CV-encoded values differ from simple means to prevent leakage!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical Transformations",
    "",
    "**Common Transformations:**",
    "",
    "**A. Log Transform:** For skewed data",
    "- $x' = \\log(x + 1)$",
    "- Makes distribution more Gaussian",
    "",
    "**B. Square Root:** For count data",
    "- $x' = \\sqrt{x}$",
    "",
    "**C. Box-Cox:** Automatic optimal transformation",
    "",
    "**D. Binning:** Convert continuous to categorical",
    "- Equal-width bins",
    "- Equal-frequency bins (quantiles)",
    "- Custom thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation for skewed data",
    "skewed_data = np.exp(np.random.normal(0, 1, 1000))  # Highly skewed",
    "",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))",
    "ax1.hist(skewed_data, bins=50)",
    "ax1.set_title('Original (Skewed)', fontweight='bold')",
    "ax1.set_xlabel('Value')",
    "",
    "log_transformed = np.log1p(skewed_data)",
    "ax2.hist(log_transformed, bins=50)",
    "ax2.set_title('Log Transformed (More Gaussian)', fontweight='bold')",
    "ax2.set_xlabel('Log(Value + 1)')",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "print(f'Original skewness: {pd.Series(skewed_data).skew():.2f}')",
    "print(f'Log-transformed skewness: {pd.Series(log_transformed).skew():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling",
    "",
    "**Why Scale?** Many algorithms (KNN, SVM, Neural Networks) are sensitive to feature magnitude.",
    "",
    "**Methods:**",
    "",
    "**A. Standardization (Z-score):**",
    "- $x' = \\frac{x - \\mu}{\\sigma}$",
    "- Mean=0, Std=1",
    "- Use when data is Gaussian",
    "",
    "**B. Min-Max Normalization:**",
    "- $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$",
    "- Range: [0, 1]",
    "- Use when need bounded range",
    "",
    "**C. Robust Scaling:**",
    "- Uses median and IQR (robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of scaling methods",
    "data = np.array([[1, 2000], [2, 3000], [3, 5000], [4, 10000], [5, 2500]])",
    "df_scale = pd.DataFrame(data, columns=['feature1', 'feature2'])",
    "",
    "print('Original data:')",
    "print(df_scale)",
    "",
    "# StandardScaler",
    "scaler_std = StandardScaler()",
    "df_std = pd.DataFrame(scaler_std.fit_transform(df_scale), columns=['f1_std', 'f2_std'])",
    "",
    "# MinMaxScaler  ",
    "scaler_mm = MinMaxScaler()",
    "df_mm = pd.DataFrame(scaler_mm.fit_transform(df_scale), columns=['f1_mm', 'f2_mm'])",
    "",
    "print('\\nStandardized:')",
    "print(df_std.head())",
    "print('\\nMin-Max Normalized:')",
    "print(df_mm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Features",
    "",
    "**Idea:** Combine features to capture relationships",
    "",
    "**Examples:**",
    "- Price Ã— Quantity = Total Revenue",
    "- Height Ã— Weight = BMI indicator",
    "- Day Ã— Hour = Time of day patterns",
    "",
    "**Polynomial Features:**",
    "- Degree 2: $[x_1, x_2] â†’ [x_1, x_2, x_1^2, x_1x_2, x_2^2]$",
    "- Captures non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features",
    "X = np.array([[2, 3], [4, 5]])",
    "poly = PolynomialFeatures(degree=2, include_bias=False)",
    "X_poly = poly.fit_transform(X)",
    "",
    "print('Original features: [x1, x2]')",
    "print(X)",
    "print('\\nPolynomial features (degree=2): [x1, x2, x1Â², x1*x2, x2Â²]')",
    "print(X_poly)",
    "print(f'\\nFeature names: {poly.get_feature_names_out()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time-Based Features",
    "",
    "**From timestamps, extract:**",
    "- Hour, day of week, month, quarter, year",
    "- Is_weekend, is_holiday",
    "- Days since epoch",
    "- Cyclical encoding (sin/cos for hour/month)",
    "",
    "**Cyclical Encoding:**",
    "- Hour 23 and Hour 0 are close, but numerically 23 apart",
    "- Solution: $hour\\_sin = \\sin(2\\pi \\times hour / 24)$",
    "- $hour\\_cos = \\cos(2\\pi \\times hour / 24)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based feature extraction",
    "dates = pd.date_range('2024-01-01', periods=5, freq='12H')",
    "df_time = pd.DataFrame({'timestamp': dates})",
    "",
    "# Extract features",
    "df_time['hour'] = df_time['timestamp'].dt.hour",
    "df_time['day_of_week'] = df_time['timestamp'].dt.dayofweek",
    "df_time['is_weekend'] = (df_time['day_of_week'] >= 5).astype(int)",
    "",
    "# Cyclical encoding for hour",
    "df_time['hour_sin'] = np.sin(2 * np.pi * df_time['hour'] / 24)",
    "df_time['hour_cos'] = np.cos(2 * np.pi * df_time['hour'] / 24)",
    "",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Feature Engineering",
    "",
    "**Tools:**",
    "",
    "**Featuretools:** Automates feature creation using Deep Feature Synthesis",
    "- Aggregation features",
    "- Transformation features",
    "- Handles multi-table data",
    "",
    "**Best Practices:**",
    "1. Start with domain knowledge",
    "2. Create features based on business logic",
    "3. Use automated tools for exploration",
    "4. Always validate with cross-validation",
    "5. Remove highly correlated features (>0.95)",
    "6. Use feature importance to prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "**Feature Engineering Checklist:**",
    "",
    "**Data Cleaning:**",
    "- â˜‘ Handle missing values",
    "- â˜‘ Remove duplicates",
    "- â˜‘ Fix data types",
    "",
    "**Encoding:**",
    "- â˜‘ One-hot encode nominal categories (<10 levels)",
    "- â˜‘ Target encode high-cardinality (>10 levels)",
    "- â˜‘ Label encode ordinal categories",
    "",
    "**Transformations:**",
    "- â˜‘ Log transform skewed features",
    "- â˜‘ Scale features for distance-based models",
    "- â˜‘ Create polynomial/interaction features",
    "",
    "**Domain-Specific:**",
    "- â˜‘ Extract time-based features",
    "- â˜‘ Create ratios and aggregations",
    "- â˜‘ Add domain knowledge features",
    "",
    "**Validation:**",
    "- â˜‘ Check for data leakage",
    "- â˜‘ Validate with cross-validation",
    "- â˜‘ Analyze feature importance",
    "",
    "**Remember:** Good features > Complex models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}