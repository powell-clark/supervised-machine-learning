{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFk8wWoi805F"
      },
      "source": [
        "# Lesson 2C: ATLAS - Decision Tree Comparison\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Ok so if you've gotten this far you might be wondering: Whats the best type of decision tree for predicting house prices? How best to configure such a tree?  What features give the best predictions without overfitting?\n",
        "\n",
        "To do this we'll need a pipeline for running and comparing lots of tree-based models at once. So lets build that!\n",
        "\n",
        "So in this lesson we're building ATLAS - an Automated Tree Learning Analysis System\n",
        "\n",
        "Why ATLAS? Well, comparing loads of models is heavy work, and like its mythological namesake, our pipeline carries that load.\n",
        "\n",
        "We're going to explore:\n",
        "\n",
        "1. How to compare lots of models without losing our sanity\n",
        "2. Why feature engineering is still more art than science\n",
        "3. The eternal trade-off between complexity and performance\n",
        "4. What we've learned about model evaluation (mostly through making mistakes)\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "1. [Why build ATLAS?](#why-build-atlas)\n",
        "2. [ATLAS architecture](#atlas-architecture)\n",
        "  - [Understanding k-fold cross-validation in ATLAS](#understanding-k-fold-cross-validation-in-atlas)\n",
        "  -  [Core components](#core-components)\n",
        "  -  [Class design and model persistence](#class-design-and-model-persistence)\n",
        "  -  [System workflow](#system-workflow)\n",
        "  -  [Key challenges solved](#key-challenges-solved)\n",
        "  -  [Next steps](#next-steps)\n",
        "7. [Running ATLAS: The pipeline in action](#running-atlas-the-pipeline-in-action)\n",
        "8. [Unveiling the drivers of London house prices](#unveiling-the-drivers-of-london-house-prices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PhH0C_805I"
      },
      "source": [
        "## Why build ATLAS?\n",
        "Let's be upfront - we're building this pipeline because:\n",
        "\n",
        "1. Running models one at a time is tedious and error-prone\n",
        "2. We keep forgetting which model performed best and with which features\n",
        "3. Copying and pasting code between notebooks is a recipe for disaster\n",
        "4. We want to spend time understanding results, not running experiments\n",
        "\n",
        "Let's also acknowledge something else upfront - we're all using AI tools to help write code these days and this gives us the opportunity to quickly build this comparison engine in a day, a task that would have taken much longer a year ago.\n",
        "\n",
        "We want to encourage the use of these tools as they are brilliant at the boilerplate stuff, but its important we remember we're the editors, proof-readers, and decision makers.\n",
        "\n",
        "Every line gets our scrutiny, and every design choice serves a purpose. The interesting questions - the \"why are we doing this?\" and \"what does this actually mean?\" - that's where we humans get to shine.\n",
        "\n",
        "This is where ATLAS comes in. Think of it as our research assistant that handles the repetitive work while we focus on what matters - understanding and interpreting the results.\n",
        "\n",
        "In building this system, we face three critical challenges:\n",
        "\n",
        "1. We need to compare many different approaches systematically\n",
        "2. We need to ensure our comparisons are fair and reliable\n",
        "3. We need to avoid common pitfalls that can invalidate our results\n",
        "\n",
        "Let's dive in by setting up our tools and loading our data.\n",
        "\n",
        "## ATLAS architecture\n",
        "\n",
        "At its core, ATLAS is a pipeline that automates the process of:\n",
        "```\n",
        "Raw Data â†’ Feature Engineering â†’ Model Training â†’ Evaluation â†’ Results\n",
        "```\n",
        "\n",
        "But it does this across:\n",
        "- Multiple feature combinations\n",
        "- Multiple model types (Decision Trees, Random Forests, XGBoost)\n",
        "- Multiple training/validation splits\n",
        "- All while preventing common mistakes such as target variable leakage\n",
        "\n",
        "### Understanding k-fold cross-validation in ATLAS\n",
        "\n",
        "Cross-validation sits at the heart of ATLAS's validation strategy. It lets us systematically rotate which portion of our training data is used for validation, with 'k' referring to the number of groups we split our data into.\n",
        "\n",
        "#### Why do we need it?\n",
        "When evaluating our house price models, using a single train-validation split is risky - our results might depend heavily on which properties end up in which set. Cross-validation solves this by validating each model multiple times on different splits of the data.\n",
        "\n",
        "#### How it works in practice\n",
        "The general procedure is as follows:\n",
        "\n",
        "1. Mix up the data randomly\n",
        "2. Cut it into `k` equal chunks\n",
        "3. For each chunk:\n",
        "  - Use it to validate\n",
        "  - Use everything else to train\n",
        "  - Build a model and see how it does\n",
        "  - Write down the score\n",
        "4. Average all those scores together\n",
        "\n",
        "Let's see how this works with our house prices.\n",
        "\n",
        "Imagine you have 1000 house prices and want to test your model. You could:\n",
        "\n",
        "1. **Simple split (not great):**\n",
        "   ```\n",
        "   800 houses for training â†’ Train Model â†’ Test on 200 houses\n",
        "   ```\n",
        "   Problem: Your results might depend heavily on which 200 houses you picked\n",
        "\n",
        "2. **Use 5-fold cross-validation (much better):**\n",
        "   ```\n",
        "   Split 800 training houses into 5 folds of 160 each\n",
        "   \n",
        "  Fold 1: [Val][Train][Train][Train][Train]\n",
        "  Fold 2: [Train][Val][Train][Train][Train]\n",
        "  Fold 3: [Train][Train][Val][Train][Train]\n",
        "  Fold 4: [Train][Train][Train][Val][Train]\n",
        "  Fold 5: [Train][Train][Train][Train][Val]\n",
        "   ```\n",
        "   \n",
        "   Now you:\n",
        "   - Train 5 different times\n",
        "   - Each time, use 4 folds (640 houses) for training\n",
        "   - Validate on the remaining fold (160 houses)\n",
        "   - Average the results\n",
        "\n",
        "This gives you much more reliable performance estimates and tells you how much your model's performance varies.\n",
        "\n",
        "For ATLAS, this means we can tell if a model really works across all kinds of London properties and hasn't just gotten lucky with one split.\n",
        "\n",
        "### Core components\n",
        "\n",
        "#### 1. PreProcessor: The data guardian\n",
        "```python\n",
        "preprocessor = PreProcessor()\n",
        "train_data, test_data = preprocessor.create_train_test_split(raw_data)\n",
        "```\n",
        "\n",
        "The PreProcessor's job is simple but crucial:\n",
        "- Split data into training and test sets\n",
        "- Ensure the splits represent all price ranges using stratified sampling\n",
        "- Keep the test data untouched until final evaluation\n",
        "\n",
        "#### 2. FeatureEncoder: The feature factory\n",
        "\n",
        "The FeatureEncoder tackles our biggest challenge: how to use location information without leaking price data. Here's the problem:\n",
        "\n",
        "##### The price leakage problem\n",
        "\n",
        "Imagine you're predicting house prices in London. You know that houses in Chelsea are expensive, so you want to encode this information. A naive approach would be:\n",
        "\n",
        "```python\n",
        "# ðŸš« BAD APPROACH - Price Leakage!\n",
        "chelsea_average_price = all_data[all_data['area'] == 'Chelsea']['price'].mean()\n",
        "data['chelsea_price_level'] = chelsea_average_price\n",
        "```\n",
        "\n",
        "This leaks future price information because you're using the entire dataset's prices to create features. Instead, ATLAS does this:\n",
        "\n",
        "```python\n",
        "# âœ… GOOD APPROACH - No Leakage\n",
        "def encode_location(train_fold, validation_fold):\n",
        "    # Calculate price levels using ONLY training data\n",
        "    area_prices = train_fold.groupby('area')['price'].mean()\n",
        "    \n",
        "    # Apply to validation data without leakage\n",
        "    validation_fold['area_price_level'] = validation_fold['area'].map(area_prices)\n",
        "```\n",
        "\n",
        "ATLAS also encodes area based price information hierarchically:\n",
        "```\n",
        "City Level (e.g., London)\n",
        "   â†“\n",
        "Area Level (e.g., North London)\n",
        "   â†“\n",
        "Neighborhood Level (e.g., Islington)\n",
        "```\n",
        "\n",
        "For areas with limited data, we fall back to broader geographic averages.\n",
        "\n",
        "#### 3. CrossValidator: The experiment runner\n",
        "\n",
        "The CrossValidator is where everything comes together.\n",
        "\n",
        "It manages running training, evaluating each model and feature set combinations, and calls the feature encoders within the training folds to prevent target variable information leakage.\n",
        "\n",
        "```python\n",
        "class CrossValidator:\n",
        "    def evaluate_all_combinations(self, train_data, test_data):\n",
        "        results = []\n",
        "        \n",
        "        # PART 1: K-FOLD CROSS VALIDATION\n",
        "        kf = KFold(n_splits=5, shuffle=True)\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
        "            # Get this fold's data\n",
        "            fold_train = train_data.iloc[train_idx]\n",
        "            fold_val = train_data.iloc[val_idx]\n",
        "            \n",
        "            # Create features fresh for this fold\n",
        "            feature_sets = encoder.create_fold_features(fold_train, fold_val)\n",
        "            \n",
        "            # Try each feature set and model combination\n",
        "            for feature_set in feature_sets:\n",
        "                for model_name, model in self.models.items():\n",
        "                    # Train and evaluate\n",
        "                    model.fit(feature_set.X_train, feature_set.y_train)\n",
        "                    val_pred = model.predict(feature_set.X_val)\n",
        "                    \n",
        "                    # Record results\n",
        "                    results.append({\n",
        "                        'fold': fold_idx,\n",
        "                        'feature_set': feature_set.name,\n",
        "                        'model': model_name,\n",
        "                        'performance': calculate_metrics(val_pred)\n",
        "                    })\n",
        "        \n",
        "        # PART 2: FINAL EVALUATION\n",
        "        # Only best models get evaluated on test data\n",
        "        best_models = select_best_models(results)\n",
        "        final_results = evaluate_on_test_data(best_models, test_data)\n",
        "```\n",
        "\n",
        "This ensures each fold is truly independent, with its own feature encoding.\n",
        "\n",
        "### Class design and model persistence\n",
        "\n",
        "ATLAS is built for deployment, not just experimentation. Each class can save its state:\n",
        "\n",
        "```python\n",
        "class FeatureEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoding_maps = {}  # Stores encoding information\n",
        "        \n",
        "    def save(self, path):\n",
        "        # Save encoding_maps for future use\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self.encoding_maps, f)\n",
        "            \n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        # Load saved encoder\n",
        "        with open(path, 'rb') as f:\n",
        "            encoder = cls()\n",
        "            encoder.encoding_maps = pickle.load(f)\n",
        "            return encoder\n",
        "```\n",
        "\n",
        "This means when you deploy your model, you can:\n",
        "1. Save all the preprocessing steps\n",
        "2. Load them in production\n",
        "3. Apply the exact same transformations to new data\n",
        "\n",
        "### System workflow\n",
        "\n",
        "Let's follow how ATLAS processes our housing data:\n",
        "\n",
        "1. **Initial split** (PreProcessor)\n",
        "   ```\n",
        "   Raw Data â†’ Training Data (80%) + Test Data (20%)\n",
        "   ```\n",
        "   Test data remains locked away until final evaluation.\n",
        "\n",
        "2. **Cross-validation split** (CrossValidator)\n",
        "   ```\n",
        "   Training Data â†’ 5 Folds\n",
        "   Each fold gets a turn as validation data\n",
        "   ```\n",
        "   This gives us reliable performance estimates.\n",
        "\n",
        "3. **Feature engineering** (FeatureEncoder)\n",
        "   ```\n",
        "   For each fold:\n",
        "      Create features using only training portion\n",
        "      Apply those features to validation portion\n",
        "   ```\n",
        "   This prevents data leakage while giving us multiple feature combinations to try.\n",
        "\n",
        "4. **Model training and evaluation**\n",
        "   ```\n",
        "   Cross-validation phase:\n",
        "     For each fold:\n",
        "         For each feature set:\n",
        "             For each model type:\n",
        "                 Train on fold training data\n",
        "                 Evaluate on fold validation data\n",
        "                 Record performance metrics\n",
        "\n",
        "   Final evaluation phase:\n",
        "     For best performing models:\n",
        "         Train on full training data\n",
        "         Evaluate on held-out test data\n",
        "\n",
        "   ```\n",
        "   This gives us comprehensive comparison results.\n",
        "\n",
        "### Key challenges solved\n",
        "\n",
        "The architecture of ATLAS directly addresses common machine learning challenges:\n",
        "\n",
        "1. **The feature selection problem**\n",
        "   \n",
        "   Instead of guessing what drives house prices, ATLAS methodically evaluates all combinations of features across different models.\n",
        "\n",
        "2. **The reliability problem**\n",
        "   \n",
        "   Rather than trusting a single train/test split ATLAS uses cross-validation for robust estimates, maintaining a separate test set for final validation.\n",
        "\n",
        "3. **The leakage problem**\n",
        "\n",
        "   By encoding features separately for each fold, ATLAS ensures our models never see future price information during training.\n",
        "\n",
        "4. **The deployment problem**\n",
        "\n",
        "   All components save their state, ensuring our deployed models use exactly the same transformations we tested.\n",
        "\n",
        "This systematic approach means our predictions are based on evidence, not intuition.\n",
        "\n",
        "\n",
        "### Next steps\n",
        "\n",
        "Now that we understand how ATLAS works, let's see what it reveals about London house prices. We'll:\n",
        "1. See ATLAS in action on real data\n",
        "2. Learn how to interpret its results\n",
        "3. Use those insights to choose the best model for our needs\n",
        "4. Discuss the real world implications of our models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_BHS1Hk805K"
      },
      "source": [
        "## ATLAS implementation\n",
        "\n",
        "### Required Libraries\n",
        "\n",
        "Our pipeline requires three core capabilities: processing large housing datasets, training multiple models in parallel, and maintaining production-ready code.\n",
        "\n",
        "| Library | Purpose |\n",
        "|---------|----------|\n",
        "| Pandas | Data manipulation and feature engineering |\n",
        "| NumPy | Numerical computations |\n",
        "| scikit-learn DecisionTreeRegressor, RandomForestRegressor | Tree-based models |\n",
        "| XGBoost | Gradient boosting |\n",
        "| scikit-learn metrics, preprocessing, model_selection | Model evaluation and data preparation |\n",
        "| typing, dataclasses | Code organisation |\n",
        "| pickle | Model persistence |\n",
        "| tqdm, IPython.display | Progress tracking |\n",
        "\n",
        "### Configuration\n",
        "- Fixed random seeds for reproducibility\n",
        "- Formatted DataFrame output\n",
        "- Full column visibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCMa72E8805K"
      },
      "outputs": [],
      "source": [
        "# Core data and analysis libraries\n",
        "import numpy as np                                     # For numerical computations and array operations\n",
        "import pandas as pd                                    # For data manipulation and analysis using DataFrames\n",
        "from typing import TypedDict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Machine Learning Framework\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,                                 # Splits data into training and test sets\n",
        "    KFold,                                            # Performs k-fold cross-validation\n",
        ")\n",
        "\n",
        "from sklearn.preprocessing import (\n",
        "    OneHotEncoder                                     # Converts categorical variables into binary features\n",
        ")\n",
        "\n",
        "# Tree-based Models\n",
        "from sklearn.tree import DecisionTreeRegressor        # Basic decision tree implementation\n",
        "from sklearn.ensemble import RandomForestRegressor    # Ensemble of decision trees\n",
        "from xgboost import XGBRegressor                      # Gradient boosting implementation\n",
        "\n",
        "# Model Evaluation Metrics\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,                             # Measures average magnitude of prediction errors\n",
        "    r2_score,                                        # Measures proportion of variance explained by model\n",
        ")\n",
        "\n",
        "# Development Infrastructure\n",
        "from typing import Dict, List, Tuple, Optional        # For type annotations\n",
        "from dataclasses import dataclass                     # For creating data classes\n",
        "# import pickle                                         # For saving/loading objects\n",
        "\n",
        "# Progress Tracking\n",
        "from tqdm.notebook import tqdm                        # For displaying progress bars\n",
        "from IPython.display import display                   # For rich output in notebooks\n",
        "\n",
        "# Display Configuration\n",
        "pd.set_option('display.max_columns', None)            # Show all columns in DataFrames\n",
        "pd.set_option('display.float_format',                 # Format floating point numbers to 2 decimal places\n",
        "              lambda x: '{:,.2f}'.format(x))\n",
        "\n",
        "# Reproducibility Settings\n",
        "RANDOM_STATE = 42                                     # Fixed seed for reproducible results\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAFPBAZT805L"
      },
      "source": [
        "### Data Loading and Feature Set Implementation\n",
        "\n",
        "- Data loading\n",
        "- FeatureSet implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O1SwLMH805L"
      },
      "outputs": [],
      "source": [
        "def validate_housing_data(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Validate housing data has correct columns and content\"\"\"\n",
        "    # Check required columns exist\n",
        "    required_columns = [\n",
        "        'Price', 'log_price', 'Area in sq ft', 'No. of Bedrooms',\n",
        "        'House Type', 'Outcode', 'Postal Code', 'Location', 'City/County'\n",
        "    ]\n",
        "\n",
        "    missing = set(required_columns) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "    # Basic data validation\n",
        "    if (df['Price'] <= 0).any():\n",
        "        raise ValueError(\"Found non-positive prices\")\n",
        "\n",
        "    if (df['Area in sq ft'] <= 0).any():\n",
        "        raise ValueError(\"Found non-positive areas\")\n",
        "\n",
        "    if ((df['No. of Bedrooms'] <= 0) | (df['No. of Bedrooms'] > 20)).any():\n",
        "        raise ValueError(\"Invalid number of bedrooms\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"Data validation complete!\")\n",
        "    print(f\"Rows: {len(df)}\")\n",
        "    print(f\"Price range: Â£{df['Price'].min():,.0f} - Â£{df['Price'].max():,.0f}\")\n",
        "    print(f\"Area range: {df['Area in sq ft'].min():,.0f} - {df['Area in sq ft'].max():,.0f} sq ft\")\n",
        "    print(f\"Bedrooms range: {df['No. of Bedrooms'].min()} - {df['No. of Bedrooms'].max()}\")\n",
        "    print(f\"Missing locations: {df['Location'].isnull().sum()}\")\n",
        "\n",
        "# Load and validate data\n",
        "df_with_outcode = pd.read_csv('../data/df_with_outcode.csv')\n",
        "\n",
        "validate_housing_data(df_with_outcode)\n",
        "display(df_with_outcode.head())\n",
        "\n",
        "@dataclass\n",
        "class FeatureSet:\n",
        "    X_train: pd.DataFrame\n",
        "    X_val: pd.DataFrame\n",
        "    y_train: pd.Series\n",
        "    y_val: pd.Series\n",
        "    name: str\n",
        "    description: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J05xrGxJ805M"
      },
      "source": [
        "### PreProcessor: Standardising Model Training\n",
        "\n",
        "Building on Lesson 2b's exploration of model evaluation (see \"Stratified Sampling for Price Data\"), we learned that proper model evaluation requires careful data splitting. Specifically, we need to ensure our train and test sets have similar price distributions.\n",
        "\n",
        "The PreProcessor class exists to:\n",
        "1. Provide a dedicated place for any data preparation needed before model training\n",
        "2. Ensure consistent data splitting across all our experiments\n",
        "3. Store any preprocessing parameters (like means, standard deviations, or encoding mappings) that need to be reused during inference\n",
        "\n",
        "Currently, we only need it for:\n",
        "1. Creating price bands for stratification\n",
        "2. Performing the stratified train/test split\n",
        "\n",
        "#### Input Requirements\n",
        "The DataFrame must include:\n",
        "- `log_price` (transformed in previous lesson)\n",
        "- Clean features ready for modeling\n",
        "\n",
        "While our implementation is simple, preprocessor classes are a crucial part of ML pipelines. They ensure that any transformations learned from training data (like scaling parameters or encoding dictionaries) are properly saved and can be applied to new data during prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YI_zuIC805M"
      },
      "outputs": [],
      "source": [
        "class PreProcessor:\n",
        "    \"\"\"Handles initial data transformations and train/test splitting\"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = RANDOM_STATE):\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def prepare_pre_split_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Creates features that must be calculated before train/test split\"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Create price bands for stratification\n",
        "        df_processed['price_band'] = pd.qcut(df_processed['log_price'], q=10, labels=False)\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_train_test_split(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Performs stratified train/test split using price bands\"\"\"\n",
        "        train_data, test_data = train_test_split(\n",
        "            df,\n",
        "            test_size=0.2,\n",
        "            stratify=df['price_band'],\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        return train_data, test_data\n",
        "\n",
        "print(\"PreProcessor class loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx7oz61e805N"
      },
      "source": [
        "### The FeatureEncoder: Production-Ready Feature Engineering\n",
        "\n",
        "The FeatureEncoder solves the core challenge in house price prediction: transforming categorical location data into numerical features without compromising model validation integrity. Let's dive deep into how it works.\n",
        "\n",
        "#### Core Responsibilities\n",
        "\n",
        "The FeatureEncoder handles three critical tasks:\n",
        "1. Safe target encoding of geographic features\n",
        "2. Generation of multiple feature combinations\n",
        "3. Consistent handling of training, validation, and test data\n",
        "\n",
        "#### Geographic Encoding: A Three-Level Challenge\n",
        "\n",
        "Our housing data presents a hierarchical location structure:\n",
        "```\n",
        "Outcode (e.g., \"SW1\")\n",
        "   â†“\n",
        "Postal Code (e.g., \"SW1A 1AA\")\n",
        "   â†“\n",
        "Location (e.g., \"Westminster\")\n",
        "```\n",
        "\n",
        "Let's examine how each level is handled:\n",
        "\n",
        "#### 1. Outcode Level (Primary Signal)\n",
        "```python\n",
        "def _encode_outcode_target(self, train_data, eval_data):\n",
        "    if 'cv_fold' in train_data.columns:  # Cross-validation mode\n",
        "        oof_predictions = pd.Series(index=train_data.index)\n",
        "        for train_idx, val_idx in kf.split(train_data):\n",
        "            inner_train = train_data.iloc[train_idx]\n",
        "            outcode_means = inner_train.groupby('Outcode')['log_price'].mean()\n",
        "            oof_predictions.iloc[val_idx] = val_data['Outcode'].map(outcode_means)\n",
        "    else:  # Test/Production mode\n",
        "        outcode_means = train_data.groupby('Outcode')['log_price'].mean()\n",
        "        encoded = eval_data['Outcode'].map(outcode_means)\n",
        "```\n",
        "- Most robust due to larger sample sizes\n",
        "- Primary signal for price levels\n",
        "- Handles unseen outcodes via global mean\n",
        "\n",
        "#### 2. Postal Code Level (Enhanced Precision)\n",
        "```python\n",
        "def _encode_postcode_target(self, fold_train, fold_val, outcode_encoding):\n",
        "    counts = fold_train['Postal Code'].value_counts()\n",
        "    means = fold_train.groupby('Postal Code')['log_price'].mean()\n",
        "    \n",
        "    # Bayesian-style smoothing\n",
        "    weight = counts / (counts + self.smoothing_factor)\n",
        "    encoded = weight * means + (1 - weight) * outcode_encoding\n",
        "```\n",
        "- Uses outcode encoding as an informed prior\n",
        "- Smoothing factor controls trust in local data\n",
        "- Gracefully handles sparse postal codes\n",
        "\n",
        "#### 3. Location Level (Maximum Detail)\n",
        "```python\n",
        "def _encode_location_target(self, fold_train, fold_val, postcode_encoding):\n",
        "    counts = fold_train['Location'].value_counts()\n",
        "    means = fold_train.groupby('Location')['log_price'].mean()\n",
        "    \n",
        "    # Handle rare locations\n",
        "    low_freq_mask = (counts < self.min_location_freq)\n",
        "    encoded[low_freq_mask] = postcode_encoding[low_freq_mask]\n",
        "```\n",
        "- Falls back to postal code for rare locations\n",
        "- Minimum frequency threshold prevents unstable estimates\n",
        "- Preserves granular information where reliable\n",
        "\n",
        "#### Cross-Validation Safety Mechanisms\n",
        "\n",
        "The encoder implements three critical safeguards:\n",
        "\n",
        "1. **Out-of-Fold Encoding**\n",
        "```python\n",
        "for train_idx, val_idx in kf.split(train_data):\n",
        "    # Encode validation using only training data\n",
        "    inner_train = train_data.iloc[train_idx]\n",
        "    inner_val = train_data.iloc[val_idx]\n",
        "    encoded = encode_features(inner_train, inner_val)\n",
        "```\n",
        "- Prevents target leakage during model selection\n",
        "- Maintains fold independence\n",
        "- Mimics real-world information availability\n",
        "\n",
        "2. **Test Set Handling**\n",
        "```python\n",
        "if is_test_set:\n",
        "    # Use all training data for stable estimates\n",
        "    means = full_training_data.groupby('Location')['log_price'].mean()\n",
        "    encoded = test_data['Location'].map(means).fillna(global_mean)\n",
        "```\n",
        "- Maximises encoding stability for final evaluation\n",
        "- Uses full training data appropriately\n",
        "- Ready for production use\n",
        "\n",
        "3. **Hierarchical Fallbacks**\n",
        "```python\n",
        "def encode_location(self, data, means, fallback):\n",
        "    encoded = data.map(means)\n",
        "    return encoded.fillna(fallback)  # Use broader geography when needed\n",
        "```\n",
        "- Systematic fallback chain\n",
        "- No missing values possible\n",
        "- Maintains encoding stability\n",
        "\n",
        "#### Feature Set Generation\n",
        "\n",
        "The encoder creates multiple feature combinations:\n",
        "\n",
        "1. **Base Features**\n",
        "```python\n",
        "numeric_features = ['Area in sq ft', 'No. of Bedrooms']\n",
        "```\n",
        "\n",
        "2. **Property Features**\n",
        "```python\n",
        "house_features = one_hot_encode('House Type')\n",
        "city_features = one_hot_encode('City/County')\n",
        "```\n",
        "\n",
        "3. **Geographic Variants**\n",
        "```python\n",
        "geo_features = {\n",
        "    'target': location_encoded,\n",
        "    'onehot': outcode_onehot,\n",
        "    'price_sqft': price_per_sqft\n",
        "}\n",
        "```\n",
        "\n",
        "4. **Progressive Combinations**\n",
        "```python\n",
        "feature_sets = [\n",
        "    base_only,\n",
        "    base_plus_house,\n",
        "    base_plus_geo,\n",
        "    all_features\n",
        "]\n",
        "```\n",
        "\n",
        "#### Production Readiness\n",
        "\n",
        "The current implementation calculates encodings on-the-fly:\n",
        "```python\n",
        "class FeatureEncoder:\n",
        "    def _encode_outcode_target(self, train_data, eval_data):\n",
        "        means = train_data.groupby('Outcode')['log_price'].mean()\n",
        "        return eval_data['Outcode'].map(means)\n",
        "```\n",
        "\n",
        "Could be extended for persistence:\n",
        "```python\n",
        "class FeatureEncoder:\n",
        "    def save_encodings(self, path):\n",
        "        encodings = {\n",
        "            'outcode_means': self.outcode_means,\n",
        "            'global_mean': self.global_mean,\n",
        "            'smoothing_factor': self.smoothing_factor\n",
        "        }\n",
        "        pickle.dump(encodings, open(path, 'wb'))\n",
        "```\n",
        "\n",
        "Current design is appropriate because:\n",
        "1. During experimentation, we need fresh calculations for valid cross-validation\n",
        "2. Final model selection determines which encodings to persist\n",
        "3. Encoding logic remains correct for both scenarios\n",
        "\n",
        "#### Why This Architecture Succeeds\n",
        "\n",
        "1. **Statistical Validity**\n",
        "   - Proper handling of training/validation/test boundaries\n",
        "   - Appropriate use of hierarchical information\n",
        "   - Robust handling of sparse data\n",
        "\n",
        "2. **Production Viability**\n",
        "   - Clean separation of cross-validation and test logic\n",
        "   - Ready for persistence extension\n",
        "   - Systematic feature generation\n",
        "\n",
        "3. **Engineering Quality**\n",
        "   - Clear single responsibility for each method\n",
        "   - Explicit handling of edge cases\n",
        "   - Well-documented assumptions\n",
        "\n",
        "The FeatureEncoder isn't just converting data - it's ensuring our entire modeling pipeline maintains statistical validity while remaining practical for production deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyJABc9K805N"
      },
      "outputs": [],
      "source": [
        "class FeatureEncoder:\n",
        "    \"\"\"Handles all feature engineering and encoding with fold awareness\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing_factor: int = 10, min_location_freq: int = 5, random_state: int = RANDOM_STATE):\n",
        "        self.smoothing_factor = smoothing_factor\n",
        "        self.min_location_freq = min_location_freq\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _calculate_outcode_price_per_sqft(self,\n",
        "                                        fold_train: pd.DataFrame,\n",
        "                                        fold_val: pd.DataFrame) -> Dict[str, pd.Series]:\n",
        "        \"\"\"\n",
        "        Calculate mean price per square foot using out-of-fold means for outcodes\n",
        "\n",
        "        Args:\n",
        "            fold_train: Training data for current fold\n",
        "            fold_val: Validation data for current fold\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing train and validation series of outcode mean price per sqft\n",
        "        \"\"\"\n",
        "        # Initialise empty series for OOF predictions\n",
        "        oof_price_per_sqft = pd.Series(index=fold_train.index, dtype='float64')\n",
        "\n",
        "        # Calculate OOF means for training data\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
        "        for train_idx, val_idx in kf.split(fold_train):\n",
        "            inner_train = fold_train.iloc[train_idx]\n",
        "            inner_val = fold_train.iloc[val_idx]\n",
        "\n",
        "            # Calculate price per sqft for inner training set\n",
        "            inner_price_per_sqft = inner_train['Price'] / inner_train['Area in sq ft']\n",
        "            outcode_means = inner_price_per_sqft.groupby(inner_train['Outcode']).mean()\n",
        "            global_mean = inner_price_per_sqft.mean()\n",
        "\n",
        "            # Apply to inner validation set\n",
        "            oof_price_per_sqft.iloc[val_idx] = (\n",
        "                inner_val['Outcode']\n",
        "                .map(outcode_means)\n",
        "                .fillna(global_mean)\n",
        "            )\n",
        "\n",
        "        # Calculate means for validation data using full training set\n",
        "        train_price_per_sqft = fold_train['Price'] / fold_train['Area in sq ft']\n",
        "        outcode_means = train_price_per_sqft.groupby(fold_train['Outcode']).mean()\n",
        "        global_mean = train_price_per_sqft.mean()\n",
        "\n",
        "        val_price_per_sqft = (\n",
        "            fold_val['Outcode']\n",
        "            .map(outcode_means)\n",
        "            .fillna(global_mean)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'train': oof_price_per_sqft,\n",
        "            'val': val_price_per_sqft\n",
        "        }\n",
        "\n",
        "    def _encode_house_type(self,\n",
        "                          fold_train: pd.DataFrame,\n",
        "                          fold_val: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Create one-hot encoding for house type\"\"\"\n",
        "        # Initialise encoder for this fold\n",
        "        house_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "        # Fit on fold's training data\n",
        "        train_encoded = pd.DataFrame(\n",
        "            house_encoder.fit_transform(fold_train[['House Type']]),\n",
        "            columns=house_encoder.get_feature_names_out(['House Type']),\n",
        "            index=fold_train.index\n",
        "        )\n",
        "\n",
        "        # Transform validation data\n",
        "        val_encoded = pd.DataFrame(\n",
        "            house_encoder.transform(fold_val[['House Type']]),\n",
        "            columns=house_encoder.get_feature_names_out(['House Type']),\n",
        "            index=fold_val.index\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'train': train_encoded,\n",
        "            'val': val_encoded\n",
        "        }\n",
        "\n",
        "    def _encode_city_country(self,\n",
        "                           fold_train: pd.DataFrame,\n",
        "                           fold_val: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Create one-hot encoding for city/county\"\"\"\n",
        "        # Initialise encoder for this fold\n",
        "        city_country_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "        # Fit on fold's training data\n",
        "        train_encoded = pd.DataFrame(\n",
        "            city_country_encoder.fit_transform(fold_train[['City/County']]),\n",
        "            columns=city_country_encoder.get_feature_names_out(['City/County']),\n",
        "            index=fold_train.index\n",
        "        )\n",
        "\n",
        "        # Transform validation data\n",
        "        val_encoded = pd.DataFrame(\n",
        "            city_country_encoder.transform(fold_val[['City/County']]),\n",
        "            columns=city_country_encoder.get_feature_names_out(['City/County']),\n",
        "            index=fold_val.index\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'train': train_encoded,\n",
        "            'val': val_encoded\n",
        "        }\n",
        "\n",
        "    def _encode_outcode_onehot(self,\n",
        "                              fold_train: pd.DataFrame,\n",
        "                              fold_val: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Create one-hot encoding for outcodes\"\"\"\n",
        "        outcode_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "        train_encoded = pd.DataFrame(\n",
        "            outcode_encoder.fit_transform(fold_train[['Outcode']]),\n",
        "            columns=outcode_encoder.get_feature_names_out(['Outcode']),\n",
        "            index=fold_train.index\n",
        "        )\n",
        "\n",
        "        val_encoded = pd.DataFrame(\n",
        "            outcode_encoder.transform(fold_val[['Outcode']]),\n",
        "            columns=outcode_encoder.get_feature_names_out(['Outcode']),\n",
        "            index=fold_val.index\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'train': train_encoded,\n",
        "            'val': val_encoded\n",
        "        }\n",
        "\n",
        "    def _encode_outcode_postcode_location_target_hierarchical(self,\n",
        "                                                            fold_train: pd.DataFrame,\n",
        "                                                            fold_val: pd.DataFrame\n",
        "                                                            ) -> Tuple[Dict[str, pd.Series],\n",
        "                                                                     Dict[str, pd.Series],\n",
        "                                                                     Dict[str, pd.Series]]:\n",
        "        \"\"\"\n",
        "        Create hierarchical target encoding for geographic features:\n",
        "        - Outcode encoding\n",
        "        - Postcode encoding using outcode as prior\n",
        "        - Location encoding using postcode as prior\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (outcode_encoding, postcode_encoding, location_encoding)\n",
        "        \"\"\"\n",
        "        # 1. Outcode encoding\n",
        "        outcode_encoding = self._encode_outcode_target(fold_train, fold_val)\n",
        "\n",
        "        # 2. Postcode encoding using outcode as prior\n",
        "        postcode_encoding = self._encode_postcode_target(\n",
        "            fold_train,\n",
        "            fold_val,\n",
        "            outcode_encoding\n",
        "        )\n",
        "\n",
        "        # 3. Location encoding using postcode as prior\n",
        "        location_encoding = self._encode_location_target(\n",
        "            fold_train,\n",
        "            fold_val,\n",
        "            postcode_encoding\n",
        "        )\n",
        "\n",
        "        return outcode_encoding, postcode_encoding, location_encoding\n",
        "\n",
        "    def _encode_outcode_target(self,\n",
        "                             train_data: pd.DataFrame,\n",
        "                             eval_data: pd.DataFrame) -> Dict[str, pd.Series]:\n",
        "        \"\"\"Create target encoding for outcodes\"\"\"\n",
        "        if 'cv_fold' in train_data.columns:  # We're in cross-validation\n",
        "            # Use out-of-fold encoding for training data\n",
        "            oof_predictions = pd.Series(index=train_data.index, dtype='float64')\n",
        "\n",
        "            kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
        "            for inner_train_idx, inner_val_idx in kf.split(train_data):\n",
        "                inner_train = train_data.iloc[inner_train_idx]\n",
        "                inner_val = train_data.iloc[inner_val_idx]\n",
        "\n",
        "                outcode_means = inner_train.groupby('Outcode')['log_price'].mean()\n",
        "                global_mean = inner_train['log_price'].mean()\n",
        "\n",
        "                oof_predictions.iloc[inner_val_idx] = (\n",
        "                    inner_val['Outcode']\n",
        "                    .map(outcode_means)\n",
        "                    .fillna(global_mean)\n",
        "                )\n",
        "\n",
        "            # For validation data, use means from all training data\n",
        "            outcode_means = train_data.groupby('Outcode')['log_price'].mean()\n",
        "            global_mean = train_data['log_price'].mean()\n",
        "\n",
        "            val_encoded = (\n",
        "                eval_data['Outcode']\n",
        "                .map(outcode_means)\n",
        "                .fillna(global_mean)\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'train': oof_predictions,\n",
        "                'val': val_encoded\n",
        "            }\n",
        "\n",
        "        else:  # We're encoding for the test set\n",
        "            # Use all training data to encode test set\n",
        "            outcode_means = train_data.groupby('Outcode')['log_price'].mean()\n",
        "            global_mean = train_data['log_price'].mean()\n",
        "\n",
        "            test_encoded = (\n",
        "                eval_data['Outcode']\n",
        "                .map(outcode_means)\n",
        "                .fillna(global_mean)\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'train': train_data['Outcode'].map(outcode_means).fillna(global_mean),\n",
        "                'val': test_encoded\n",
        "            }\n",
        "\n",
        "    def _encode_postcode_target(self,\n",
        "                              fold_train: pd.DataFrame,\n",
        "                              fold_val: pd.DataFrame,\n",
        "                              outcode_encoding: Dict[str, pd.Series]) -> Dict[str, pd.Series]:\n",
        "        \"\"\"Create hierarchical encoding for postcodes using outcode prior\"\"\"\n",
        "        postcode_means = fold_train.groupby('Postal Code')['log_price'].mean()\n",
        "        postcode_counts = fold_train['Postal Code'].value_counts()\n",
        "\n",
        "        def encode_postcodes(df: pd.DataFrame, outcode_encoded: pd.Series) -> pd.Series:\n",
        "            counts = df['Postal Code'].map(postcode_counts)\n",
        "            means = df['Postal Code'].map(postcode_means)\n",
        "\n",
        "            # Handle unseen categories using outcode encoding\n",
        "            means = means.fillna(outcode_encoded)\n",
        "            counts = counts.fillna(0)\n",
        "\n",
        "            # Calculate smoothed values\n",
        "            weight = counts / (counts + self.smoothing_factor)\n",
        "            return weight * means + (1 - weight) * outcode_encoded\n",
        "\n",
        "        return {\n",
        "            'train': encode_postcodes(fold_train, outcode_encoding['train']),\n",
        "            'val': encode_postcodes(fold_val, outcode_encoding['val'])\n",
        "        }\n",
        "\n",
        "    def _encode_location_target(self,\n",
        "                              fold_train: pd.DataFrame,\n",
        "                              fold_val: pd.DataFrame,\n",
        "                              postcode_encoding: Dict[str, pd.Series]) -> Dict[str, pd.Series]:\n",
        "        \"\"\"Create hierarchical encoding for locations using postcode prior\"\"\"\n",
        "        location_means = fold_train.groupby('Location')['log_price'].mean()\n",
        "        location_counts = fold_train['Location'].value_counts()\n",
        "\n",
        "        def encode_locations(df: pd.DataFrame, postcode_encoded: pd.Series) -> pd.Series:\n",
        "            counts = df['Location'].map(location_counts)\n",
        "            means = df['Location'].map(location_means)\n",
        "\n",
        "            # Handle missing and unseen locations using postcode encoding\n",
        "            means = means.fillna(postcode_encoded)\n",
        "            counts = counts.fillna(0)\n",
        "\n",
        "            # Use postcode encoding for low-frequency locations\n",
        "            low_freq_mask = (counts < self.min_location_freq) | counts.isna()\n",
        "\n",
        "            # Calculate smoothed values\n",
        "            weight = counts / (counts + self.smoothing_factor)\n",
        "            encoded = weight * means + (1 - weight) * postcode_encoded\n",
        "\n",
        "            # Replace low frequency locations with postcode encoding\n",
        "            encoded[low_freq_mask] = postcode_encoded[low_freq_mask]\n",
        "\n",
        "            return encoded\n",
        "\n",
        "        return {\n",
        "            'train': encode_locations(fold_train, postcode_encoding['train']),\n",
        "            'val': encode_locations(fold_val, postcode_encoding['val'])\n",
        "        }\n",
        "\n",
        "    def create_fold_features(self, fold_train: pd.DataFrame, fold_val: pd.DataFrame) -> List[FeatureSet]:\n",
        "        \"\"\"Create all feature set variations for a fold\"\"\"\n",
        "\n",
        "        house_features = self._encode_house_type(fold_train, fold_val)\n",
        "        city_country_features = self._encode_city_country(fold_train, fold_val)\n",
        "\n",
        "        # Exploded geographic features with hierarchical encoding\n",
        "        outcode_target_hierarchical, postcode_target_hierarchical, location_target_hierarchical = (\n",
        "            self._encode_outcode_postcode_location_target_hierarchical(fold_train, fold_val)\n",
        "        )\n",
        "\n",
        "        outcode_onehot = self._encode_outcode_onehot(fold_train, fold_val)\n",
        "        outcode_price_per_sqft = self._calculate_outcode_price_per_sqft(fold_train, fold_val)\n",
        "\n",
        "        feature_combinations = [\n",
        "            # Base features\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': None,\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms'\n",
        "            },\n",
        "            # Single feature additions\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': house_features,\n",
        "                'city': None,\n",
        "                'geo_target': None,\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_house',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, House Type'\n",
        "            },\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': city_country_features,\n",
        "                'geo_target': None,\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_city',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, City/County'\n",
        "            },\n",
        "            # Individual geographic features - Target encoded\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': {'outcode': outcode_target_hierarchical},\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_outcode_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Outcode (Target)'\n",
        "            },\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': {'postcode': postcode_target_hierarchical},\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_postcode_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Postcode (Target)'\n",
        "            },\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': {'location': location_target_hierarchical},\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_location_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Location (Target)'\n",
        "            },\n",
        "            # One-hot encoded outcode\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': None,\n",
        "                'geo_onehot': {'outcode': outcode_onehot},\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_outcode_onehot',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Outcode (One-hot)'\n",
        "            },\n",
        "            # Price per square foot\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': None,\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': outcode_price_per_sqft,\n",
        "                'name': 'area_bedrooms_pricesqft',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Price/sqft'\n",
        "            },\n",
        "            # Two feature combinations\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': house_features,\n",
        "                'city': city_country_features,\n",
        "                'geo_target': None,\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_house_city',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, House Type, City/County'\n",
        "            },\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': {\n",
        "                    'outcode': outcode_target_hierarchical,\n",
        "                    'postcode': postcode_target_hierarchical\n",
        "                },\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_outcode_postcode_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Outcode & Postcode (Target)'\n",
        "            },\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': {\n",
        "                    'postcode': postcode_target_hierarchical,\n",
        "                    'location': location_target_hierarchical\n",
        "                },\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_postcode_location_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, Postcode & Location (Target)'\n",
        "            },\n",
        "            # Three feature combinations\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': house_features,\n",
        "                'city': city_country_features,\n",
        "                'geo_target': {'outcode': outcode_target_hierarchical},\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_house_city_outcode_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, House Type, City/County, Outcode (Target)'\n",
        "            },\n",
        "            # All geographic features\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': None,\n",
        "                'city': None,\n",
        "                'geo_target': {\n",
        "                    'outcode': outcode_target_hierarchical,\n",
        "                    'postcode': postcode_target_hierarchical,\n",
        "                    'location': location_target_hierarchical\n",
        "                },\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': None,\n",
        "                'name': 'area_bedrooms_all_geo_target',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, All Geographic Features (Target)'\n",
        "            },\n",
        "            # Complex combinations\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': house_features,\n",
        "                'city': None,\n",
        "                'geo_target': {'outcode': outcode_target_hierarchical},\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': outcode_price_per_sqft,\n",
        "                'name': 'area_bedrooms_house_outcode_target_pricesqft',\n",
        "                'desc': 'Area in sq ft, No. of Bedrooms, House Type, Outcode (Target), Price/sqft'\n",
        "            },\n",
        "            # All features\n",
        "            {\n",
        "                'numeric': ['Area in sq ft', 'No. of Bedrooms'],\n",
        "                'house': house_features,\n",
        "                'city': city_country_features,\n",
        "                'geo_target': {\n",
        "                    'outcode': outcode_target_hierarchical,\n",
        "                    'postcode': postcode_target_hierarchical,\n",
        "                    'location': location_target_hierarchical\n",
        "                },\n",
        "                'geo_onehot': None,\n",
        "                'price_sqft': outcode_price_per_sqft,\n",
        "                'name': 'all_features',\n",
        "                'desc': 'All Features Combined'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        return [self._combine_features(\n",
        "            fold_train,\n",
        "            fold_val,\n",
        "            combo['numeric'],\n",
        "            combo['house'],\n",
        "            combo['city'],\n",
        "            combo['geo_target'],\n",
        "            combo['geo_onehot'],\n",
        "            combo['price_sqft'],\n",
        "            combo['name'],\n",
        "            combo['desc']\n",
        "        ) for combo in feature_combinations]\n",
        "\n",
        "    def _combine_features(self,\n",
        "                         fold_train: pd.DataFrame,\n",
        "                         fold_val: pd.DataFrame,\n",
        "                         base_numeric: List[str],\n",
        "                         house_features: Optional[Dict[str, pd.DataFrame]],\n",
        "                         city_country_features: Optional[Dict[str, pd.DataFrame]],\n",
        "                         geo_target_features: Optional[Dict[str, Dict[str, pd.Series]]],\n",
        "                         geo_onehot_features: Optional[Dict[str, Dict[str, pd.DataFrame]]],\n",
        "                         price_sqft_features: Optional[Dict[str, pd.Series]],\n",
        "                         name: str,\n",
        "                         description: str) -> FeatureSet:\n",
        "        \"\"\"\n",
        "        Combine different feature types into a single feature set\n",
        "        \"\"\"\n",
        "        # Start with base numeric features\n",
        "        X_train = fold_train[base_numeric].copy()\n",
        "        X_val = fold_val[base_numeric].copy()\n",
        "\n",
        "        # Add house type features if provided\n",
        "        if house_features:\n",
        "            X_train = pd.concat([X_train, house_features['train']], axis=1)\n",
        "            X_val = pd.concat([X_val, house_features['val']], axis=1)\n",
        "\n",
        "        # Add city/country features if provided\n",
        "        if city_country_features:\n",
        "            X_train = pd.concat([X_train, city_country_features['train']], axis=1)\n",
        "            X_val = pd.concat([X_val, city_country_features['val']], axis=1)\n",
        "\n",
        "        # Add target-encoded geographic features if provided\n",
        "        if geo_target_features:\n",
        "            for feature_name, feature_dict in geo_target_features.items():\n",
        "                X_train[feature_name] = feature_dict['train']\n",
        "                X_val[feature_name] = feature_dict['val']\n",
        "\n",
        "        # Add one-hot encoded geographic features if provided\n",
        "        if geo_onehot_features:\n",
        "            for feature_name, feature_dict in geo_onehot_features.items():\n",
        "                X_train = pd.concat([X_train, feature_dict['train']], axis=1)\n",
        "                X_val = pd.concat([X_val, feature_dict['val']], axis=1)\n",
        "\n",
        "        # Add price per square foot features if provided\n",
        "        if price_sqft_features:\n",
        "            X_train['outcode_price_per_sqft'] = price_sqft_features['train']\n",
        "            X_val['outcode_price_per_sqft'] = price_sqft_features['val']\n",
        "\n",
        "        return FeatureSet(\n",
        "            X_train=X_train,\n",
        "            X_val=X_val,\n",
        "            y_train=fold_train['log_price'],\n",
        "            y_val=fold_val['log_price'],\n",
        "            name=name,\n",
        "            description=description\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z9x_-Ev805O"
      },
      "source": [
        "### The CrossValidator: ATLAS's Experiment Runner\n",
        "\n",
        "Remember how ATLAS is our mythological helper, carrying the heavy load of model comparison? Well, the CrossValidator is like its brain - planning experiments, running tests, and keeping track of what works best.\n",
        "\n",
        "#### The Big Picture\n",
        "\n",
        "Our CrossValidator handles three key responsibilities:\n",
        "```\n",
        "Raw Data â†’ Train/Test Splitting â†’ Model Training â†’ Performance Tracking\n",
        "```\n",
        "\n",
        "But it does this:\n",
        "- Across multiple data folds\n",
        "- For different model types\n",
        "- With various feature combinations\n",
        "- While preventing common mistakes\n",
        "\n",
        "## Understanding the Code\n",
        "\n",
        "### 1. Model Management\n",
        "```python\n",
        "class CrossValidator:\n",
        "    def __init__(self, n_folds: int = 5):\n",
        "        self.models = {\n",
        "            'decision_tree': DecisionTreeRegressor(),\n",
        "            'random_forest': RandomForestRegressor(n_estimators=100),\n",
        "            'xgboost': XGBRegressor(n_estimators=100)\n",
        "        }\n",
        "```\n",
        "\n",
        "This setup gives us:\n",
        "- A simple decision tree (our baseline)\n",
        "- A random forest (for robustness)\n",
        "- XGBoost (for high performance)\n",
        "\n",
        "Each configured consistently for fair comparison.\n",
        "\n",
        "### 2. Experiment Design\n",
        "\n",
        "The evaluation happens in two stages:\n",
        "\n",
        "```\n",
        "Stage 1: Cross-Validation\n",
        "    â”œâ”€â”€ Split training data into 5 folds\n",
        "    â”œâ”€â”€ Train on 4 folds, test on 1\n",
        "    â”œâ”€â”€ Repeat 5 times\n",
        "    â””â”€â”€ Average the results\n",
        "\n",
        "Stage 2: Final Testing\n",
        "    â”œâ”€â”€ Train on all training data\n",
        "    â”œâ”€â”€ Test on held-out test set\n",
        "    â””â”€â”€ Compare with CV results\n",
        "```\n",
        "\n",
        "### 3. Progress Tracking\n",
        "\n",
        "We use nested progress bars:\n",
        "```python\n",
        "with tqdm(total=n_folds) as fold_pbar:          # Outer loop: folds\n",
        "    with tqdm(total=n_features) as feature_pbar: # Inner loop: features\n",
        "```\n",
        "\n",
        "This shows:\n",
        "- Overall progress through folds\n",
        "- Detailed progress within each fold\n",
        "- Estimated time remaining\n",
        "\n",
        "### 4. Performance Metrics\n",
        "\n",
        "We track four key metrics:\n",
        "\n",
        "| Metric | Purpose | Calculation |\n",
        "|--------|----------|------------|\n",
        "| RMSE | Overall error magnitude | `sqrt(mean((y_true - y_pred)Â²))` |\n",
        "| RÂ² | Explained variance | `1 - (residual_var / total_var)` |\n",
        "| MAE | Average error in pounds | `mean(abs(exp(y_true) - exp(y_pred)))` |\n",
        "| % MAE | Relative error | `mean(abs((true_price - pred_price) / true_price))` |\n",
        "\n",
        "### 5. Results Collection\n",
        "\n",
        "Results are stored in a structured format:\n",
        "```python\n",
        "{\n",
        "    'fold': fold_idx,                    # Which fold (or 'final' for test)\n",
        "    'feature_set': feature_set.name,     # What features were used\n",
        "    'model': model_name,                 # Which model type\n",
        "    'rmse': rmse_score,                  # Root Mean Squared Error\n",
        "    'r2': r2_score,                      # R-squared value\n",
        "    'mae': mean_absolute_error,          # Mean Absolute Error in pounds\n",
        "    'pct_mae': percentage_error          # Percentage Mean Absolute Error\n",
        "}\n",
        "```\n",
        "\n",
        "## Why This Design Works\n",
        "\n",
        "1. **Consistency**: Each model gets the same treatment\n",
        "2. **Fairness**: No data leakage between folds\n",
        "3. **Completeness**: Multiple metrics for different needs\n",
        "4. **Clarity**: Progress tracking and organised results\n",
        "\n",
        "## Common Pitfalls Avoided\n",
        "\n",
        "1. **Data Leakage**: Features are created independently for each fold\n",
        "2. **Selection Bias**: Cross-validation provides robust estimates\n",
        "3. **Overfitting**: Test set remains untouched until final evaluation\n",
        "4. **Metric Misuse**: Multiple metrics give complete picture\n",
        "\n",
        "## Looking Forward\n",
        "\n",
        "In the next section, we'll see ATLAS in action, running experiments and helping us understand what really drives house prices. But first, let's look at how to actually use this CrossValidator in practice.\n",
        "\n",
        "Remember: The CrossValidator isn't just running models - it's conducting scientific experiments to help us understand what really works for predicting house prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yk2z5Jy805P"
      },
      "outputs": [],
      "source": [
        "class CrossValidator:\n",
        "    \"\"\"Handles cross-validation and model evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, n_folds: int = 5, random_state: int = RANDOM_STATE):\n",
        "        self.n_folds = n_folds\n",
        "        self.random_state = random_state\n",
        "        self.models = {\n",
        "            'decision_tree': DecisionTreeRegressor(random_state=random_state),\n",
        "            'random_forest': RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                random_state=random_state\n",
        "            ),\n",
        "            'xgboost': XGBRegressor(\n",
        "                n_estimators=100,\n",
        "                random_state=random_state\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def evaluate_all_combinations(self,\n",
        "                                train_data: pd.DataFrame,\n",
        "                                test_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Evaluate all feature set and model combinations using:\n",
        "        1. K-fold CV on training data\n",
        "        2. Final evaluation on test set\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        encoder = FeatureEncoder()\n",
        "\n",
        "        # Calculate total iterations for progress tracking\n",
        "        n_folds = self.n_folds\n",
        "        n_models = len(self.models)\n",
        "\n",
        "        # PART 1: K-FOLD CROSS VALIDATION ON TRAINING DATA ONLY\n",
        "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
        "\n",
        "        print(\"\\nRunning cross-validation...\")\n",
        "\n",
        "        status_display = display('Starting fold 1...', display_id=True)\n",
        "\n",
        "        # Create main progress bar for folds\n",
        "        with tqdm(total=n_folds, desc=\"Folds\") as fold_pbar:\n",
        "            for fold_idx, (fold_train_idx, fold_val_idx) in enumerate(kf.split(train_data)):\n",
        "                # Get this fold's train/val split\n",
        "                fold_train = train_data.iloc[fold_train_idx].copy()\n",
        "                fold_val = train_data.iloc[fold_val_idx].copy()\n",
        "\n",
        "                # Mark as CV fold (for target encoding)\n",
        "                fold_train['cv_fold'] = fold_idx\n",
        "                fold_val['cv_fold'] = fold_idx\n",
        "\n",
        "                # Create features for this fold\n",
        "                feature_sets = encoder.create_fold_features(fold_train, fold_val)\n",
        "                n_features = len(feature_sets)\n",
        "\n",
        "                # Create nested progress bar for feature sets\n",
        "                with tqdm(total=n_features * n_models,\n",
        "                         desc=f\"Fold {fold_idx + 1} Progress\") as feature_pbar:\n",
        "\n",
        "                    # Evaluate each feature set and model combination\n",
        "                    for feature_set in feature_sets:\n",
        "                        for model_name, model in self.models.items():\n",
        "                            # Update status display\n",
        "                            status_display.update(\n",
        "                                f\"Fold {fold_idx + 1}: {model_name} on {feature_set.name}\"\n",
        "                            )\n",
        "\n",
        "                            model.fit(feature_set.X_train, feature_set.y_train)\n",
        "                            fold_val_pred = model.predict(feature_set.X_val)\n",
        "\n",
        "                            results.append({\n",
        "                                'fold': fold_idx,\n",
        "                                'feature_set': feature_set.name,\n",
        "                                'description': feature_set.description,\n",
        "                                'model': model_name,\n",
        "                                'split_type': 'cv_fold',\n",
        "                                'rmse': self._calculate_rmse(feature_set.y_val, fold_val_pred),\n",
        "                                'r2': r2_score(feature_set.y_val, fold_val_pred),\n",
        "                                'mae': mean_absolute_error(\n",
        "                                    np.exp(feature_set.y_val),\n",
        "                                    np.exp(fold_val_pred)\n",
        "                                ),\n",
        "                                'pct_mae': np.mean(np.abs(\n",
        "                                    (np.exp(feature_set.y_val) - np.exp(fold_val_pred)) /\n",
        "                                    np.exp(feature_set.y_val)\n",
        "                                )) * 100,\n",
        "                                'n_features': feature_set.X_train.shape[1]\n",
        "                            })\n",
        "                            feature_pbar.update(1)\n",
        "                fold_pbar.update(1)\n",
        "\n",
        "        # PART 2: FINAL EVALUATION ON TEST SET\n",
        "        print(\"\\nRunning final evaluation on test set...\")\n",
        "        status_display.update(\"Starting test set evaluation...\")\n",
        "\n",
        "        # Remove CV marking\n",
        "        train_data = train_data.drop('cv_fold', axis=1, errors='ignore')\n",
        "\n",
        "        # Create features using full training set and test set\n",
        "        final_feature_sets = encoder.create_fold_features(train_data, test_data)\n",
        "\n",
        "        # Create progress bar for final evaluation\n",
        "        with tqdm(total=len(final_feature_sets) * len(self.models),\n",
        "                 desc=\"Test Set Evaluation\") as test_pbar:\n",
        "\n",
        "            for feature_set in final_feature_sets:\n",
        "                for model_name, model in self.models.items():\n",
        "                    # Update status display\n",
        "                    status_display.update(\n",
        "                        f\"Test Set: {model_name} on {feature_set.name}\"\n",
        "                    )\n",
        "\n",
        "                    # Train on full training data\n",
        "                    model.fit(feature_set.X_train, feature_set.y_train)\n",
        "                    test_pred = model.predict(feature_set.X_val)\n",
        "\n",
        "                    results.append({\n",
        "                        'fold': 'final',\n",
        "                        'feature_set': feature_set.name,\n",
        "                        'description': feature_set.description,\n",
        "                        'model': model_name,\n",
        "                        'split_type': 'test',\n",
        "                        'rmse': self._calculate_rmse(feature_set.y_val, test_pred),\n",
        "                        'r2': r2_score(feature_set.y_val, test_pred),\n",
        "                        'mae': mean_absolute_error(\n",
        "                            np.exp(feature_set.y_val),\n",
        "                            np.exp(test_pred)\n",
        "                        ),\n",
        "                        'pct_mae': np.mean(np.abs(\n",
        "                            (np.exp(feature_set.y_val) - np.exp(test_pred)) /\n",
        "                            np.exp(feature_set.y_val)\n",
        "                        )) * 100,\n",
        "                        'n_features': feature_set.X_train.shape[1]\n",
        "                    })\n",
        "                    test_pbar.update(1)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def _calculate_rmse(self,\n",
        "                       y_true: pd.Series,\n",
        "                       y_pred: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Root Mean Squared Error\n",
        "        \"\"\"\n",
        "        return np.sqrt(np.mean((y_true - y_pred) ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z9sRfPA805P"
      },
      "source": [
        "### Running ATLAS: The Pipeline in Action\n",
        "\n",
        "ATLAS processes our house price data through a straightforward sequence:\n",
        "\n",
        "```python\n",
        "Raw Data â†’ PreProcessor â†’ Train/Test Split â†’ CrossValidator â†’ Results\n",
        "```\n",
        "\n",
        "## Running the Pipeline\n",
        "```python\n",
        "preprocessor = PreProcessor()\n",
        "df_processed = preprocessor.prepare_pre_split_features(df_with_outcode)\n",
        "train_data, test_data = preprocessor.create_train_test_split(df_processed)\n",
        "validator = CrossValidator()\n",
        "results = validator.evaluate_all_combinations(train_data, test_data)\n",
        "```\n",
        "\n",
        "For each model and feature combination, ATLAS:\n",
        "1. Splits data appropriately\n",
        "2. Creates features inside each fold\n",
        "3. Trains and evaluates models\n",
        "4. Collects performance metrics\n",
        "\n",
        "The progress bars show:\n",
        "- Overall completion (outer bar)\n",
        "- Current fold progress (inner bar)\n",
        "- Estimated time remaining\n",
        "\n",
        "Each experiment's results include:\n",
        "- Model type and feature set\n",
        "- Cross-validation scores\n",
        "- Final test set performance\n",
        "- Performance metrics (RMSE, RÂ², MAE, %MAE)\n",
        "\n",
        "This systematic approach lets us focus on interpreting results rather than managing experiments. Let's examine what ATLAS discovered about house price prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4k0u8TN805P"
      },
      "outputs": [],
      "source": [
        "def ATLAS_pipeline(df_with_outcode: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Run complete pipeline from raw data to model comparison\"\"\"\n",
        "\n",
        "    preprocessor = PreProcessor()\n",
        "\n",
        "    # Create pre-split features\n",
        "    df_processed = preprocessor.prepare_pre_split_features(df_with_outcode)\n",
        "\n",
        "    # Create initial train/test split\n",
        "    train_data, test_data = preprocessor.create_train_test_split(df_processed)\n",
        "\n",
        "    # Run cross-validation evaluation\n",
        "    validator = CrossValidator()\n",
        "    results = validator.evaluate_all_combinations(train_data, test_data)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Time the pipeline execution\n",
        "print(\"Running ATLAS pipeline (estimated time: 2 minutes)...\")\n",
        "\n",
        "results = ATLAS_pipeline(df_with_outcode)\n",
        "\n",
        "\n",
        "# Display key information about the results DataFrame\n",
        "print(\"\\nResults DataFrame Info:\")\n",
        "print(f\"Shape: {results.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(results.head())\n",
        "\n",
        "def display_results(results: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Display model performance summary with cross-validation and test results.\n",
        "\n",
        "    Args:\n",
        "        results: DataFrame containing model evaluation results with columns:\n",
        "                feature_set, model, split_type, r2, rmse, mae, pct_mae, description\n",
        "    \"\"\"\n",
        "    print(\"\\nModel Performance Summary:\")\n",
        "    print(\"-\" * 170)\n",
        "\n",
        "    # Print header\n",
        "    header = \"Features - Model\".ljust(100) + \" \"\n",
        "    header += \"CV RÂ²\".ljust(15)\n",
        "    header += \"CV RMSE\".ljust(15)\n",
        "    header += \"CV MAE (Â£)\".ljust(20)\n",
        "    header += \"CV %Error\".ljust(20)\n",
        "    print(header)\n",
        "    print(\"-\" * 170)\n",
        "\n",
        "    for (feature_set, model), group in results.groupby(['feature_set', 'model']):\n",
        "        cv_results = group[group['split_type'] == 'cv_fold']\n",
        "        test_results = group[group['split_type'] == 'test'].iloc[0]\n",
        "\n",
        "        # Create feature_model string using description\n",
        "        feature_model = f\"{test_results['description']} - {model}\"\n",
        "\n",
        "        # Print CV results\n",
        "        cv_line = feature_model.ljust(100) + \" \"\n",
        "        cv_line += f\"{cv_results['r2'].mean():.3f} Â±{cv_results['r2'].std():.3f}\".ljust(15)\n",
        "        cv_line += f\"{cv_results['rmse'].mean():.3f} Â±{cv_results['rmse'].std():.3f}\".ljust(15)\n",
        "        cv_line += f\"Â£{cv_results['mae'].mean():,.0f} Â±{cv_results['mae'].std():,.0f}\".ljust(20)\n",
        "        cv_line += f\"{cv_results['pct_mae'].mean():.1f} Â±{cv_results['pct_mae'].std():.1f}%\"\n",
        "        print(cv_line)\n",
        "\n",
        "        # Print test results (indented)\n",
        "        test_line = \"â†’ Test Results\".ljust(100) + \" \"\n",
        "        test_line += f\"{test_results['r2']:.3f}\".ljust(15)\n",
        "        test_line += f\"{test_results['rmse']:.3f}\".ljust(15)\n",
        "        test_line += f\"Â£{test_results['mae']:,.0f}\".ljust(20)\n",
        "        test_line += f\"{test_results['pct_mae']:.1f}%\"\n",
        "        print(test_line)\n",
        "\n",
        "# Usage:\n",
        "display_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znHR3cx9805P"
      },
      "source": [
        "## Unveiling the Drivers of London House Prices: Lessons from ATLAS\n",
        "\n",
        "Our exploration of London house price prediction, guided by the ATLAS pipeline, has yielded a wealth of insights. By systematically comparing a diverse set of models and feature combinations, we've gained a nuanced understanding of the key drivers of property value in this complex market. This analysis will delve into the performance of our top models, the impact of including price information, the effectiveness of our feature engineering techniques, and the implications of our findings for both model deployment and further research.\n",
        "\n",
        "### Top Performing Models: A Closer Look\n",
        "\n",
        "Among the myriad of models evaluated, three have distinguished themselves with their exceptional performance:\n",
        "\n",
        "| Model                                                                          | CV RÂ²         | Test RÂ²      | CV MAE (Â£)       | Test MAE (Â£)  |\n",
        "|-------------------------------------------------------------------------------|---------------|--------------|------------------|---------------|\n",
        "| Random Forest with All Features                                               | 0.903 Â±0.011  | 0.912        | Â£368,595 Â±49,041 | Â£387,417      |\n",
        "| XGBoost with All Features                                                     | 0.899 Â±0.009  | 0.913        | Â£379,417 Â±55,543 | Â£397,701      |\n",
        "| Random Forest with Area, Bedrooms, House Type, Outcode (Target), and Price/sqft | 0.902 Â±0.012 | 0.907        | Â£372,747 Â±48,481 | Â£416,147      |\n",
        "\n",
        "These models have achieved remarkable accuracy, explaining over 90% of the price variance (as measured by RÂ²) in both cross-validation and on the unseen test set. Their mean absolute errors (MAE) range from around Â£370,000 to Â£420,000, which, while substantial in absolute terms, are quite reasonable given the high prices and wide price range in the London market.\n",
        "\n",
        "The strong and consistent performance of these models across both cross-validation and the test set is a testament to the robustness of our modeling approach. It suggests that these models have successfully captured the underlying patterns and relationships in the data, rather than simply memorising noise or idiosyncrasies of the training set.\n",
        "\n",
        "### The Price Information Paradox\n",
        "\n",
        "One of the most striking findings from our experiments is the significant impact of including price-derived features, such as the average price per square foot at the outcode level. Models that incorporate this information consistently outperform those that don't, with improvements in MAE ranging from Â£40,000 to Â£50,000.\n",
        "\n",
        "This improvement in accuracy is substantial and underscores the importance of considering current market conditions in property valuation. By providing the models with information about prevailing price levels in different areas, we enable them to make more context-aware predictions.\n",
        "\n",
        "However, this boost in performance comes with a important caveat. By including current price information, our models risk amplifying feedback loops in the housing market. If such models were to be widely adopted and used to inform pricing decisions, they could potentially exacerbate both upward and downward price trends. In a rising market, the models would predict higher prices, which could in turn drive actual prices higher if used to set asking prices or guide bidding. Conversely, in a falling market, the models could contribute to a downward spiral.\n",
        "\n",
        "This is a well-known challenge in the real estate industry, and one that major players like Zoopla and Rightmove actively monitor and manage. It highlights the importance of considering not just the accuracy of our models, but also their potential impact on the market they seek to predict.\n",
        "\n",
        "### Models without Price Information: A Fundamental Perspective\n",
        "\n",
        "Given the potential risks associated with including current price information, it's worth examining the performance of models that rely solely on fundamental property characteristics and location.\n",
        "\n",
        "Among these models, several stand out:\n",
        "\n",
        "| Model                                                                            | CV RÂ²         | Test RÂ²      | CV MAE (Â£)       | Test MAE (Â£)  |\n",
        "|--------------------------------------------------------------------------------|---------------|--------------|------------------|---------------|\n",
        "| XGBoost with Area, Bedrooms, and Outcode (One-hot)                              | 0.881 Â±0.013  | 0.899        | Â£418,617 Â±58,112 | Â£432,491      |\n",
        "| Random Forest with Area, Bedrooms, Location (Target)                            | 0.822 Â±0.031  | 0.815        | Â£506,152 Â±84,254 | Â£593,446      |\n",
        "| Random Forest with Area, Bedrooms, House Type, City/County, Outcode (Target)    | 0.855 Â±0.023  | 0.887        | Â£447,107 Â±54,538 | Â£449,116      |\n",
        "\n",
        "While these models don't quite match the accuracy of those including price information, they still achieve impressive performance. With RÂ² scores mostly in the 0.80 to 0.90 range and MAEs around Â£400,000 to Â£600,000, they demonstrate that a substantial portion of a property's value can be explained by its intrinsic characteristics and location.\n",
        "\n",
        "These models provide a valuable perspective on the fundamental drivers of house prices, independent of current market conditions. They can help identify areas or property types that may be over- or under-valued relative to their inherent attributes. In practice, such models could be used in conjunction with price-aware models to provide a more comprehensive view of a property's value.\n",
        "\n",
        "### The Importance of Validation: Ensuring Reliability\n",
        "\n",
        "A crucial aspect of our modeling process that deserves highlighting is the rigor of our validation strategy. By employing stratified k-fold cross-validation, we ensure that our performance estimates are reliable and representative of the models' true predictive power.\n",
        "\n",
        "Stratified k-fold cross-validation involves splitting the data into k folds (in our case, 5), while ensuring that each fold has a similar distribution of the target variable (price). The model is then trained k times, each time using k-1 folds for training and the remaining fold for validation. The performance is then averaged across all k validation folds.\n",
        "\n",
        "This approach has several advantages over a simple train-test split:\n",
        "1. It provides a more robust estimate of model performance, as it averages over k different train-test splits rather than relying on a single split.\n",
        "2. By stratifying the folds based on the target variable, it ensures that each fold is representative of the overall data distribution, reducing the risk of lucky or unlucky splits.\n",
        "\n",
        "Moreover, by maintaining a strict separation between our training and validation data within each fold, and between all the training folds and the final test set, we avoid the pitfalls of data leakage and overfitting. Data leakage occurs when information from the validation or test sets inadvertently leaks into the model training process, leading to overly optimistic performance estimates. Overfitting happens when a model learns to fit the noise or peculiarities of the training data, rather than the underlying patterns, leading to poor generalisation to new data.\n",
        "\n",
        "Our models' strong and consistent performance across the cross-validation folds and on the unseen test set demonstrates that they have successfully learned genuine patterns in the data and can generalise well to new, unseen properties. This is crucial for real-world application, where the model will be applied to properties it has never seen before.\n",
        "\n",
        "### Feature Engineering: The Art of Extracting Signal from Noise\n",
        "\n",
        "Another key lesson from our analysis is the importance and nuance of feature engineering, particularly when dealing with geographical data.\n",
        "\n",
        "Our geographical features presented a hierarchy of granularity:\n",
        "- Outcode (e.g., \"SW1\")\n",
        "- Postcode (e.g., \"SW1A 1AA\")\n",
        "- Location (e.g., \"Westminster\")\n",
        "\n",
        "Each level provided a different trade-off between specificity and data sparsity. While more granular levels (like location) can potentially provide more specific information, they also suffer from data sparsity, with many locations having very few or even just one property.\n",
        "\n",
        "Our solution was a hierarchical target encoding scheme. For each level, we calculated the mean price in the training data. Then, when encoding a particular property, if the specific level (e.g., postcode) had sufficient data, we used its mean price. If not, we fell back to the mean price of the next higher level (e.g., outcode). This way, we extracted as much specific information as the data allowed, while still providing a robust fallback for sparse levels.\n",
        "\n",
        "This encoding scheme proved very effective, with models using these features achieving strong performance. It demonstrates that, with careful engineering, geographical information can be a powerful predictor of house prices, even without resorting to complex geospatial techniques.\n",
        "\n",
        "Beyond geographical features, our experiments also highlighted the predictive power of even simple property attributes like area, number of bedrooms, and property type. Models using just these features achieved respectable performance, forming a strong baseline upon which more complex models could improve.\n",
        "\n",
        "### Ethical Considerations, Human Impact, and Future Directions\n",
        "\n",
        "As we marvel at the predictive power of our models, it's crucial that we also pause to consider the ethical implications of our work. Housing is not just a financial asset, but a fundamental human need. The prices predicted by our models have real consequences for real people - they can determine whether a family can afford to buy their dream home, whether a pensioner can comfortably retire, or whether a young professional can afford to live near their work.\n",
        "\n",
        "With this in mind, we have a responsibility to ensure that our models are not just accurate, but also fair and unbiased. We must be vigilant to potential sources of bias in our data and algorithms, and work to mitigate them. For example, if our training data under-represents certain areas or demographic groups, our models may learn to undervalue these properties, perpetuating or even amplifying existing inequalities.\n",
        "\n",
        "Moreover, we must consider the potential unintended consequences of our models' usage. If used improperly, such as to guide predatory pricing practices or to justify rent hikes, our models could harm the very people they're meant to serve. It's our responsibility to ensure that our models are used ethically and for the benefit of all stakeholders.\n",
        "\n",
        "On a more positive note, our models also have the potential to empower individuals and promote transparency in the housing market. By providing accurate and unbiased valuations, they can help buyers and sellers make informed decisions, reducing information asymmetries and the potential for exploitation. They can also help policymakers and urban planners better understand the dynamics of the housing market, informing policies that promote affordability and social equity.\n",
        "\n",
        "Our journey into London house price prediction has been one of technical exploration, but also one of growing awareness of the human implications of our work. We've seen the power of machine learning to uncover complex patterns and dynamics in the housing market, but also the potential pitfalls and ethical considerations that come with this power.\n",
        "\n",
        "As we look to the future, several exciting directions beckon:\n",
        "\n",
        "1. **Ensemble Methods**: Given the strong performance of multiple models, combining their predictions through techniques like stacking or blending could potentially yield even greater accuracy and robustness.\n",
        "\n",
        "2. **Advanced Feature Engineering**: While our current features have proven effective, there's always room for refinement. Techniques like feature interaction, clustering, or dimensional reduction could uncover additional predictive signals.\n",
        "\n",
        "3. **Temporal Dynamics**: Our current models provide a static snapshot of the market. Incorporating temporal features like price trends, economic indicators, or seasonal effects could enable more dynamic and forward-looking predictions.\n",
        "\n",
        "4. **Model Interpretability**: As powerful as our models are, their complexity can hinder interpretation. Techniques like feature importance analysis, partial dependence plots, or SHAP values could help shed light on how the models make their predictions, increasing transparency and trust.\n",
        "\n",
        "5. **Application and Deployment**: Finally, the true test of our models will be in their application to real-world pricing decisions. This will require not just technical excellence, but also close collaboration with domain experts to ensure the models are used appropriately and responsibly.\n",
        "\n",
        "As we embark on these future directions, let us proceed with a commitment to not just technical excellence, but also to social responsibility. Let us strive to build models that are not just accurate, but also fair, transparent, and beneficial to all. Let us engage closely with the communities impacted by our work, learning from their perspectives and ensuring that our models serve their needs.\n",
        "\n",
        "In doing so, we have the potential to not just predict house prices, but to contribute to a housing market that is more efficient, more equitable, and more responsive to the needs of its participants. This is the ultimate promise and challenge of our work - to use the power of data and algorithms to build a better, fairer world for all.\n",
        "\n",
        "As we conclude this phase of our journey, let us do so with gratitude for the insights gained, with humility in the face of the challenges ahead, and with hope for the positive impact we can make. Ultimately, the true measure of our success will not be just the accuracy of our predictions, but the positive impact we have on the lives of those touched by the housing market. The path forward is not always clear or easy, but with ATLAS as our guide and our values as our compass, I am confident that we will navigate it successfully, one home at a time.\n",
        "\n",
        "### Thanks for Learning!\n",
        "\n",
        "This notebook is part of the Supervised Machine Learning from First Principles series.\n",
        "\n",
        "Â© 2025 Powell-Clark Limited. Licensed under Apache License 2.0.\n",
        "\n",
        "If you found this helpful, please cite as:\n",
        "```\n",
        "Powell-Clark (2025). Supervised Machine Learning from First Principles.\n",
        "GitHub: https://github.com/powell-clark/supervised-machine-learning\n",
        "```\n",
        "\n",
        "Questions or feedback? Contact emmanuel@powellclark.com"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}