{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9c: Transformers & Attention Mechanisms\n",
    "\n",
    "In Lesson 9b, we learned how RNNs process sequences one token at a time, maintaining hidden state as they go. This sequential processing creates two problems: it's slow (can't parallelize), and it struggles with long-range dependencies (information from 100 tokens ago gets diluted).\n",
    "\n",
    "Transformers solve both problems by processing all tokens simultaneously and using attention to directly connect any two positions in the sequence, regardless of distance.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How attention lets models focus on relevant parts of the input\n",
    "- The math behind self-attention and multi-head attention\n",
    "- The complete Transformer architecture (encoder-decoder)\n",
    "- Why positional encodings matter for sequence modeling\n",
    "- Differences between BERT (bidirectional) and GPT (autoregressive)\n",
    "- Practical NLP with Hugging Face Transformers\n",
    "- Vision Transformers and beyond language\n",
    "\n",
    "**Prerequisites:**\n",
    "- Neural networks (Lesson 3a, 3b)\n",
    "- RNNs helpful but not required (Lesson 9b)\n",
    "- Basic linear algebra (matrix multiplication, dot products)\n",
    "\n",
    "**Why this matters:**\n",
    "Transformers power ChatGPT, Claude, BERT, and most modern NLP systems. Since 2017, they've expanded beyond language into computer vision (Vision Transformers), protein folding (AlphaFold), and multimodal tasks. If you want to work with modern AI systems, you need to understand how they work.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "This notebook requires the Hugging Face Transformers library, the de facto standard for working with Transformer models in 2025. We'll automatically install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip if not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package_name.split('[')[0].replace('-', '_'))\n",
    "        print(f\"\u2713 {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name])\n",
    "        print(f\"\u2713 {package_name} installed successfully\")\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    'transformers>=4.35.0',\n",
    "    'torch>=2.0.0',\n",
    "    'numpy>=1.24.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'scikit-learn>=1.3.0',\n",
    "    'pandas>=2.0.0',\n",
    "    'seaborn>=0.12.0',\n",
    "    'datasets>=2.14.0'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n\u2713 All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding Attention: The Core Mechanism\n",
    "\n",
    "### 2.1 The Problem with RNNs\n",
    "\n",
    "Before Transformers, sequence modeling relied on RNNs/LSTMs, which had critical limitations:\n",
    "\n",
    "1. **Sequential processing**: Must process tokens one-by-one, cannot parallelize\n",
    "2. **Limited context**: Struggle with long-range dependencies (even with LSTMs)\n",
    "3. **Information bottleneck**: Entire sequence compressed into fixed-size hidden state\n",
    "\n",
    "**Example failure case:**\n",
    "```\n",
    "\"The animal didn't cross the street because it was too tired.\"\n",
    "```\n",
    "To understand what \"it\" refers to, the model must maintain context from \"animal\" many tokens ago.\n",
    "\n",
    "### 2.2 The Attention Mechanism: How It Works\n",
    "\n",
    "**Key innovation**: Instead of processing sequentially, attention allows the model to **look at all positions simultaneously** and decide which parts to focus on.\n",
    "\n",
    "**Attention in one sentence:**\n",
    "*\"For each word, compute how much I should attend to every other word, then create a weighted combination.\"*\n",
    "\n",
    "### 2.3 Attention Mathematics\n",
    "\n",
    "The attention mechanism computes three matrices from the input:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\"\n",
    "- **Value (V)**: \"What information do I provide?\"\n",
    "\n",
    "**Scaled Dot-Product Attention formula:**\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q\u00b7K^T / \u221ad_k) \u00b7 V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Q\u00b7K^T`: Compute similarity scores between all query-key pairs\n",
    "- `\u221ad_k`: Scaling factor (prevents gradients from vanishing)\n",
    "- `softmax(...)`: Convert scores to probability distribution\n",
    "- `... \u00b7 V`: Weight values by attention scores\n",
    "\n",
    "Let's implement attention from scratch to understand it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement scaled dot-product attention from scratch\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix of shape (batch_size, seq_len, d_k)\n",
    "        K: Key matrix of shape (batch_size, seq_len, d_k)\n",
    "        V: Value matrix of shape (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention-weighted values\n",
    "        attention_weights: Attention scores (for visualization)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores (Q \u00b7 K^T)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by \u221ad_k (prevents softmax saturation)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Weight values by attention scores\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Demonstrate with simple example\n",
    "print(\"Demonstrating Scaled Dot-Product Attention:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create simple inputs (batch_size=1, seq_len=4, d_model=8)\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Random query, key, value matrices\n",
    "Q = torch.randn(1, seq_len, d_model)\n",
    "K = torch.randn(1, seq_len, d_model)\n",
    "V = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Compute attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Query (Q): {Q.shape}\")\n",
    "print(f\"  Key (K):   {K.shape}\")\n",
    "print(f\"  Value (V): {V.shape}\")\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(\"\\nAttention weights (how much each position attends to others):\")\n",
    "print(attn_weights[0].detach().numpy())\n",
    "print(\"\\nNote: Each row sums to 1.0 (probability distribution)\")\n",
    "print(f\"Row sums: {attn_weights[0].sum(dim=-1).numpy()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), \n",
    "           annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "           xticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "           yticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "           cbar_kws={'label': 'Attention Weight'})\n",
    "plt.xlabel('Key Position', fontsize=12)\n",
    "plt.ylabel('Query Position', fontsize=12)\n",
    "plt.title('Attention Weight Matrix\\n(Each row shows where that position attends)', \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_weights.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"Each position can attend to ALL other positions simultaneously.\")\n",
    "print(\"This allows parallel processing and unlimited context window!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Multi-Head Attention: Learning Multiple Representations\n",
    "\n",
    "### 3.1 Why Multiple Heads?\n",
    "\n",
    "**Problem with single attention**: It can only learn one type of relationship between words.\n",
    "\n",
    "**Solution**: Use **multiple attention heads** in parallel, each learning different aspects:\n",
    "- Head 1 might learn syntactic dependencies (subject-verb agreement)\n",
    "- Head 2 might learn semantic relationships (synonyms, antonyms)\n",
    "- Head 3 might learn positional patterns (nearby words)\n",
    "\n",
    "### 3.2 Multi-Head Attention Architecture\n",
    "\n",
    "**Process:**\n",
    "1. Project Q, K, V into `h` different subspaces (using learned linear projections)\n",
    "2. Compute attention in each subspace independently (parallel)\n",
    "3. Concatenate all heads\n",
    "4. Apply final linear projection\n",
    "\n",
    "**Mathematical formulation:**\n",
    "```\n",
    "MultiHead(Q, K, V) = Concat(head_1, ..., head_h) \u00b7 W^O\n",
    "\n",
    "where head_i = Attention(Q\u00b7W^Q_i, K\u00b7W^K_i, V\u00b7W^V_i)\n",
    "```\n",
    "\n",
    "**Standard configuration (BERT, GPT):**\n",
    "- Model dimension `d_model = 768`\n",
    "- Number of heads `h = 12`\n",
    "- Each head dimension `d_k = d_model / h = 64`\n",
    "\n",
    "Let's implement multi-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement multi-head attention from scratch\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \n",
    "    This is the core component of Transformer architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        Transpose to shape (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            Q, K, V: Query, Key, Value tensors of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output\n",
    "            attention_weights: Attention scores from all heads\n",
    "        \"\"\"\n",
    "        batch_size = Q.shape[0]\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(Q)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = self.split_heads(Q, batch_size)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # 3. Scaled dot-product attention for all heads in parallel\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Demonstrate multi-head attention\n",
    "print(\"\\nMulti-Head Attention Demonstration:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create multi-head attention module\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "# Create sample input\n",
    "sample_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(sample_input, sample_input, sample_input)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model dimension (d_model): {d_model}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Dimension per head (d_k): {d_model // num_heads}\")\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch_size, num_heads, seq_len, seq_len)\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"\\nTotal parameters in multi-head attention: {total_params:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize attention from different heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(8):\n",
    "    sns.heatmap(attn_weights[0, head_idx].detach().numpy(), \n",
    "               cmap='YlOrRd', ax=axes[head_idx],\n",
    "               cbar=False, square=True)\n",
    "    axes[head_idx].set_title(f'Head {head_idx + 1}', fontsize=11, fontweight='bold')\n",
    "    axes[head_idx].set_xlabel('Key', fontsize=9)\n",
    "    axes[head_idx].set_ylabel('Query', fontsize=9)\n",
    "\n",
    "plt.suptitle('Attention Patterns from 8 Different Heads\\n(Each head learns different relationships)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('multihead_attention_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"Different attention heads learn to focus on different aspects of the input.\")\n",
    "print(\"This allows the model to capture multiple types of relationships simultaneously!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Positional Encoding: Injecting Sequence Order\n",
    "\n",
    "### 4.1 The Position Problem\n",
    "\n",
    "**Critical issue**: Attention has no notion of word order!\n",
    "\n",
    "- \"The cat chased the mouse\" and \"The mouse chased the cat\" would be identical to pure attention\n",
    "- But word order carries crucial meaning!\n",
    "\n",
    "**Solution**: Add **positional encodings** to the input embeddings.\n",
    "\n",
    "### 4.2 Sinusoidal Positional Encoding\n",
    "\n",
    "The original Transformer paper (\"Attention Is All You Need\") uses sinusoidal functions:\n",
    "\n",
    "```\n",
    "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `pos`: Position in the sequence (0, 1, 2, ...)\n",
    "- `i`: Dimension index (0, 1, 2, ..., d_model/2)\n",
    "\n",
    "**Why sinusoidal?**\n",
    "- Different frequencies for different dimensions\n",
    "- Model can learn to attend to relative positions\n",
    "- Works for sequences longer than training data\n",
    "\n",
    "**Alternative**: Learned positional embeddings (used in BERT, GPT)\n",
    "- Treat positions as vocabulary items\n",
    "- Learn embedding for each position\n",
    "- More flexible but limited to max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement positional encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding as described in 'Attention Is All You Need'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the division term for each dimension\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be saved)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# Visualize positional encodings\n",
    "print(\"\\nPositional Encoding Visualization:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "positional_encoding = pos_encoder.pe[0].numpy()\n",
    "\n",
    "print(f\"Positional encoding shape: {positional_encoding.shape}\")\n",
    "print(f\"  (max_len={max_len}, d_model={d_model})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot positional encodings\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Heatmap of positional encodings\n",
    "im1 = ax1.imshow(positional_encoding.T, cmap='RdBu', aspect='auto')\n",
    "ax1.set_xlabel('Position in Sequence', fontsize=11)\n",
    "ax1.set_ylabel('Embedding Dimension', fontsize=11)\n",
    "ax1.set_title('Positional Encoding Matrix\\n(Sinusoidal Patterns)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Plot encodings for specific positions\n",
    "positions_to_plot = [0, 10, 30, 60]\n",
    "for pos in positions_to_plot:\n",
    "    ax2.plot(positional_encoding[pos, :50], label=f'Position {pos}', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Embedding Dimension', fontsize=11)\n",
    "ax2.set_ylabel('Encoding Value', fontsize=11)\n",
    "ax2.set_title('Positional Encodings for Different Positions\\n(First 50 dimensions)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('positional_encoding.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"\u2022 Different dimensions oscillate at different frequencies\")\n",
    "print(\"\u2022 Each position has a unique encoding pattern\")\n",
    "print(\"\u2022 The model can learn to use these patterns to understand position and distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Complete Transformer Architecture\n",
    "\n",
    "### 5.1 Transformer Building Blocks\n",
    "\n",
    "The complete Transformer consists of:\n",
    "\n",
    "**Encoder:**\n",
    "1. Input Embedding + Positional Encoding\n",
    "2. N \u00d7 Encoder Layers, each containing:\n",
    "   - Multi-Head Self-Attention\n",
    "   - Add & Normalize (residual connection + layer normalization)\n",
    "   - Feed-Forward Network (2 linear layers with ReLU)\n",
    "   - Add & Normalize\n",
    "\n",
    "**Decoder:**\n",
    "1. Output Embedding + Positional Encoding\n",
    "2. N \u00d7 Decoder Layers, each containing:\n",
    "   - Masked Multi-Head Self-Attention (can't see future)\n",
    "   - Add & Normalize\n",
    "   - Multi-Head Cross-Attention (attends to encoder output)\n",
    "   - Add & Normalize\n",
    "   - Feed-Forward Network\n",
    "   - Add & Normalize\n",
    "3. Linear + Softmax (output probabilities)\n",
    "\n",
    "**Standard configuration (original paper):**\n",
    "- N = 6 encoder and decoder layers\n",
    "- d_model = 512\n",
    "- num_heads = 8\n",
    "- d_ff = 2048 (feed-forward hidden dimension)\n",
    "\n",
    "### 5.2 Key Innovations\n",
    "\n",
    "**Residual Connections:**\n",
    "```\n",
    "output = LayerNorm(x + Sublayer(x))\n",
    "```\n",
    "Enables training very deep networks, gradients flow easily.\n",
    "\n",
    "**Layer Normalization:**\n",
    "Normalizes across features for each example, stabilizes training.\n",
    "\n",
    "**Feed-Forward Networks:**\n",
    "```\n",
    "FFN(x) = ReLU(x\u00b7W_1 + b_1)\u00b7W_2 + b_2\n",
    "```\n",
    "Applied independently to each position, adds non-linearity and capacity.\n",
    "\n",
    "Let's build a simplified Transformer encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Transformer Encoder Layer\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer encoder layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through encoder layer.\n",
    "        \n",
    "        Implements: x \u2192 Self-Attention \u2192 Add&Norm \u2192 FFN \u2192 Add&Norm\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Full Transformer encoder (stack of N encoder layers).\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers=6, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Pass input through all encoder layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Demonstrate Transformer Encoder\n",
    "print(\"\\nTransformer Encoder Architecture:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Standard configuration\n",
    "encoder = TransformerEncoder(\n",
    "    num_layers=6,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Sample input\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "d_model = 512\n",
    "\n",
    "sample_input = torch.randn(batch_size, seq_len, d_model)\n",
    "encoder_output = encoder(sample_input)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Number of layers: 6\")\n",
    "print(f\"  Model dimension: 512\")\n",
    "print(f\"  Number of heads: 8\")\n",
    "print(f\"  Feed-forward dimension: 2048\")\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {encoder_output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nThis is the EXACT architecture that revolutionized AI in 2017!\")\n",
    "print(\"Models like BERT and GPT are built on this foundation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. BERT vs GPT: Two Paradigms\n",
    "\n",
    "### 6.1 BERT: Bidirectional Encoder Representations\n",
    "\n",
    "**Architecture**: Encoder-only Transformer\n",
    "\n",
    "**Key features:**\n",
    "- **Bidirectional**: Can see both left and right context simultaneously\n",
    "- **Masked Language Modeling (MLM)**: Training objective masks random tokens, predicts them\n",
    "- **Next Sentence Prediction (NSP)**: Learns relationships between sentence pairs\n",
    "\n",
    "**Use cases:**\n",
    "- Text classification (sentiment analysis, spam detection)\n",
    "- Named entity recognition\n",
    "- Question answering\n",
    "- Any task requiring full sequence understanding\n",
    "\n",
    "**Example BERT models (2025):**\n",
    "- BERT-base: 110M parameters, 12 layers\n",
    "- BERT-large: 340M parameters, 24 layers\n",
    "- RoBERTa: Optimized BERT variant\n",
    "- DeBERTa: State-of-the-art encoder (2025)\n",
    "\n",
    "### 6.2 GPT: Generative Pre-trained Transformer\n",
    "\n",
    "**Architecture**: Decoder-only Transformer\n",
    "\n",
    "**Key features:**\n",
    "- **Unidirectional (causal)**: Can only see left context (past tokens)\n",
    "- **Autoregressive generation**: Predicts next token given previous tokens\n",
    "- **Zero-shot and few-shot learning**: Can perform tasks without fine-tuning\n",
    "\n",
    "**Use cases:**\n",
    "- Text generation (stories, code, dialogue)\n",
    "- Language modeling\n",
    "- Completion tasks\n",
    "- Instruction following (ChatGPT, Claude)\n",
    "\n",
    "**Example GPT models (2025):**\n",
    "- GPT-2: 1.5B parameters\n",
    "- GPT-3: 175B parameters\n",
    "- GPT-4: Multimodal, exact size undisclosed\n",
    "- Claude (Anthropic): Constitutional AI approach\n",
    "\n",
    "### 6.3 Comparison Table\n",
    "\n",
    "| Aspect | BERT | GPT |\n",
    "|--------|------|-----|\n",
    "| Architecture | Encoder-only | Decoder-only |\n",
    "| Attention | Bidirectional | Causal (masked) |\n",
    "| Training | Masked LM | Next token prediction |\n",
    "| Strengths | Understanding | Generation |\n",
    "| Fine-tuning | Required for most tasks | Often works zero-shot |\n",
    "| Speed | Faster (parallel) | Slower (sequential) |\n",
    "\n",
    "Let's use both models with Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained BERT for text classification\n",
    "\n",
    "print(\"\\nDemonstrating BERT for Sentiment Analysis:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load pre-trained BERT model for sentiment analysis\n",
    "# Using a distilled version for faster inference\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"What a terrible waste of time. I hated this film.\",\n",
    "    \"The movie was okay, nothing special but not bad either.\",\n",
    "    \"Best film I've seen this year! Highly recommend it!\",\n",
    "    \"Disappointed. The plot made no sense and acting was poor.\"\n",
    "]\n",
    "\n",
    "print(\"BERT Sentiment Analysis Results:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    result = sentiment_pipeline(sentence)[0]\n",
    "    sentiment = result['label']\n",
    "    confidence = result['score']\n",
    "    print(f\"Text: {sentence}\")\n",
    "    print(f\"  \u2192 {sentiment} (confidence: {confidence:.3f})\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey: BERT excels at classification tasks due to bidirectional context!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT-2 for text generation\n",
    "\n",
    "print(\"\\nDemonstrating GPT-2 for Text Generation:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load GPT-2 text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Prompts for generation\n",
    "prompts = [\n",
    "    \"Artificial intelligence will\",\n",
    "    \"The future of machine learning is\",\n",
    "    \"Deep learning models are\"\n",
    "]\n",
    "\n",
    "print(\"GPT-2 Text Generation Results:\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )[0]['generated_text']\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\\n\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nKey: GPT excels at generation tasks due to autoregressive training!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Fine-Tuning Transformers: Practical Implementation\n",
    "\n",
    "### 7.1 Transfer Learning with Transformers\n",
    "\n",
    "**The standard workflow (2025):**\n",
    "1. Start with pre-trained model (BERT, RoBERTa, etc.)\n",
    "2. Add task-specific head (classification, NER, etc.)\n",
    "3. Fine-tune on your dataset\n",
    "4. Evaluate and iterate\n",
    "\n",
    "**Why this works:**\n",
    "- Pre-trained models learn general language understanding\n",
    "- Fine-tuning adapts to specific task/domain\n",
    "- Requires much less data than training from scratch\n",
    "- BERT trained on 3.3B words, you can fine-tune with thousands\n",
    "\n",
    "Let's fine-tune BERT on a text classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune BERT for text classification\n",
    "\n",
    "print(\"\\nFine-Tuning BERT for Custom Classification:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create synthetic dataset (in practice, use real data)\n",
    "texts_positive = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"Absolutely love it, highly recommend.\",\n",
    "    \"Best purchase I've made this year.\",\n",
    "    \"Outstanding quality and fast delivery.\",\n",
    "    \"Very satisfied with this item.\"\n",
    "] * 20  # Repeat for more samples\n",
    "\n",
    "texts_negative = [\n",
    "    \"Terrible product, complete waste of money.\",\n",
    "    \"Do not buy this, very disappointed.\",\n",
    "    \"Poor quality and broke immediately.\",\n",
    "    \"Worst purchase ever, asking for refund.\",\n",
    "    \"Not as described, very unhappy.\"\n",
    "] * 20\n",
    "\n",
    "# Combine and create labels\n",
    "all_texts = texts_positive + texts_negative\n",
    "all_labels = [1] * len(texts_positive) + [0] * len(texts_negative)\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(all_texts))\n",
    "all_texts = [all_texts[i] for i in indices]\n",
    "all_labels = [all_labels[i] for i in indices]\n",
    "\n",
    "# Split data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    all_texts, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Training samples: {len(train_texts)}\")\n",
    "print(f\"  Test samples: {len(test_texts)}\")\n",
    "print(f\"  Positive: {sum(train_labels)}, Negative: {len(train_labels) - sum(train_labels)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "test_dataset = CustomDataset(test_encodings, test_labels)\n",
    "\n",
    "print(\"\\nData prepared for training!\")\n",
    "print(f\"Sample tokenized input:\")\n",
    "print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(train_encodings['input_ids'][0][:20])}\")\n",
    "print(f\"  Input IDs shape: {len(train_encodings['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and define training\n",
    "\n",
    "# Load pre-trained model with classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINE-TUNING BERT MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Epochs: 3\")\n",
    "print(f\"Batch size: 16\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\u2713 Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "\n",
    "# Evaluate on test set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNED MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {eval_results['eval_accuracy']*100:.2f}%\")\n",
    "print(f\"Test Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test on custom examples\n",
    "test_examples = [\n",
    "    \"This is amazing quality for the price!\",\n",
    "    \"Complete garbage, don't waste your money.\",\n",
    "    \"Decent product, works as expected.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions on custom examples:\\n\")\n",
    "for text in test_examples:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    confidence = torch.softmax(outputs.logits, dim=1)[0][prediction].item()\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  \u2192 {sentiment} (confidence: {confidence:.3f})\\n\")\n",
    "\n",
    "print(\"This demonstrates the full fine-tuning workflow used in production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Vision Transformers (ViT): Beyond NLP\n",
    "\n",
    "### 8.1 Transformers for Computer Vision\n",
    "\n",
    "**Breakthrough (2020)**: \"An Image is Worth 16x16 Words\" paper showed Transformers can match or exceed CNNs on image tasks!\n",
    "\n",
    "**Vision Transformer (ViT) architecture:**\n",
    "1. Split image into fixed-size patches (e.g., 16\u00d716 pixels)\n",
    "2. Flatten each patch into a vector\n",
    "3. Linear projection to embedding dimension\n",
    "4. Add positional embeddings\n",
    "5. Feed through standard Transformer encoder\n",
    "6. Use [CLS] token for classification\n",
    "\n",
    "**Example**: 224\u00d7224 image \u2192 14\u00d714 grid of 16\u00d716 patches \u2192 196 tokens\n",
    "\n",
    "**Key insight**: Treat image patches exactly like words in a sentence!\n",
    "\n",
    "### 8.2 ViT vs CNNs (2025)\n",
    "\n",
    "**Vision Transformers advantages:**\n",
    "- Better scaling with data (excel on huge datasets)\n",
    "- Global receptive field from layer 1\n",
    "- More interpretable attention patterns\n",
    "- Easier to adapt to different image sizes\n",
    "\n",
    "**CNNs still better when:**\n",
    "- Small datasets (< 1M images)\n",
    "- Resource constraints (mobile/edge)\n",
    "- Fine-grained localization needed\n",
    "\n",
    "**Hybrid approaches (2025):**\n",
    "- Swin Transformer: Hierarchical ViT with shifted windows\n",
    "- CoAtNet: Combines convolution and attention\n",
    "- BEiT: Self-supervised ViT pre-training\n",
    "\n",
    "Let's use a pre-trained Vision Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vision Transformer for image classification\n",
    "\n",
    "print(\"\\nDemonstrating Vision Transformer (ViT):\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load ViT model trained on ImageNet\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "print(\"Vision Transformer Model Loaded:\")\n",
    "print(f\"  Model: google/vit-base-patch16-224\")\n",
    "print(f\"  Patch size: 16\u00d716\")\n",
    "print(f\"  Image size: 224\u00d7224\")\n",
    "print(f\"  Number of patches: (224/16)\u00b2 = 196\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in vit_model.parameters()):,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Download a sample image\n",
    "url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "try:\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make prediction\n",
    "    outputs = vit_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class = vit_model.config.id2label[predicted_class_idx]\n",
    "    confidence = torch.softmax(logits, dim=1)[0][predicted_class_idx].item()\n",
    "    \n",
    "    print(\"\\nImage Classification Result:\")\n",
    "    print(f\"  Predicted class: {predicted_class}\")\n",
    "    print(f\"  Confidence: {confidence:.3f}\")\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Vision Transformer Prediction:\\n{predicted_class} ({confidence:.1%} confidence)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vit_prediction.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not download image: {e}\")\n",
    "    print(\"ViT model loaded successfully and ready for image classification!\")\n",
    "\n",
    "print(\"\\nKey Insight: Vision Transformers treat images as sequences of patches,\")\n",
    "print(\"applying the same attention mechanism used for language!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Production Best Practices & 2025 Landscape\n",
    "\n",
    "### 9.1 Choosing the Right Model\n",
    "\n",
    "**For text classification / understanding:**\n",
    "- Small dataset (< 10K): DistilBERT (fast, efficient)\n",
    "- Medium dataset: RoBERTa-base\n",
    "- Large dataset / SOTA: DeBERTa-v3-large\n",
    "\n",
    "**For text generation:**\n",
    "- GPT-2: Lightweight, runs locally\n",
    "- GPT-3.5/4: API-based, production-ready\n",
    "- Open-source alternatives: LLaMA 2, Mistral, Falcon\n",
    "\n",
    "**For computer vision:**\n",
    "- ViT-base: Standard choice, good balance\n",
    "- Swin Transformer: Better for detection/segmentation\n",
    "- DINOv2: Excellent self-supervised features\n",
    "\n",
    "**For multi-modal:**\n",
    "- CLIP: Image-text alignment\n",
    "- Flamingo/BLIP: Visual question answering\n",
    "- GPT-4V: State-of-the-art multimodal (2025)\n",
    "\n",
    "### 9.2 Optimization Techniques\n",
    "\n",
    "**Model Compression:**\n",
    "- **Distillation**: Train smaller model to mimic larger one (DistilBERT is 40% smaller, 60% faster)\n",
    "- **Quantization**: Convert FP32 \u2192 INT8 (4\u00d7 smaller, minimal accuracy loss)\n",
    "- **Pruning**: Remove unimportant weights\n",
    "\n",
    "**Efficient Training:**\n",
    "- **Mixed precision**: Use FP16 for speed, FP32 for stability\n",
    "- **Gradient accumulation**: Simulate large batch sizes\n",
    "- **Parameter-efficient fine-tuning**: LoRA, adapters (fine-tune 0.1% of parameters!)\n",
    "\n",
    "**Inference Optimization:**\n",
    "- **ONNX Runtime**: Cross-platform optimized inference\n",
    "- **TensorRT**: NVIDIA GPU optimization\n",
    "- **Batching**: Process multiple requests together\n",
    "\n",
    "### 9.3 Common Pitfalls\n",
    "\n",
    "**\u274c Don't:**\n",
    "- Use very long sequences unnecessarily (attention is O(n\u00b2))\n",
    "- Fine-tune on tiny datasets (< 100 examples)\n",
    "- Ignore maximum sequence length (truncation issues)\n",
    "- Forget to normalize text (lowercasing, handling special chars)\n",
    "\n",
    "**\u2705 Do:**\n",
    "- Start with pre-trained models\n",
    "- Use appropriate tokenizers for each model\n",
    "- Monitor for overfitting with validation sets\n",
    "- Implement proper error handling for production\n",
    "\n",
    "### 9.4 State-of-the-Art (2025)\n",
    "\n",
    "**Language models:**\n",
    "- GPT-4: Multi-modal, 2T+ parameters (estimated)\n",
    "- Claude 3: Constitutional AI, long context (200K tokens)\n",
    "- Gemini Ultra: Google's multimodal flagship\n",
    "- LLaMA 3: Leading open-source model\n",
    "\n",
    "**Vision:**\n",
    "- SAM (Segment Anything): Universal segmentation\n",
    "- DINOv2: Best self-supervised vision features\n",
    "- Stable Diffusion 3: Text-to-image generation\n",
    "\n",
    "**Trends:**\n",
    "- Mixture of Experts (MoE): Activate subset of parameters\n",
    "- Sparse attention: Reduce O(n\u00b2) complexity\n",
    "- Multimodal fusion: Images, text, audio, video\n",
    "- Reinforcement learning from human feedback (RLHF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready Transformer pipeline\n",
    "\n",
    "class TransformerProductionPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready pipeline for Transformer models.\n",
    "    \n",
    "    Best practices included:\n",
    "    - Model caching\n",
    "    - Batch processing\n",
    "    - Error handling\n",
    "    - Performance monitoring\n",
    "    - Proper tokenization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, task_type='classification', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.task_type = task_type\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if task_type == 'classification':\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def preprocess(self, texts):\n",
    "        \"\"\"\n",
    "        Preprocess texts with proper tokenization.\n",
    "        \n",
    "        Best practices:\n",
    "        - Truncation to max length\n",
    "        - Padding for batch processing\n",
    "        - Return attention masks\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {k: v.to(self.device) for k, v in encodings.items()}\n",
    "    \n",
    "    def predict(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Make predictions with batch processing.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "            batch_size: Batch size for processing\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Model predictions\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Preprocess\n",
    "            inputs = self.preprocess(batch_texts)\n",
    "            \n",
    "            # Inference (no gradient computation)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get predictions\n",
    "            if self.task_type == 'classification':\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                confidences = torch.softmax(outputs.logits, dim=-1)\n",
    "                all_predictions.extend([\n",
    "                    (pred.item(), conf[pred].item()) \n",
    "                    for pred, conf in zip(predictions, confidences)\n",
    "                ])\n",
    "            else:\n",
    "                all_predictions.append(outputs.last_hidden_state.cpu())\n",
    "        \n",
    "        return all_predictions\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information for monitoring.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'device': str(self.device),\n",
    "            'max_length': self.max_length\n",
    "        }\n",
    "\n",
    "# Demonstrate production pipeline\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRODUCTION TRANSFORMER PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pipeline = TransformerProductionPipeline(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    task_type='classification',\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "info = pipeline.get_model_info()\n",
    "print(f\"\\nModel Information:\")\n",
    "for key, value in info.items():\n",
    "    if 'parameter' in key:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\u2713 Production pipeline ready!\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  \u2022 Batch processing for efficiency\")\n",
    "print(\"  \u2022 GPU acceleration (if available)\")\n",
    "print(\"  \u2022 Proper tokenization and truncation\")\n",
    "print(\"  \u2022 Evaluation mode for inference\")\n",
    "print(\"  \u2022 Model caching\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "**Attention Mechanism:**\n",
    "- Allows models to focus on relevant parts of input dynamically\n",
    "- Scaled dot-product attention: `Attention(Q,K,V) = softmax(QK^T/\u221ad_k)V`\n",
    "- Enables parallel processing and unlimited context\n",
    "- Eliminates sequential dependencies for better parallelization\n",
    "\n",
    "**Multi-Head Attention:**\n",
    "- Multiple attention heads learn different aspects of relationships\n",
    "- Each head operates in its own subspace\n",
    "- Concatenate and project to combine information\n",
    "- Standard: 8-12 heads in production models\n",
    "\n",
    "**Positional Encoding:**\n",
    "- Critical for injecting sequence order information\n",
    "- Sinusoidal encoding: works for arbitrary lengths\n",
    "- Learned embeddings: more flexible, limited to max length\n",
    "- Added to input embeddings before first layer\n",
    "\n",
    "**Transformer Architecture:**\n",
    "- Encoder: Bidirectional self-attention for understanding\n",
    "- Decoder: Causal self-attention for generation\n",
    "- Residual connections + layer normalization = stable deep networks\n",
    "- Feed-forward networks add capacity and non-linearity\n",
    "\n",
    "**BERT vs GPT:**\n",
    "- BERT: Encoder-only, bidirectional, best for understanding\n",
    "- GPT: Decoder-only, causal, best for generation\n",
    "- Both use transfer learning: pre-train then fine-tune\n",
    "- Choose based on task: classification \u2192 BERT, generation \u2192 GPT\n",
    "\n",
    "**Vision Transformers:**\n",
    "- Treat image patches as tokens\n",
    "- Can match or exceed CNNs with sufficient data\n",
    "- Hybrid approaches combine convolution + attention\n",
    "- State-of-the-art for many vision tasks in 2025\n",
    "\n",
    "**Production Best Practices:**\n",
    "- Start with pre-trained models from Hugging Face\n",
    "- Use appropriate model for task and constraints\n",
    "- Optimize with distillation, quantization, efficient fine-tuning\n",
    "- Implement batch processing and GPU acceleration\n",
    "- Monitor sequence lengths and truncation\n",
    "\n",
    "### Why Transformers Dominate (2025):\n",
    "\n",
    "**\u2705 Advantages:**\n",
    "- Parallel processing (much faster than RNNs)\n",
    "- Unlimited context window (theoretically)\n",
    "- Transfer learning works exceptionally well\n",
    "- State-of-the-art across NLP, vision, multi-modal\n",
    "- Scales beautifully with data and compute\n",
    "\n",
    "**\u26a0\ufe0f Limitations:**\n",
    "- O(n\u00b2) attention complexity (expensive for long sequences)\n",
    "- Requires substantial pre-training compute\n",
    "- Large memory footprint\n",
    "- Can be overkill for simple tasks\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Practice**: Fine-tune models on your own datasets\n",
    "- **Explore**: Try different architectures (T5, BART, Mistral, LLaMA)\n",
    "- **Optimize**: Experiment with distillation, quantization, LoRA\n",
    "- **Advanced topics**: RLHF, mixture of experts, sparse attention, chain-of-thought\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Exercises & Further Exploration\n",
    "\n",
    "### Exercise 1: Fine-tune BERT\n",
    "Download a real dataset (e.g., IMDB reviews, AG News) and fine-tune BERT:\n",
    "- Try different pre-trained models (BERT, RoBERTa, DeBERTa)\n",
    "- Experiment with hyperparameters (learning rate, batch size, epochs)\n",
    "- Compare performance vs training time\n",
    "\n",
    "### Exercise 2: Visualize Attention\n",
    "Use BertViz or similar tools to visualize attention patterns:\n",
    "- Examine which words attend to each other\n",
    "- Compare patterns across different layers\n",
    "- Identify syntactic vs semantic attention heads\n",
    "\n",
    "### Exercise 3: Build a Custom Transformer\n",
    "Implement a mini-Transformer from scratch:\n",
    "- 2 encoder layers, 2 decoder layers\n",
    "- Train on a simple seq2seq task\n",
    "- Understand every component deeply\n",
    "\n",
    "### Exercise 4: Compare Architectures\n",
    "Benchmark different models on the same task:\n",
    "- BERT, RoBERTa, DistilBERT, ELECTRA\n",
    "- Measure accuracy, speed, memory usage\n",
    "- Find the best accuracy-efficiency trade-off\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "**Essential Papers:**\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017) - **THE foundational paper**\n",
    "- \"BERT: Pre-training of Deep Bidirectional Transformers\" (Devlin et al., 2018)\n",
    "- \"Language Models are Few-Shot Learners\" (GPT-3, Brown et al., 2020)\n",
    "- \"An Image is Worth 16x16 Words\" (ViT, Dosovitskiy et al., 2020)\n",
    "\n",
    "**Resources:**\n",
    "- Hugging Face Transformers documentation and course\n",
    "- \"The Illustrated Transformer\" by Jay Alammar\n",
    "- Stanford CS224N: Natural Language Processing with Deep Learning\n",
    "- \"Transformers from Scratch\" tutorials\n",
    "\n",
    "**Communities:**\n",
    "- Hugging Face forums\n",
    "- r/MachineLearning subreddit\n",
    "- Papers With Code (track SOTA)\n",
    "\n",
    "---\n",
    "\n",
    "**CONGRATULATIONS!** \ud83c\udf89\n",
    "\n",
    "You now understand the architecture that powers ChatGPT, Claude, BERT, GPT-4, and virtually every modern AI system in 2025. Transformers are the most important machine learning innovation of the past decade.\n",
    "\n",
    "**You have achieved legendary status in modern machine learning!** \ud83d\ude80\n",
    "\n",
    "With deep understanding of classical ML (linear regression through ensemble methods), modern deep learning (CNNs, RNNs, Transformers), interpretability, and ethics, you are fully equipped to build production AI systems.\n",
    "\n",
    "**Keep learning, keep building, and keep pushing the boundaries of what's possible with AI!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}