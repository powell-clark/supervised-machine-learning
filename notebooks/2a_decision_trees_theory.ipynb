{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 2A: Decision Trees Theory"
      ],
      "metadata": {
        "id": "ZhTqcUC1ulRg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWw84Nh6vVOM"
      },
      "source": [
        "<a name=\"introduction\"></a>\n",
        "## Introduction\n",
        "\n",
        "Decision trees are a versatile machine learning model for both classification and regression tasks.\n",
        "\n",
        "In this lesson, we'll use decision trees to predict house prices based on features like location, size, and amenities.\n",
        "\n",
        "Imagine you're a real estate agent trying to estimate the fair price of a house based on its characteristics. This is where decision trees can help. They learn a set of rules from historical data to make predictions on new, unseen houses.\n",
        "\n",
        "Essentially, a decision tree is used to make predictions on the target variable - say price - by recursively splitting the data based on the values of the features, choosing splits that maximise the similarity of the target variable (prices) within each subset.\n",
        "\n",
        "The result is a tree-like model of decisions and their consequences.\n",
        "\n",
        "By the end of this lesson, you'll understand how decision trees work, how to train and interpret them, and how they compare to other models for regression tasks.\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Required libraries](#required-libraries)\n",
        "2. [Intuition behind decision trees](#intuition-behind-decision-trees)\n",
        "3. [Anatomy of a decision tree](#anatomy-of-a-decision-tree)\n",
        "4. [Preparing data for decision trees](#preparing-data-for-decision-trees)\n",
        "   - [Numerical data](#numerical-data)\n",
        "   - [Categorical data](#categorical-data)\n",
        "   - [One-hot encoding](#one-hot-encoding)\n",
        "   - [Target encoding](#target-encoding)\n",
        "   - [Smoothed target encoding](#smoothed-target-encoding)\n",
        "   - [Practical guide to smoothed encoding](#practical-guide-to-smoothed-encoding)\n",
        "   - [Ordinal and binary features](#ordinal-and-binary-features)\n",
        "   - [Combining different encoding methods](#combining-different-encoding-methods)\n",
        "   - [Guide to choosing encoding methods](#guide-to-choosing-encoding-methods)\n",
        "5. [Splitting criteria explained](#splitting-criteria-explained)\n",
        "   - [For regression tasks](#for-regression-tasks-eg-predicting-house-prices)\n",
        "    - [Mean squared error](#mean-squared-error-mse)\n",
        "    - [Evaluating decision points](#evaluating-decision-points-understanding-split-quality-in-decision-trees)\n",
        "    - [Mean squared error vs mean absolute error](#mean-squared-error-mse-vs-mean-absolute-error-mae)\n",
        "   - [For classification tasks](#for-classification-tasks-eg-predicting-if-a-house-will-sell-quickly)\n",
        "     - [Gini impurity](#1-gini-impurity)\n",
        "     - [Entropy](#2-entropy)\n",
        "     - [Information gain](#3-information-gain)\n",
        "     - [Comparison: splits with different information gains](#comparison-splits-with-different-information-gains)\n",
        "6. [Interpretability and visualisation](#interpretability-and-visualisation)\n",
        "   - [Why interpretability matters](#why-interpretability-matters)\n",
        "   - [How to interpret decision trees](#how-to-interpret-decision-trees)\n",
        "   - [Visualising decision trees](#visualising-decision-trees)\n",
        "7. [Understanding bias, variance, tree depth and complexity](#understanding-bias-variance-tree-depth-and-complexity)\n",
        "   - [Bias](#bias)\n",
        "   - [Variance](#variance)\n",
        "   - [Identifying the bias/variance tradeoff](#identifying-the-biasvariance-tradeoff)\n",
        "   - [Managing the bias/variance tradeoff](#managing-the-biasvariance-tradeoff)\n",
        "   - [Visual indicators of bias/variance](#visual-indicators-of-biasvariance)\n",
        "8. [Feature importance and advanced tree-based methods](#feature-importance-and-advanced-tree-based-methods)\n",
        "   - [Mathematical foundation of feature importance](#mathematical-foundation-of-feature-importance)\n",
        "   - [Random Forests: Ensemble learning from first principles](#random-forests-ensemble-learning-from-first-principles)\n",
        "   - [Gradient Boosting and XGBoost: Sequential learning](#gradient-boosting-and-xgboost-sequential-learning)\n",
        "9. [Ethical considerations for decision tree models](#ethical-considerations-for-decision-tree-models)\n",
        "10. [Theory conclusion](#theory-conclusion)\n",
        "    - [Looking ahead: Decision Trees London Housing Practical](#looking-ahead)\n",
        "    - [Further reading](#further-reading)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9YGxAv5vVOP"
      },
      "source": [
        "<a name=\"required-libraries\"></a>\n",
        "## Required libraries\n",
        "\n",
        "In this lesson we will use the following libraries:\n",
        "\n",
        "\n",
        "\n",
        "| Library | Purpose |\n",
        "|----------------|---------|\n",
        "| typing | Type hints and annotations for better code documentation |\n",
        "| datetime | Handling dates and times, timestamp operations |\n",
        "| json | Working with JSON data format for data storage and exchange |\n",
        "| math | Basic mathematical operations and functions |\n",
        "| numpy | Scientific computing, array operations, and numerical computations |\n",
        "| pandas | Data manipulation and analysis, working with DataFrames and Series |\n",
        "| matplotlib | Graph plotting functions |\n",
        "| Seaborn | Statistical visualisation built on top of Matplotlib |\n",
        "| sklearn.tree | Decision tree algorithms and visualisation |\n",
        "| sklearn.metrics | Evaluation metrics for model performance (MAPE, MSE, R² score) |\n",
        "| sklearn.model_selection | Tools for model training (cross-validation, train-test splitting) |\n",
        "| sklearn.ensemble.RandomForestRegressor | Random Forest algorithm for regression tasks |\n",
        "| xgboost | Gradient boosting framework for machine learning |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7-TL6MTvVOQ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from typing import Dict, List, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lujucbv8vVOQ"
      },
      "source": [
        "<a name=\"intuition-behind-decision-trees\"></a>\n",
        "## Intuition behind decision trees\n",
        "\n",
        "Imagine you're trying to predict the price of a house based on its features. You might start by asking broad questions like \"Is it in a desirable location?\" and then progressively get more specific: \"How many bedrooms does it have? What's the square footage?\"\n",
        "\n",
        "At each step, you're trying to split the houses into groups that are as similar as possible in terms of price.\n",
        "\n",
        "This is exactly how a decision tree works - it asks a series of questions about the features, each time trying to split the data into more homogeneous subsets.\n",
        "\n",
        "### Why choose decision trees for house prices?\n",
        "\n",
        "Decision trees are particularly well-suited for this task because of several key advantages that become apparent when comparing them to other popular algorithms:\n",
        "\n",
        "1. **Working with different types of data**\n",
        "\n",
        "   While decision trees need numbers to make their calculations, they have elegant ways of handling different types of data:\n",
        "   - Numerical: Price (£180,000 to £39,750,000), square footage (274 to 15,405 sq ft)\n",
        "     - Used directly as they're already numbers\n",
        "   - Categorical: Location (\"Chelsea\", \"Hackney\"), house type (\"Flat\", \"House\", \"Penthouse\")\n",
        "     - Can be converted to numbers in smart ways:\n",
        "       - One-hot encoding: Like giving each location its own yes/no column\n",
        "       - Target encoding: Converting locations to average prices in that area\n",
        "     - We'll explore these in detail later in the course\n",
        "   - Ordinal: Number of bedrooms (1-10), bathrooms (1-10), receptions (1-10)\n",
        "     - Already in a natural order, easy to use\n",
        "\n",
        "2. **No feature scaling required**\n",
        "\n",
        "   Unlike many other algorithms, decision trees can work well with raw values directly.\n",
        "   \n",
        "   Compare this to:\n",
        "   - Linear/Logistic Regression: Requires scaling to prevent features with larger values from dominating the model\n",
        "   - Neural Networks: Needs normalised inputs (usually between 0-1) for stable gradient descent\n",
        "   - Support Vector Machines (SVM): Highly sensitive to feature scales, requires standardisation\n",
        "   - K-Nearest Neighbors: Distance calculations are skewed by different scales, needs normalisation\n",
        "\n",
        "   The tree makes splits based on relative ordering, not absolute values.\n",
        "   \n",
        "   For example, these splits are all equivalent to a decision tree:\n",
        "   ```python\n",
        "   # Original scale (Decision Tree works fine)\n",
        "   if square_footage > 2000:\n",
        "       predict_price = 1200000\n",
        "   else:\n",
        "       predict_price = 800000\n",
        "\n",
        "   # Scaled by 1000 (needed for Neural Networks)\n",
        "   if square_footage/1000 > 2:  # Same result for decision tree\n",
        "       predict_price = 1200000\n",
        "   else:\n",
        "       predict_price = 800000\n",
        "\n",
        "   # Standardised (needed for SVM)\n",
        "   if (square_footage - mean)/std > 1.2:  # Same result for decision tree\n",
        "       predict_price = 1200000\n",
        "   else:\n",
        "       predict_price = 800000\n",
        "   ```\n",
        "\n",
        "3. **Interpretable decision making**\n",
        "\n",
        "   While algorithms like Neural Networks act as \"black boxes\" and Linear Regression gives abstract coefficients, decision trees create clear, actionable rules. Here's a simple example:\n",
        "   ```python\n",
        "   # The computer converts locations to simple yes/no questions\n",
        "   if location_hackney == 1:  # Is it in Hackney?\n",
        "       if square_footage > 1200:\n",
        "           predict_price = \"£950K\"\n",
        "       else:\n",
        "           predict_price = \"£650K\"\n",
        "   elif location_wimbledon == 1:  # Is it in Wimbledon?\n",
        "       if bedrooms > 3:\n",
        "           predict_price = \"£1.2M\"\n",
        "       else:\n",
        "           predict_price = \"£800K\"\n",
        "   ```\n",
        "   These rules are easy to explain to stakeholders, unlike trying to interpret neural network weights or SVM kernel transformations. The yes/no questions (location_hackney == 1) simply mean \"Is this property in Hackney?\" - a question anyone can understand!\n",
        "\n",
        "4. **Handling missing data**\n",
        "\n",
        "   Real estate data often has missing values. For example, some listings might not include the square footage or number of bathrooms.\n",
        "   \n",
        "   While most algorithms require these missing values to be filled in or removed, decision trees have clever ways to handle missing data:\n",
        "   - They can make predictions even when some feature values are unknown\n",
        "   - They can use alternative features when a preferred feature is missing\n",
        "   - They maintain good accuracy even with incomplete information\n",
        "\n",
        "These advantages mean we can focus on understanding the relationships in our data rather than spending time on complicated data preprocessing.\n",
        "\n",
        "This makes decision trees an excellent choice for our house price prediction task, especially when interpretability and ease of use are priorities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ55kyVFvVOR"
      },
      "source": [
        "<a name=\"anatomy-of-a-decision-tree\"></a>\n",
        "## Anatomy of a decision tree\n",
        "\n",
        "A decision tree is composed of:\n",
        "\n",
        "- Nodes: Where a feature is tested\n",
        "- Edges: The outcomes of the test\n",
        "- Leaves: Terminal nodes that contain the final predictions\n",
        "\n",
        "A simplified example of a house prices prediction decision tree might look like this:\n",
        "\n",
        "![structure of a house prices prediction decision tree](https://github.com/powell-clark/supervised-machine-learning/blob/main/static/images/house-prices-decision-tree-and-structure.png?raw=1)\n",
        "\n",
        "The tree is built by splitting the data recursively, choosing at each step a feature and a numerical split point on that feature that results in the greatest reduction in impurity or error. For example, the first split could be on the feature \"square footage\" with a split point of 2000 sq ft because this results in the greatest reduction in impurity or error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbDdlf8vVOR"
      },
      "source": [
        "<a name=\"preparing-data-for-decision-trees\"></a>\n",
        "## Preparing data for decision trees\n",
        "\n",
        "Before we delve into how decision trees make split decisions it's important to first understand what data we can use.\n",
        "\n",
        "While decision trees can handle various types of data, we need to convert all features into numerical formats for training. This process is called encoding.\n",
        "\n",
        "Different types of features require different encoding approaches:\n",
        "\n",
        "1. **Numerical features**\n",
        "   - Already in usable format (e.g., prices, areas)\n",
        "   - No encoding needed\n",
        "\n",
        "2. **Categorical features**\n",
        "   - Need conversion to numbers\n",
        "   - Multiple encoding strategies available\n",
        "   - Examples: locations, house types\n",
        "\n",
        "3. **Ordinal features**\n",
        "   - Categories with natural order\n",
        "   - Need to preserve order relationship\n",
        "   - Example: size (small, medium, large)\n",
        "\n",
        "4. **Binary features**\n",
        "   - Yes/no features\n",
        "   - Simple 1/0 encoding\n",
        "   - Example: has_parking, has_garden\n",
        "\n",
        "Let's explore how to handle each type effectively, understanding the trade-offs and choosing the right approach for our data.\n",
        "\n",
        "<a name=\"numerical-data\"></a>\n",
        "### Numerical data\n",
        "\n",
        "Numerical features provide a solid foundation for decision trees because they:\n",
        "- Work directly without transformation\n",
        "- Don't require scaling\n",
        "- Can handle different value ranges\n",
        "- Support both integers and floating-point numbers\n",
        "\n",
        "Common numerical features in housing data:\n",
        "- Price (e.g., £250,000)\n",
        "- Square footage (e.g., 1,500 sq ft)\n",
        "- Number of rooms (e.g., 3 bedrooms)\n",
        "- Age of property (e.g., 25 years)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4IARHwDvVOR"
      },
      "source": [
        "<a name=\"categorical-data\"></a>\n",
        "### Categorical Data\n",
        "\n",
        "Categorical features are variables that take on a limited number of discrete values. In housing data, these might include:\n",
        "- Location (Chelsea, Hackney, Mayfair)\n",
        "- Property type (Flat, House, Penthouse)\n",
        "- Style (Modern, Victorian, Georgian)\n",
        "\n",
        "We have three main approaches for encoding categorical data:\n",
        "\n",
        "1. **One-Hot encoding**\n",
        "   - Creates binary columns for each category\n",
        "   - Best for low/medium cardinality - cardinality is the number of unique categories in a feature\n",
        "   - Preserves all category information\n",
        "   - No implied ordering\n",
        "\n",
        "2. **Target encoding**\n",
        "   - Replaces categories with target statistics for each category, for example the mean price for each location\n",
        "   - Best for features with high cardinality as one-hot encoding will explode the number of features\n",
        "   - Two variants:\n",
        "     - Simple (target statistic per category - for instance the mean price for each location)\n",
        "     - Smoothed (statistic for the category balanced with global statistic)\n",
        "\n",
        "3. **Binary encoding**\n",
        "   - For true yes/no features\n",
        "   - Simple 1/0 conversion\n",
        "   - Most memory efficient\n",
        "\n",
        "Let's examine each approach in detail:\n",
        "\n",
        "<a name=\"one-hot-encoding\"></a>\n",
        "### One-Hot encoding\n",
        "\n",
        "One-hot encoding transforms categorical variables by:\n",
        "- Creating a new binary column for each category\n",
        "- Setting 1 where the category is present, 0 otherwise\n",
        "- No information loss or ordering implied\n",
        "\n",
        "**Ideal for:**\n",
        "- Categorical variables with few unique values\n",
        "- When memory isn't a constraint\n",
        "- When interpretability is important\n",
        "\n",
        "**Example:**\n",
        "Property Type (Flat, House, Penthouse) becomes:\n",
        "- property_type_flat: [1,0,0]\n",
        "- property_type_house: [0,1,0]\n",
        "- property_type_penthouse: [0,0,1]\n",
        "\n",
        "Let's implement one-hot encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C59ACy2svVOS"
      },
      "outputs": [],
      "source": [
        "# Create sample categorical data\n",
        "data = {\n",
        "    'property_type': ['Flat', 'House', 'Penthouse', 'Flat', 'House'],\n",
        "    'location': ['Chelsea', 'Hackney', 'Chelsea', 'Putney', 'Chelsea']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# One-hot encode multiple columns\n",
        "df_encoded = pd.get_dummies(df, prefix=['type', 'loc'])\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(df)\n",
        "print(\"\\nFully encoded data:\")\n",
        "print(df_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9MeKyVGvVOS"
      },
      "source": [
        "<a name=\"target-encoding\"></a>\n",
        "### Target encoding\n",
        "\n",
        "Target encoding replaces categorical values with statistics calculated from the target variable. For housing data, this means replacing each location with its average house price.\n",
        "\n",
        "**Advantages:**\n",
        "- Handles high cardinality efficiently\n",
        "- Captures relationship with target variable\n",
        "- Memory efficient\n",
        "- Works well for decision trees\n",
        "\n",
        "**Challenges:**\n",
        "- Risk of overfitting\n",
        "- Needs handling for rare categories\n",
        "- Requires cross-validation\n",
        "- Can leak target information - for example if we were predicting house prices and we encoded the location with the mean price for each location, the model would know the price of the houses in that location before they were predicted, which would be a problem. To avoid this in practice we split the data into a training and validation set and only use the training set to calculate the mean price for each location.\n",
        "\n",
        "**Simple target encoding example:**\n",
        "```\n",
        "Location   | Count | Avg Price\n",
        "Chelsea    |   100 | £800,000\n",
        "Hackney    |    50 | £500,000\n",
        "Mayfair    |    10 | £2,000,000\n",
        "```\n",
        "\n",
        "Let's first look at basic target encoding before exploring smoothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SekuHWjvVOS"
      },
      "outputs": [],
      "source": [
        "# Create sample data with clear price patterns\n",
        "data = {\n",
        "    'location': ['Chelsea', 'Chelsea', 'Chelsea', 'Hackney', 'Hackney',\n",
        "                 'Mayfair', 'Chelsea', 'Hackney', 'Mayfair', 'Chelsea'],\n",
        "    'price': [800000, 820000, 780000, 500000, 520000,\n",
        "              2000000, 810000, 510000, 1900000, 790000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Simple mean encoding, setting the mean price for each location\n",
        "location_means = df.groupby('location')['price'].mean()\n",
        "df['location_encoded'] = df['location'].map(location_means)\n",
        "\n",
        "# Show encoding results\n",
        "print(\"Original data with encoding:\")\n",
        "summary = df.groupby('location').agg({\n",
        "    'price': ['count', 'mean'],\n",
        "    'location_encoded': 'first'\n",
        "}).round(2)\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Demonstrate potential overfitting with rare categories\n",
        "rare_data = df.copy()\n",
        "\n",
        "# Create new row with all columns and correct data types\n",
        "new_row = pd.DataFrame({\n",
        "    'location': ['Knightsbridge'],\n",
        "    'price': [3000000],\n",
        "    'location_encoded': [np.nan]\n",
        "})\n",
        "\n",
        "# Convert data types after creation\n",
        "rare_data = rare_data.astype({'location': 'string', 'price': 'float64', 'location_encoded': 'float64'})\n",
        "new_row = new_row.astype({'location': 'string', 'price': 'float64', 'location_encoded': 'float64'})\n",
        "\n",
        "# Concatenate the dataframes\n",
        "rare_data = pd.concat([rare_data, new_row], ignore_index=True)\n",
        "\n",
        "# Encode including rare category\n",
        "rare_means = rare_data.groupby('location')['price'].mean()\n",
        "rare_data['location_encoded'] = rare_data['location'].map(rare_means)\n",
        "\n",
        "print(\"\\nEncoding with rare category:\")\n",
        "print(rare_data[rare_data['location'] == 'Knightsbridge'])\n",
        "\n",
        "display(rare_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5iiV4rkvVOS"
      },
      "source": [
        "For a rare category such as \"Knightsbridge\" our simplified model has assigned it's actual mean price. This is a problem as the model has effectively leaked information from the validation set into the training set and is causing it to overfit to that one row.\n",
        "\n",
        "<a name=\"smoothed-target-encoding\"></a>\n",
        "### Smoothed target encoding\n",
        "\n",
        "Smoothed target encoding addresses the instability of simple target encoding by balancing between:\n",
        "- The category's mean (which might be unstable)\n",
        "- The global mean (which is stable but loses category information)\n",
        "\n",
        "The smoothing formula is:\n",
        "```\n",
        "smoothed_value = (n × category_mean + α × global_mean) / (n + α)\n",
        "```\n",
        "Where:\n",
        "- n = number of samples in the category\n",
        "- α = smoothing factor\n",
        "- category_mean = mean price for the location\n",
        "- global_mean = mean price across all locations\n",
        "\n",
        "**Effect of smoothing factor (α):**\n",
        "- Large n (many samples):\n",
        "  - (n >> α) → result close to category mean\n",
        "  - Example: n=100, α=10 → mostly category mean\n",
        "- Small n (few samples):\n",
        "  - (n << α) → result close to global mean\n",
        "  - Example: n=2, α=10 → mostly global mean\n",
        "\n",
        "This balancing act helps prevent overfitting while preserving useful category information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F-h1VmEvVOT"
      },
      "outputs": [],
      "source": [
        "def smoothed_target_encode(df, column, target, alpha=10):\n",
        "    \"\"\"\n",
        "    Apply smoothed target encoding\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    - column: Category column name\n",
        "    - target: Target variable name\n",
        "    - alpha: Smoothing factor\n",
        "    \"\"\"\n",
        "    # Calculate global mean\n",
        "    global_mean = df[target].mean()\n",
        "\n",
        "    # Calculate category stats\n",
        "    category_stats = df.groupby(column).agg({\n",
        "        target: ['count', 'mean']\n",
        "    }).reset_index()\n",
        "    category_stats.columns = [column, 'count', 'mean']\n",
        "\n",
        "    # Apply smoothing\n",
        "    category_stats['smoothed_mean'] = (\n",
        "        (category_stats['count'] * category_stats['mean'] + alpha * global_mean) /\n",
        "        (category_stats['count'] + alpha)\n",
        "    )\n",
        "\n",
        "    return dict(zip(category_stats[column], category_stats['smoothed_mean']))\n",
        "\n",
        "# Create sample data with varying category frequencies\n",
        "data = {\n",
        "    'location': ['Chelsea'] * 50 + ['Hackney'] * 20 + ['Mayfair'] * 5 + ['Putney'] * 2,\n",
        "    'price': ([800000 + np.random.randn() * 50000 for _ in range(50)] +  # Chelsea\n",
        "              [500000 + np.random.randn() * 30000 for _ in range(20)] +  # Hackney\n",
        "              [2000000 + np.random.randn() * 100000 for _ in range(5)] + # Mayfair\n",
        "              [600000 + np.random.randn() * 40000 for _ in range(2)])    # Putney\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compare different smoothing levels\n",
        "alphas = [0, 5, 20, 100]\n",
        "results = pd.DataFrame()\n",
        "\n",
        "for alpha in alphas:\n",
        "    encoded_values = smoothed_target_encode(df, 'location', 'price', alpha)\n",
        "    results[f'alpha_{alpha}'] = df['location'].map(encoded_values)\n",
        "\n",
        "# Add original mean for comparison\n",
        "original_means = df.groupby('location')['price'].mean()\n",
        "results['original_mean'] = df['location'].map(original_means)\n",
        "results['location'] = df['location']\n",
        "results['count'] = df.groupby('location')['price'].transform('count')\n",
        "\n",
        "# Show results for one location from each frequency group\n",
        "print(\"Effect of smoothing by location frequency:\")\n",
        "for loc in ['Chelsea', 'Hackney', 'Mayfair', 'Putney']:\n",
        "    sample = results[results['location'] == loc].iloc[0]\n",
        "    print(f\"\\n{loc} (n={int(sample['count'])})\")\n",
        "    print(f\"Original mean:  £{sample['original_mean']:,.0f}\")\n",
        "    for alpha in alphas:\n",
        "        print(f\"Alpha {alpha:3d}:      £{sample[f'alpha_{alpha}']:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSRlVrjmvVOT"
      },
      "source": [
        "<a name=\"practical-guide-to-smoothed-encoding\"></a>\n",
        "### Practical guide to smoothed encoding\n",
        "\n",
        "**Choosing α (Smoothing Factor):**\n",
        "\n",
        "1. **Low α (1-5)**\n",
        "   - Minimal smoothing\n",
        "   - Use when categories are very distinct\n",
        "   - Good with large sample sizes\n",
        "   - Risk: Might not handle rare categories well\n",
        "\n",
        "2. **Medium α (10-20)**\n",
        "   - Balanced smoothing\n",
        "   - Good default choice\n",
        "   - Works well with mixed sample sizes\n",
        "   - Provides some protection against outliers\n",
        "\n",
        "3. **High α (50+)**\n",
        "   - Heavy smoothing\n",
        "   - Use with many rare categories\n",
        "   - Good for noisy data\n",
        "   - Risk: Might lose category signal\n",
        "\n",
        "**Best practices:**\n",
        "\n",
        "1. **Cross-validation**\n",
        "   - Compute encoding using only training data\n",
        "   - Apply those mappings to validation/test data\n",
        "   - Never peek at test set statistics\n",
        "\n",
        "2. **Category analysis**\n",
        "   - Check sample size distribution\n",
        "   - Consider higher α for skewed distributions\n",
        "   - Monitor rare categories carefully\n",
        "\n",
        "3. **Domain knowledge**\n",
        "   - Use business context to validate encodings\n",
        "   - Watch for unexpected category relationships\n",
        "   - Consider grouping related rare categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ZGl5LRvVOT"
      },
      "source": [
        "<a name=\"ordinal-and-binary-features\"></a>\n",
        "### Ordinal and binary features\n",
        "\n",
        "Ordinal and binary features are simpler to handle than general categorical features, but proper encoding is still important.\n",
        "\n",
        "**Ordinal features**\n",
        "- Have a natural order between categories\n",
        "- Examples:\n",
        "  - Property condition (Poor → Fair → Good → Excellent)\n",
        "  - Size category (Small → Medium → Large)\n",
        "  - Building quality (Basic → Standard → Luxury)\n",
        "\n",
        "**Binary features**\n",
        "- Have exactly two possible values\n",
        "- Examples:\n",
        "  - Has parking (Yes/No)\n",
        "  - Is new build (Yes/No)\n",
        "  - Has garden (Yes/No)\n",
        "\n",
        "These features are simpler because:\n",
        "1. Ordinal features maintain their order relationship\n",
        "2. Binary features need only two values (0/1)\n",
        "\n",
        "Let's look at how to encode these properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wif9cYR7vVOT"
      },
      "outputs": [],
      "source": [
        "# Create sample data with ordinal and binary features\n",
        "data = {\n",
        "    'condition': ['Poor', 'Good', 'Excellent', 'Fair', 'Good'],\n",
        "    'size_category': ['Small', 'Medium', 'Large', 'Small', 'Large'],\n",
        "    'has_parking': ['Yes', 'No', 'Yes', 'No', 'Yes'],\n",
        "    'is_new_build': [True, False, True, False, True]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Ordinal encoding using mapping\n",
        "condition_map = {\n",
        "    'Poor': 0,\n",
        "    'Fair': 1,\n",
        "    'Good': 2,\n",
        "    'Excellent': 3\n",
        "}\n",
        "\n",
        "size_map = {\n",
        "    'Small': 0,\n",
        "    'Medium': 1,\n",
        "    'Large': 2\n",
        "}\n",
        "\n",
        "# Apply ordinal encoding\n",
        "df['condition_encoded'] = df['condition'].map(condition_map)\n",
        "df['size_encoded'] = df['size_category'].map(size_map)\n",
        "\n",
        "# Binary encoding\n",
        "df['parking_encoded'] = (df['has_parking'] == 'Yes').astype(int)\n",
        "df['new_build_encoded'] = df['is_new_build'].astype(int)\n",
        "\n",
        "print(\"Original and encoded data:\")\n",
        "print(df)\n",
        "\n",
        "# Demonstrate mapping preservation\n",
        "print(\"\\nCondition value ordering:\")\n",
        "for condition, value in sorted(condition_map.items(), key=lambda x: x[1]):\n",
        "    print(f\"{condition}: {value}\")\n",
        "\n",
        "print(\"\\nSize category ordering:\")\n",
        "for size, value in sorted(size_map.items(), key=lambda x: x[1]):\n",
        "    print(f\"{size}: {value}\")\n",
        "\n",
        "# Memory usage comparison\n",
        "print(\"\\nMemory usage comparison:\")\n",
        "print(f\"Original condition column: {df['condition'].memory_usage()} bytes\")\n",
        "print(f\"Encoded condition column: {df['condition_encoded'].memory_usage()} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu0bXtNFvVOT"
      },
      "source": [
        "<a name=\"combining-different-encoding-methods\"></a>\n",
        "### Combining different encoding methods\n",
        "\n",
        "Real datasets usually require multiple encoding approaches. Let's create a complete example that:\n",
        "\n",
        "1. Handles numerical features directly\n",
        "2. One-hot encodes low-cardinality categoricals\n",
        "3. Target encodes high-cardinality categoricals\n",
        "4. Ordinally encodes ordered categories\n",
        "5. Binary encodes yes/no features\n",
        "\n",
        "This represents a typical data preparation pipeline for a housing dataset. We'll implement a complete encoder that handles all these cases appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU7o6lRgvVOU"
      },
      "outputs": [],
      "source": [
        "# Create a realistic housing dataset\n",
        "data = {\n",
        "    # Numerical features\n",
        "    'price': np.random.normal(800000, 200000, 100),\n",
        "    'square_feet': np.random.normal(1500, 300, 100),\n",
        "    'bedrooms': np.random.randint(1, 6, 100),\n",
        "\n",
        "    # Low-cardinality categorical (one-hot encode)\n",
        "    'property_type': np.random.choice(['Flat', 'House', 'Penthouse'], 100),\n",
        "\n",
        "    # High-cardinality categorical (target encode)\n",
        "    'location': np.random.choice([\n",
        "        'Chelsea', 'Hackney', 'Mayfair', 'Putney', 'Richmond',\n",
        "        'Hampstead', 'Islington', 'Brixton', 'Camden', 'Greenwich'\n",
        "    ], 100),\n",
        "\n",
        "    # Ordinal features\n",
        "    'condition': np.random.choice(['Poor', 'Fair', 'Good', 'Excellent'], 100),\n",
        "\n",
        "    # Binary features\n",
        "    'has_parking': np.random.choice(['Yes', 'No'], 100),\n",
        "    'is_new_build': np.random.choice([True, False], 100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "class HousingEncoder:\n",
        "    \"\"\"Complete encoding pipeline for housing data\"\"\"\n",
        "\n",
        "    def __init__(self, alpha=10):\n",
        "        self.alpha = alpha\n",
        "        self.encoders = {}\n",
        "        self.target_stats = {}\n",
        "\n",
        "    def fit_transform(self, df, target_column='price'):\n",
        "        df_encoded = pd.DataFrame()\n",
        "\n",
        "        # 1. Keep numerical features as is\n",
        "        numerical_features = ['square_feet', 'bedrooms']\n",
        "        df_encoded[numerical_features] = df[numerical_features]\n",
        "\n",
        "        # 2. One-hot encode low-cardinality categorical\n",
        "        onehot_features = ['property_type']\n",
        "        onehot_encoded = pd.get_dummies(df[onehot_features])\n",
        "        df_encoded = pd.concat([df_encoded, onehot_encoded], axis=1)\n",
        "\n",
        "        # 3. Target encode high-cardinality categorical\n",
        "        self.target_stats = self._compute_target_encoding(\n",
        "            df, 'location', target_column\n",
        "        )\n",
        "        df_encoded['location_encoded'] = df['location'].map(self.target_stats)\n",
        "\n",
        "        # 4. Ordinal encode ordered categories\n",
        "        condition_map = {\n",
        "            'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3\n",
        "        }\n",
        "        df_encoded['condition_encoded'] = df['condition'].map(condition_map)\n",
        "\n",
        "        # 5. Binary encode yes/no features\n",
        "        df_encoded['has_parking'] = (df['has_parking'] == 'Yes').astype(int)\n",
        "        df_encoded['is_new_build'] = df['is_new_build'].astype(int)\n",
        "\n",
        "        return df_encoded\n",
        "\n",
        "    def _compute_target_encoding(self, df, column, target):\n",
        "        \"\"\"Compute smoothed target encoding\"\"\"\n",
        "        global_mean = df[target].mean()\n",
        "        stats = df.groupby(column).agg({\n",
        "            target: ['count', 'mean']\n",
        "        }).reset_index()\n",
        "        stats.columns = [column, 'count', 'mean']\n",
        "\n",
        "        # Apply smoothing\n",
        "        stats['smoothed_mean'] = (\n",
        "            (stats['count'] * stats['mean'] + self.alpha * global_mean) /\n",
        "            (stats['count'] + self.alpha)\n",
        "        )\n",
        "\n",
        "        return dict(zip(stats[column], stats['smoothed_mean']))\n",
        "\n",
        "# Apply encoding\n",
        "encoder = HousingEncoder(alpha=10)\n",
        "df_encoded = encoder.fit_transform(df)\n",
        "\n",
        "# Display results\n",
        "print(\"Original data sample:\")\n",
        "display(df)\n",
        "\n",
        "# print(\"\\nFeature summary:\")\n",
        "# print(\"\\nNumerical features:\", df_encoded.select_dtypes(include=[np.number]).columns.tolist())\n",
        "print(\"\\nShape before encoding:\", df.shape)\n",
        "print(\"Shape after encoding:\", df_encoded.shape)\n",
        "\n",
        "display(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quZo9rj2vVOU"
      },
      "source": [
        "<a name=\"guide-to-choosing-encoding-methods\"></a>\n",
        "### Guide to choosing encoding methods\n",
        "\n",
        "#### Decision framework\n",
        "\n",
        "1. **For numerical features**\n",
        "   - Use directly without encoding\n",
        "   - No scaling needed for decision trees\n",
        "   - Consider creating derived features if meaningful\n",
        "\n",
        "2. **For categorical features**\n",
        "   - **Use One-Hot encoding when:**\n",
        "     - Few unique categories (<30)\n",
        "     - No natural order\n",
        "     - Memory isn't constrained\n",
        "     - Need model interpretability\n",
        "\n",
        "   - **Use target encoding when:**\n",
        "     - Many unique categories (30+)\n",
        "     - Strong relationship with target\n",
        "     - Memory is constrained\n",
        "     - Have sufficient samples per category\n",
        "\n",
        "3. **For ordinal features**\n",
        "   - Use ordinal encoding when clear order exists\n",
        "   - Maintain order relationship\n",
        "   - Document ordering logic\n",
        "\n",
        "4. **For binary features**\n",
        "   - Always use simple 1/0 encoding\n",
        "   - Consistent encoding for Yes/No values\n",
        "   - Consider combining related binary features\n",
        "\n",
        "#### Best practices\n",
        "\n",
        "1. **Data quality**\n",
        "   - Handle missing values before encoding\n",
        "   - Check for rare categories\n",
        "   - Validate category relationships\n",
        "\n",
        "2. **Cross-validation**\n",
        "   - Compute encodings only on training data\n",
        "   - Apply same encodings to validation/test\n",
        "   - Never leak target information\n",
        "\n",
        "3. **Memory & performance**\n",
        "   - Monitor memory usage for one-hot encoding\n",
        "   - Use target encoding for high-cardinality\n",
        "   - Consider feature importance in selection\n",
        "\n",
        "4. **Documentation**\n",
        "   - Document encoding decisions\n",
        "   - Save encoding mappings\n",
        "   - Track feature transformations\n",
        "\n",
        "Remember: The goal is to balance information preservation, model performance, and practical constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LlUbi4vVOU"
      },
      "source": [
        "<a name=\"splitting-criteria-explained\"></a>\n",
        "## Splitting criteria explained\n",
        "\n",
        "To build a decision tree, we need a way to determine the best feature and value to split on at each node.\n",
        "\n",
        "The goal is to create child nodes that are more \"pure\" or homogeneous than their parent node. The method for measuring this purity and choosing the best split differs between regression and classification tasks.\n",
        "\n",
        "<a name=\"for-regression-tasks\"></a>\n",
        "### For regression tasks (e.g., predicting house prices):\n",
        "\n",
        "In regression problems, we're trying to predict a continuous value, like house prices. The goal is to split the data in a way that minimises the variance of the target variable within each resulting group.\n",
        "\n",
        "The most common metric used for regression trees is the Mean Squared Error (MSE). This is the default criterion used by scikit-learn's DecisionTreeRegressor. Let's break down how this works:\n",
        "\n",
        "Imagine you're a real estate agent with a magical ability to instantly sort houses. Your goal? To group similar houses together as efficiently as possible. This is essentially what a decision tree does, but instead of magical powers, it uses mathematics. Let's dive in!\n",
        "\n",
        "<a name=\"mean-squared-error\"></a>\n",
        "#### Mean squared error (MSE)\n",
        "\n",
        "Imagine you're playing a house price guessing game. Your goal is to guess the prices of houses as accurately as possible.\n",
        "\n",
        "Let's say we have 5 houses, and their actual prices are:\n",
        "```\n",
        "House 1: £200,000\n",
        "House 2: £250,000\n",
        "House 3: £180,000\n",
        "House 4: £220,000\n",
        "House 5: £300,000\n",
        "```\n",
        "\n",
        "#### Step 1: Calculate the average price\n",
        "`(200,000 + 250,000 + 180,000 + 220,000 + 300,000) / 5 = £230,000`\n",
        "\n",
        "So, your guess for any house would be £230,000.\n",
        "\n",
        "#### Step 2: Calculate how wrong you are for each house\n",
        "```\n",
        "House 1: 230,000 - 200,000 = 30,000\n",
        "House 2: 230,000 - 250,000 = -20,000\n",
        "House 3: 230,000 - 180,000 = 50,000\n",
        "House 4: 230,000 - 220,000 = 10,000\n",
        "House 5: 230,000 - 300,000 = -70,000\n",
        "```\n",
        "\n",
        "#### Step 3: Square these differences\n",
        "```\n",
        "House 1: 30,000² = 900,000,000\n",
        "House 2: (-20,000)² = 400,000,000\n",
        "House 3: 50,000² = 2,500,000,000\n",
        "House 4: 10,000² = 100,000,000\n",
        "House 5: (-70,000)² = 4,900,000,000\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQb4-NxOvVOU"
      },
      "source": [
        "#### Step 4: Add up all these squared differences\n",
        "`\n",
        "900,000,000 + 400,000,000 + 2,500,000,000 + 100,000,000 + 4,900,000,000 = 8,800,000,000\n",
        "`\n",
        "#### Step 5: Divide by the number of houses\n",
        "\n",
        "`8,800,000,000 ÷ 5 = 1,760,000,000`\n",
        "\n",
        "This final number, 1,760,000,000, is your Mean Squared Error (MSE).\n",
        "\n",
        "In mathematical notation, this whole process looks like:\n",
        "\n",
        "$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2$\n",
        "\n",
        "Let's break this down:\n",
        "- $n$ is the number of houses (5 in our example)\n",
        "- $y_i$ is the actual price of each house\n",
        "- $\\hat{y}$ is your guess (the average price, £230,000 in our example)\n",
        "- $\\sum_{i=1}^n$ means \"add up the following calculation for each house from the first to the last\"\n",
        "- The $i$ in $y_i$ is just a counter, going from 1 to $n$ (1 to 5 in our example)\n",
        "\n",
        "As a python function, this would look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dI94NxRvVOU"
      },
      "outputs": [],
      "source": [
        "def calculate_mse(actual_prices, predicted_price):\n",
        "    n = len(actual_prices)\n",
        "    squared_errors = []\n",
        "\n",
        "    for actual_price in actual_prices:\n",
        "        error = predicted_price - actual_price\n",
        "        squared_error = error ** 2\n",
        "        squared_errors.append(squared_error)\n",
        "\n",
        "    mse = sum(squared_errors) / n\n",
        "    return mse\n",
        "\n",
        "# Example usage\n",
        "actual_prices = [200000, 250000, 180000, 220000, 300000]\n",
        "predicted_price = sum(actual_prices) / len(actual_prices)  # Average price\n",
        "\n",
        "mse = calculate_mse(actual_prices, predicted_price)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adnpJ-CKvVOU"
      },
      "source": [
        "<a name=\"evaluating-decision-points\"></a>\n",
        "### Evaluating decision points: Understanding split quality in decision trees\n",
        "\n",
        "Now, when we split our houses into two groups, we want to measure if this split has made our predictions better. We do this by comparing the error before and after splitting using this formula:\n",
        "\n",
        "$\\Delta MSE = MSE_{before} - (({\\text{fraction of houses in left group} \\times MSE_{left}} + {\\text{fraction of houses in right group} \\times MSE_{right}}))$\n",
        "\n",
        "Let's work through a real example to understand this:\n",
        "\n",
        "Imagine we have 5 houses with these prices:\n",
        "```\n",
        "House 1: £200,000\n",
        "House 2: £250,000\n",
        "House 3: £180,000\n",
        "House 4: £220,000\n",
        "House 5: £300,000\n",
        "```\n",
        "\n",
        "We're considering splitting these houses based on whether they have more than 2 bedrooms:\n",
        "- Left group (≤2 bedrooms): Houses 1, 3 (£200,000, £180,000)\n",
        "- Right group (>2 bedrooms): Houses 2, 4, 5 (£250,000, £220,000, £300,000)\n",
        "\n",
        "#### 1. First, let's calculate $MSE_{before}$\n",
        "```\n",
        "Mean price = (200k + 250k + 180k + 220k + 300k) ÷ 5 = £230,000\n",
        "\n",
        "Squared differences from mean:\n",
        "House 1: (230k - 200k)² = 900,000,000\n",
        "House 2: (230k - 250k)² = 400,000,000\n",
        "House 3: (230k - 180k)² = 2,500,000,000\n",
        "House 4: (230k - 220k)² = 100,000,000\n",
        "House 5: (230k - 300k)² = 4,900,000,000\n",
        "\n",
        "MSE_before = (900M + 400M + 2,500M + 100M + 4,900M) ÷ 5\n",
        "           = 1,760,000,000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y12Jz5MkvVOU"
      },
      "source": [
        "#### 2. Now for the left group (≤2 bedrooms):\n",
        "```\n",
        "Mean price = (200k + 180k) ÷ 2 = £190,000\n",
        "\n",
        "Squared differences:\n",
        "House 1: (190k - 200k)² = 100,000,000\n",
        "House 3: (190k - 180k)² = 100,000,000\n",
        "\n",
        "MSE_left = (100M + 100M) ÷ 2 = 100,000,000\n",
        "```\n",
        "\n",
        "#### 3. And the right group (>2 bedrooms):\n",
        "```\n",
        "Mean price = (250k + 220k + 300k) ÷ 3 = £256,667\n",
        "\n",
        "Squared differences:\n",
        "House 2: (256.67k - 250k)² = 44,448,889\n",
        "House 4: (256.67k - 220k)² = 1,344,448,889\n",
        "House 5: (256.67k - 300k)² = 1,877,778,889\n",
        "\n",
        "MSE_right = (44.45M + 1,344.45M + 1,877.78M) ÷ 3 = 1,088,892,222\n",
        "```\n",
        "\n",
        "#### 4. Finally, let's put it all together:\n",
        "```\n",
        "ΔMSE = MSE_before - ((2/5 × MSE_left) + (3/5 × MSE_right))\n",
        "```\n",
        "The second part calculates our weighted mean MSE after splitting:\n",
        "\n",
        "- Left group has 2/5 of the houses, so we multiply its MSE by 2/5\n",
        "- Right group has 3/5 of the houses, so we multiply its MSE by 3/5\n",
        "\n",
        "This weighting ensures each house contributes equally to our final calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-6KqqT0vVOU"
      },
      "source": [
        "Let's solve it:\n",
        "```\n",
        "     = 1,760,000,000 - ((2/5 × 100,000,000) + (3/5 × 1,088,892,222))\n",
        "     = 1,760,000,000 - (40,000,000 + 653,335,333)\n",
        "     = 1,760,000,000 - 693,335,333        # This is our weighted mean MSE after splitting\n",
        "     = 1,066,664,667                      # ΔMSE: The reduction in prediction error\n",
        "```\n",
        "\n",
        "The ΔMSE (1,066,664,667) represents the difference between the original MSE and the weighted average MSE after splitting. This number is always non-negative due to a fundamental property of squared errors:\n",
        "\n",
        "1. MSE is always positive (we're squaring differences from the mean)\n",
        "2. When we split a group:\n",
        "   - The parent uses one mean for all samples\n",
        "   - Each subgroup uses its own mean, which minimises squared errors for that subgroup\n",
        "   - The subgroup means must perform at least as well as the parent mean (due to minimising squared errors locally)\n",
        "   - Therefore, the weighted average MSE of subgroups cannot exceed the parent MSE\n",
        "\n",
        "Therefore:\n",
        "- ΔMSE > 0 means the split has improved predictions (as in our case)\n",
        "- ΔMSE = 0 means the split makes no difference\n",
        "- ΔMSE < 0 is mathematically impossible\n",
        "\n",
        "The larger the ΔMSE, the more effective the split is at creating subgroups with similar house prices. Our large ΔMSE of 1,066,664,667 indicates this is a very effective split.\n",
        "\n",
        "\n",
        "### A simplified decision tree algorithm in Python\n",
        "In practise, you'd use a library like `sklearn` to build a decision tree, but here's a simplified version in python to illustrate the concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReXCkRiuvVOV"
      },
      "outputs": [],
      "source": [
        "class House:\n",
        "    def __init__(self, features: Dict[str, float], price: float):\n",
        "        self.features = features\n",
        "        self.price = price\n",
        "\n",
        "def find_best_split(houses: List[House], feature: str) -> tuple:\n",
        "    values = sorted(set(house.features[feature] for house in houses))\n",
        "\n",
        "    best_split = None\n",
        "    best_delta_mse = float('-inf')\n",
        "\n",
        "    for i in range(len(values) - 1):\n",
        "        split_point = (values[i] + values[i+1]) / 2\n",
        "        left = [h for h in houses if h.features[feature] < split_point]\n",
        "        right = [h for h in houses if h.features[feature] >= split_point]\n",
        "\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "\n",
        "        mse_before = np.var([h.price for h in houses])\n",
        "        mse_left = np.var([h.price for h in left])\n",
        "        mse_right = np.var([h.price for h in right])\n",
        "\n",
        "        delta_mse = mse_before - (len(left)/len(houses) * mse_left + len(right)/len(houses) * mse_right)\n",
        "\n",
        "        if delta_mse > best_delta_mse:\n",
        "            best_delta_mse = delta_mse\n",
        "            best_split = split_point\n",
        "\n",
        "    return best_split, best_delta_mse\n",
        "\n",
        "def build_tree(houses: List[House], depth: int = 0, max_depth: int = 3) -> Dict[str, Any]:\n",
        "    if depth == max_depth or len(houses) < 2:\n",
        "        return {'type': 'leaf', 'value': np.mean([h.price for h in houses])}\n",
        "\n",
        "    features = houses[0].features.keys()\n",
        "    best_feature = None\n",
        "    best_split = None\n",
        "    best_delta_mse = float('-inf')\n",
        "\n",
        "    for feature in features:\n",
        "        split, delta_mse = find_best_split(houses, feature)\n",
        "        if delta_mse > best_delta_mse:\n",
        "            best_feature = feature\n",
        "            best_split = split\n",
        "            best_delta_mse = delta_mse\n",
        "\n",
        "    if best_feature is None:\n",
        "        return {'type': 'leaf', 'value': np.mean([h.price for h in houses])}\n",
        "\n",
        "    left = [h for h in houses if h.features[best_feature] < best_split]\n",
        "    right = [h for h in houses if h.features[best_feature] >= best_split]\n",
        "\n",
        "    return {\n",
        "        'type': 'node',\n",
        "        'feature': best_feature,\n",
        "        'split': best_split,\n",
        "        'left': build_tree(left, depth + 1, max_depth),\n",
        "        'right': build_tree(right, depth + 1, max_depth)\n",
        "    }\n",
        "\n",
        "def predict(tree: Dict[str, Any], house: House) -> float:\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['value']\n",
        "\n",
        "    if house.features[tree['feature']] < tree['split']:\n",
        "        return predict(tree['left'], house)\n",
        "    else:\n",
        "        return predict(tree['right'], house)\n",
        "\n",
        "# Example usage\n",
        "houses = [\n",
        "    House({'bedrooms': 2, 'area': 80, 'distance_to_tube': 15}, 200),\n",
        "    House({'bedrooms': 3, 'area': 120, 'distance_to_tube': 10}, 250),\n",
        "    House({'bedrooms': 2, 'area': 75, 'distance_to_tube': 20}, 180),\n",
        "    House({'bedrooms': 3, 'area': 100, 'distance_to_tube': 5}, 220),\n",
        "    House({'bedrooms': 4, 'area': 150, 'distance_to_tube': 2}, 300),\n",
        "    House({'bedrooms': 3, 'area': 110, 'distance_to_tube': 12}, 240),\n",
        "    House({'bedrooms': 2, 'area': 70, 'distance_to_tube': 25}, 190),\n",
        "    House({'bedrooms': 4, 'area': 140, 'distance_to_tube': 8}, 280),\n",
        "    House({'bedrooms': 3, 'area': 130, 'distance_to_tube': 6}, 260),\n",
        "    House({'bedrooms': 2, 'area': 85, 'distance_to_tube': 18}, 210)\n",
        "]\n",
        "\n",
        "tree = build_tree(houses)\n",
        "\n",
        "def print_tree(node, indent=\"\"):\n",
        "    if node['type'] == 'leaf':\n",
        "        print(f\"{indent}Predict price: £{node['value']:.2f}k\")\n",
        "    else:\n",
        "        print(f\"{indent}{node['feature']} < {node['split']:.2f}\")\n",
        "        print(f\"{indent}If True:\")\n",
        "        print_tree(node['left'], indent + \"  \")\n",
        "        print(f\"{indent}If False:\")\n",
        "        print_tree(node['right'], indent + \"  \")\n",
        "\n",
        "print_tree(tree)\n",
        "\n",
        "# Test prediction\n",
        "new_house = House({'bedrooms': 3, 'area': 105, 'distance_to_tube': 7}, 0)  # price set to 0 as it's unknown\n",
        "predicted_price = predict(tree, new_house)\n",
        "print(f\"\\nPredicted price for new house: £{predicted_price:.2f}k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Gl6sSvvVOV"
      },
      "source": [
        "<a name=\"mean-squared-error-vs-mean-absolute-error\"></a>\n",
        "### Mean squared error (MSE) vs mean absolute error (MAE)\n",
        "\n",
        "When evaluating our decision tree's performance, we need to understand the difference between training metrics and evaluation metrics.\n",
        "\n",
        "![mean-squared-error-mean-absolute-error](https://github.com/powell-clark/supervised-machine-learning/blob/main/static/images/mean-squared-error-mean-absolute-error.png?raw=1)\n",
        "\n",
        "Our decision tree algorithm uses MSE as the splitting criterion but measures final performance using MAE.\n",
        "\n",
        "Here's why we use these different metrics:\n",
        "\n",
        "##### 1. Mean squared error (MSE)\n",
        "\n",
        "   **Calculation:** (predicted house price - actual house price)²\n",
        "\n",
        "   For example, if we predict £200,000 for a house that's actually worth £150,000, the error is £50,000 and MSE is £50,000² = £2.5 billion\n",
        "\n",
        "   **Visualisation**\n",
        "\n",
        "   If we plot how wrong our house price prediction is (like £50,000 too high or -£50,000 too low) on the x-axis, and plot the squared value of this error (like £2.5 billion) on the y-axis, we get a U-shaped curve. Because MSE squares the errors, it gives more weight to data points that are further from the mean, making it a good measure of variance within groups.\n",
        "\n",
        "   **Purpose**\n",
        "\n",
        "   The decision tree uses MSE to decide where to split data because minimising MSE is equivalent to minimising the variance within each group, which helps find splits that create distinct groups of house prices.\n",
        "\n",
        "  ##### 2. Mean absolute error (MAE)\n",
        "\n",
        "   **Calculation:** |predicted house price - actual house price|\n",
        "\n",
        "   Using the same example, if we predict £200,000 for a £150,000 house, MAE is |£50,000| = £50,000\n",
        "\n",
        "   **Visualisation**\n",
        "\n",
        "   If we plot how wrong our prediction is on the x-axis (like £50,000 too high or -£50,000 too low), and plot the absolute value of this error on the y-axis (always positive, like £50,000), we get a V-shaped curve\n",
        "\n",
        "   **Purpose**\n",
        "   \n",
        "   We use MAE to evaluate our final model because it's easier to understand - it directly tells us how many pounds we're off by on average\n",
        "\n",
        "\n",
        "The decision tree uses MSE's mathematical properties to make splitting decisions, but we report MAE because \"off by £50,000 on average\" makes more sense than \"off by £2.5 billion squared pounds\"!\n",
        "\n",
        "\n",
        "Here's an example to illustrate the difference:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZJFSoydvVOV"
      },
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "Mean Squared Error: 200.00\n",
        "Mean Absolute Error: 13.33\n",
        "```\n",
        "\n",
        "In this example, MSE and MAE provide different views of the error. MSE is more sensitive to the larger error (20) in the third prediction, while MAE treats all errors equally.\n",
        "\n",
        "For house price prediction, MAE is often preferred as it directly translates to the average error in pounds. However, MSE is still commonly used as a splitting criterion in decision trees because minimising MSE helps create groups with similar target values by minimising the variance within each group.\n",
        "\n",
        "<a name=\"for-classification-tasks\"></a>\n",
        "### For classification tasks (e.g., Predicting if a House Will Sell Quickly):\n",
        "\n",
        "In classification problems, we're trying to predict a categorical outcome, like whether a house will sell quickly or not. The goal is to split the data in a way that maximises the \"purity\" of the classes within each resulting group.\n",
        "\n",
        "There are several metrics used for classification trees, with the most common being Gini Impurity and Entropy. These metrics measure how mixed the classes are within a group.\n",
        "\n",
        "Let's explore how different distributions of marbles affect our measures of impurity. We will then explore information gain, a measure used in conjuction with impurity metrics to decide how to split the data.\n",
        "\n",
        "We'll use red marbles to represent quick-selling houses and blue marbles for slow-selling houses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLzFp1W9vVOV"
      },
      "source": [
        "<a name=\"gini-impurity\"></a>\n",
        "#### 1. Gini Impurity:\n",
        "   Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution in the set.\n",
        "\n",
        "   Formula: $Gini = 1 - \\sum_{i=1}^{c} (p_i)^2$\n",
        "\n",
        "   Where $c$ is the number of classes and $p_i$ is the probability of an object being classified to a particular class.\n",
        "\n",
        "   Let's compare three scenarios:\n",
        "\n",
        "```\n",
        "   a) 10 marbles: 7 red, 3 blue\n",
        "      Fraction of red = 7/10 = 0.7\n",
        "      Fraction of blue = 3/10 = 0.3\n",
        "      \n",
        "      Gini = 1 - (0.7² + 0.3²) = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42\n",
        "```\n",
        "\n",
        "```\n",
        "   b) 10 marbles: 5 red, 5 blue\n",
        "      Fraction of red = 5/10 = 0.5\n",
        "      Fraction of blue = 5/10 = 0.5\n",
        "      \n",
        "      Gini = 1 - (0.5² + 0.5²) = 1 - (0.25 + 0.25) = 1 - 0.5 = 0.5\n",
        "      most impure set\n",
        "```\n",
        "\n",
        "```\n",
        "   c) 10 marbles: 9 red, 1 blue\n",
        "      Fraction of red = 9/10 = 0.9\n",
        "      Fraction of blue = 1/10 = 0.1\n",
        "      \n",
        "      Gini = 1 - (0.9² + 0.1²) = 1 - (0.81 + 0.01) = 1 - 0.82 = 0.18\n",
        "      purest set\n",
        "```\n",
        "\n",
        "**The lower the Gini Impurity, the purer the set. Scenario (c) has the lowest Gini Impurity, indicating it's the most homogeneous.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj3IEsofvVOV"
      },
      "source": [
        "<a name=\"entropy\"></a>\n",
        "#### 2. Entropy:\n",
        "\n",
        "Entropy is another measure of impurity, based on the concept of information theory. It quantifies the amount of uncertainty or randomness in the data.\n",
        "\n",
        "$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
        "\n",
        "Where $c$ is the number of classes and $p_i$ is the probability of an object being classified to a particular class.\n",
        "\n",
        "Imagine you're playing a guessing game with marbles in a bag. Entropy measures how surprised you'd be when pulling out a marble. The more mixed the colours, the more surprised you might be, and the higher the entropy.\n",
        "\n",
        "#### Let's use our marble scenarios:\n",
        "\n",
        "10 marbles: 7 red, 3 blue\n",
        "\n",
        "To calculate entropy, we follow these steps:\n",
        "\n",
        "1. Calculate the fraction of each colour:\n",
        "```\n",
        "   Red: 7/10 = 0.7\n",
        "   Blue: 3/10 = 0.3\n",
        "```\n",
        "\n",
        "2. For each colour, multiply its fraction by the log2 of its fraction:   \n",
        "```\n",
        "   Red: 0.7 × log2(0.7) = 0.7 × -0.5146 = -0.360\n",
        "   Blue: 0.3 × log2(0.3) = 0.3 × -1.7370 = -0.5211\n",
        "```\n",
        "\n",
        "3. Sum these values and negate the result:\n",
        "```\n",
        "Entropy = -(-0.3602 + -0.5211) = 0.8813\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqs1-F6zvVOV"
      },
      "source": [
        "#### Let's do this for all scenarios:\n",
        "\n",
        "a) 7 red, 3 blue\n",
        "```\n",
        "   Entropy = 0.8813\n",
        "```\n",
        "b) 5 red, 5 blue\n",
        "```\n",
        "   Red: 0.5 × log2(0.5) = 0.5 × -1 = -0.5\n",
        "   Blue: 0.5 × log2(0.5) = 0.5 × -1 = -0.5\n",
        "   Entropy = -(-0.5 + -0.5) = 1\n",
        "\n",
        "Highest entropy, least predictable set\n",
        "```\n",
        "\n",
        "c) 9 red, 1 blue\n",
        "```\n",
        "   Red: 0.9 × log2(0.9) = 0.9 × -0.1520 = -0.1368\n",
        "   Blue: 0.1 × log2(0.1) = 0.1 × -3.3219 = -0.3322\n",
        "   Entropy = -(-0.1368 + -0.3322) = 0.4690\n",
        "\n",
        "Lowest entropy, most predictable set\n",
        "```\n",
        "\n",
        "Lower entropy means less surprise or uncertainty. Scenario (c) has the lowest entropy, confirming it's the most predictable (or least mixed) set.\n",
        "\n",
        "In Python, we could calculate entropy like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyd3neGuvVOW"
      },
      "outputs": [],
      "source": [
        "def calculate_entropy(marbles):\n",
        "    total = sum(marbles.values())\n",
        "    entropy = 0\n",
        "    for count in marbles.values():\n",
        "        fraction = count / total\n",
        "        entropy -= fraction * math.log2(fraction)\n",
        "    return entropy\n",
        "\n",
        "# Example usage\n",
        "scenario_a = {\"red\": 7, \"blue\": 3}\n",
        "entropy_a = calculate_entropy(scenario_a)\n",
        "print(f\"Entropy for scenario A: {entropy_a:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1whLrnomvVOW"
      },
      "source": [
        "<a name=\"information-gain\"></a>\n",
        "#### 3. Information gain:\n",
        "\n",
        "Information Gain measures how much a split improves our ability to predict the outcome. It's a way of measuring how much better you've sorted your marbles after dividing them into groups.\n",
        "\n",
        "Formula: $IG(T, a) = I(T) - \\sum_{v \\in values(a)} \\frac{|T_v|}{|T|} I(T_v)$\n",
        "\n",
        "Where:\n",
        "- $T$ is the parent set\n",
        "- $a$ is the attribute on which the split is made\n",
        "- $v$ represents each possible value of attribute $a$\n",
        "- $T_v$ is the subset of $T$ for which attribute $a$ has value $v$\n",
        "- $I(T)$ is the impurity measure (Entropy or Gini) of set $T$\n",
        "\n",
        "\n",
        "#### Let's use a scenario to calculate Information Gain:\n",
        "\n",
        "We have 20 marbles total, and we're considering splitting them based on a feature (e.g., house size: small or large).\n",
        "```\n",
        "Before split: 12 red, 8 blue\n",
        "```\n",
        "\n",
        "Step 1: Calculate the entropy before the split\n",
        "```\n",
        "Entropy_before = 0.9710 (calculated as we did above)\n",
        "```\n",
        "\n",
        "After split:\n",
        "```\n",
        "Small houses: 8 red, 2 blue\n",
        "Large houses: 4 red, 6 blue\n",
        "```\n",
        "Step 2: Calculate entropy for each group after the split\n",
        "Entropy_small = 0.7219 (calculated for 8 red, 2 blue)\n",
        "Entropy_large = 0.9710 (calculated for 4 red, 6 blue)\n",
        "\n",
        "Step 3: Calculate the weighted average of the split entropies\n",
        "```\n",
        "Weight_small = 10/20 = 0.5 (half the marbles are in small houses)\n",
        "Weight_large = 10/20 = 0.5 (half the marbles are in large houses)\n",
        "Weighted_entropy_after = (0.5 × 0.7219) + (0.5 × 0.9710) = 0.8465\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oMrVzmxvVOW"
      },
      "outputs": [],
      "source": [
        "def calculate_information_gain(before, after):\n",
        "    \"\"\"Calculate information gain from a split\n",
        "\n",
        "    Args:\n",
        "        before (dict): Distribution before split (e.g., {'red': 12, 'blue': 8})\n",
        "        after (list): List of distributions after split\n",
        "                      (e.g., [{'red': 8, 'blue': 2}, {'red': 4, 'blue': 6}])\n",
        "    \"\"\"\n",
        "    entropy_before = calculate_entropy(before)\n",
        "\n",
        "    total_after = sum(sum(group.values()) for group in after)\n",
        "    weighted_entropy_after = sum(\n",
        "        (sum(group.values()) / total_after) * calculate_entropy(group)\n",
        "        for group in after\n",
        "    )\n",
        "\n",
        "    return entropy_before - weighted_entropy_after\n",
        "\n",
        "# Example usage\n",
        "before_split = {\"red\": 12, \"blue\": 8}\n",
        "after_split = [\n",
        "    {\"red\": 8, \"blue\": 2},  # Small houses\n",
        "    {\"red\": 4, \"blue\": 6}   # Large houses\n",
        "]\n",
        "\n",
        "info_gain = calculate_information_gain(before_split, after_split)\n",
        "print(f\"Information Gain: {info_gain:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZeUmsz5vVOW"
      },
      "source": [
        "<a name=\"comparison-splits-with-different-information-gains\"></a>\n",
        "#### Comparison: Splits with different information gains\n",
        "\n",
        "The decision tree algorithm always chooses the split that provides the most Information Gain.\n",
        "\n",
        "Let's consider two potential splits of our 20 marbles:\n",
        "\n",
        "1. Split by house size (small vs large):\n",
        "   - Small houses: 8 red, 2 blue\n",
        "   - Large houses: 4 red, 6 blue\n",
        "   - Information Gain: 0.1245\n",
        "\n",
        "2. Split by garage presence:\n",
        "   - Houses with garage: 6 red, 4 blue\n",
        "   - Houses without garage: 6 red, 4 blue\n",
        "   - Information Gain: 0\n",
        "\n",
        "The algorithm would choose the split by house size because it provides more Information Gain.\n",
        "\n",
        "Zero Information Gain occurs when a split doesn't change the distribution of the target variable (in this case, marble colours or house selling speed). This happens when the proportions in each resulting group are identical to the proportions in the parent group.\n",
        "\n",
        "In practice, splits with exactly zero Information Gain are rare. More commonly, you'll see splits with varying degrees of positive Information Gain, and the algorithm will choose the one with the highest value.\n",
        "\n",
        "Features that provide little or no Information Gain are typically less valuable for prediction and should be considered for removal from the model. Eliminating these low-impact features can simplify the model, potentially improving its generalisation ability and computational efficiency without significantly compromising predictive performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjqKbUGrvVOW"
      },
      "source": [
        "<a name=\"interpretability-and-visualisation\"></a>\n",
        "## Interpretability and visualisation\n",
        "\n",
        "After understanding how decision trees split data using criteria like MSE and Gini impurity, it's crucial to explore one of their greatest strengths: interpretability.\n",
        "\n",
        "Unlike many machine learning models that act as \"black boxes,\" decision trees provide clear insights into their decision-making process.\n",
        "\n",
        "<a name=\"why-interpretability-matters\"></a>\n",
        "### Why interpretability matters\n",
        "\n",
        "For house price prediction, interpretability allows us to:\n",
        "- Explain predictions to stakeholders (buyers, sellers, agents)\n",
        "- Validate model logic against domain knowledge\n",
        "- Identify potential biases or errors\n",
        "- Meet regulatory requirements for transparency\n",
        "\n",
        "<a name=\"how-to-interpret-decision-trees\"></a>\n",
        "### How to interpret decision trees\n",
        "\n",
        "#### 1. Reading tree structure\n",
        "\n",
        "Consider this simplified tree for house prices:\n",
        "```\n",
        "Area > 2000 sq ft?\n",
        "├── Yes: Location = \"Chelsea\"?\n",
        "│   ├── Yes: £2.5M (n=50)\n",
        "│   └── No: £1.8M (n=150)\n",
        "└── No: Number of bedrooms > 2?\n",
        "    ├── Yes: £950K (n=200)\n",
        "    └── No: £650K (n=100)\n",
        "```\n",
        "\n",
        "Each node tells us:\n",
        "- The decision rule (e.g., \"Area > 2000 sq ft?\")\n",
        "- The number of samples (n)\n",
        "- The predicted value (for leaf nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpkG8uT9vVOW"
      },
      "source": [
        "#### 2. Decision paths\n",
        "\n",
        "Each path from root to leaf represents a complete prediction rule. For example:\n",
        "- IF area > 2000 sq ft AND location = \"Chelsea\" THEN price = £2.5M\n",
        "- IF area ≤ 2000 sq ft AND bedrooms > 2 THEN price = £950K\n",
        "\n",
        "This allows us to provide clear explanations for any prediction.\n",
        "\n",
        "#### 3. Feature importance\n",
        "\n",
        "Decision trees naturally reveal feature importance through:\n",
        "\n",
        "a) Position in tree:\n",
        "- Features closer to root affect more predictions\n",
        "- Top-level splits handle larger portions of data\n",
        "\n",
        "b) Usage frequency:\n",
        "- Features used multiple times may be more important\n",
        "- Different contexts show feature interactions\n",
        "\n",
        "c) Impact on predictions:\n",
        "- Splits that create large value differences are important\n",
        "- Features that reduce variance significantly\n",
        "\n",
        "<a name=\"visualising-decision-trees\"></a>\n",
        "## Visualising decision trees\n",
        "\n",
        "While our simple example above is easy to read, real trees can be much more complex. Here are key visualisation approaches:\n",
        "\n",
        "1. **Full tree visualisation**\n",
        "   - Shows complete structure\n",
        "   - Good for understanding overall patterns\n",
        "   - Can become overwhelming for deep trees\n",
        "\n",
        "2. **Pruned tree views**\n",
        "   - Show top few levels\n",
        "   - Focus on most important decisions\n",
        "   - More manageable for presentation\n",
        "\n",
        "3. **Feature importance plots**\n",
        "   - Bar charts of feature importance\n",
        "   - Easier to digest than full trees\n",
        "   - Good for high-level insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCGSMOGyvVOW"
      },
      "outputs": [],
      "source": [
        "# Create sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate features\n",
        "area = np.random.normal(2000, 500, n_samples)\n",
        "bedrooms = np.random.randint(1, 6, n_samples)\n",
        "location_code = np.random.randint(0, 3, n_samples)  # 0: Other, 1: Chelsea, 2: Mayfair\n",
        "\n",
        "# Generate target (house prices)\n",
        "base_price = 500000\n",
        "area_impact = (area - 2000) * 500\n",
        "bedroom_impact = bedrooms * 100000\n",
        "location_impact = location_code * 300000\n",
        "noise = np.random.normal(0, 50000, n_samples)\n",
        "\n",
        "price = base_price + area_impact + bedroom_impact + location_impact + noise\n",
        "\n",
        "# Create and fit the model\n",
        "X = np.column_stack([area, bedrooms, location_code])\n",
        "model = DecisionTreeRegressor(max_depth=3, min_samples_leaf=100)\n",
        "model.fit(X, price)\n",
        "\n",
        "# Plot the tree\n",
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(model,\n",
        "          feature_names=['Area', 'Bedrooms', 'Location'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10)\n",
        "plt.title('House Price Decision Tree')\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances\n",
        "importances = pd.Series(model.feature_importances_,\n",
        "                       index=['Area', 'Bedrooms', 'Location'])\n",
        "plt.figure(figsize=(10,6))\n",
        "importances.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance in House Price Prediction')\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ANomtrvVOW"
      },
      "source": [
        "<a name=\"understanding-bias-variance-tree-depth-and-complexity\"></a>\n",
        "## Understanding bias, variance, tree depth and complexity\n",
        "\n",
        "<a name=\"bias\"></a>\n",
        "### Bias\n",
        "- **The error introduced by approximating a real-world problem with a simplified model**\n",
        "- Represents how far off the model's predictions are from the true values on average\n",
        "- High bias means the model consistently misses the true patterns (underfitting)\n",
        "\n",
        "    1. **Shallow Trees (High Bias)**\n",
        "    ```\n",
        "    Root: Area > 2000 sq ft?\n",
        "    ├── Yes: £2M\n",
        "    └── No: £800K\n",
        "    ```\n",
        "    - Very simple rules\n",
        "    - Misses many important factors\n",
        "    - Similar predictions for different houses\n",
        "\n",
        "<a name=\"variance\"></a>\n",
        "### Variance\n",
        "- **The model's sensitivity to fluctuations in the training data**\n",
        "- Represents how much predictions change with different training sets\n",
        "- High variance means predictions vary significantly with small changes in training data (overfitting)\n",
        "\n",
        "    2. **Deep Trees (High Variance)**\n",
        "    ```\n",
        "    Root: Area > 2000 sq ft?\n",
        "    ├── Yes: Location = \"Chelsea\"?\n",
        "    │   ├── Yes: Bedrooms > 3?\n",
        "    │   │   ├── Yes: Garden = True?\n",
        "    │   │   │   ├── Yes: £3.2M\n",
        "    │   │   │   └── No: £2.9M\n",
        "    ...\n",
        "    ```\n",
        "    - Very specific rules\n",
        "    - Might memorise training data\n",
        "    - Can make unstable predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCMnnZ5svVOd"
      },
      "source": [
        "<a name=\"identifying-the-biasvariance-tradeoff\"></a>\n",
        "## Identifying the Bias/Variance Tradeoff\n",
        "\n",
        "Consider these scenarios:\n",
        "\n",
        "### Scenario 1: Too simple (high bias)\n",
        "```python\n",
        "# Example of underfitting\n",
        "predictions = {\n",
        "    \"2500 sq ft in Chelsea\": £2M,\n",
        "    \"2500 sq ft in Hackney\": £2M,  # Same prediction despite location\n",
        "    \"2500 sq ft in Mayfair\": £2M   # Location ignored\n",
        "}\n",
        "```\n",
        "\n",
        "### Scenario 2: Too complex (high variance)\n",
        "```python\n",
        "# Example of overfitting\n",
        "predictions = {\n",
        "    \"2500 sq ft, Chelsea, 4 bed, garden\": £3.2M,\n",
        "    \"2500 sq ft, Chelsea, 4 bed, no garden\": £2.9M,\n",
        "    # Small changes lead to large prediction differences\n",
        "    \"2499 sq ft, Chelsea, 4 bed, garden\": £2.7M  # Just 1 sq ft difference\n",
        "}\n",
        "```\n",
        "\n",
        "### Scenario 3: Balanced\n",
        "```python\n",
        "# Example of good balance\n",
        "predictions = {\n",
        "    \"Large house in Chelsea\": £2.5M-3.0M,\n",
        "    \"Large house in Hackney\": £1.5M-2.0M,\n",
        "    # Reasonable variations based on key features\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C23rB7WtvVOe"
      },
      "source": [
        "<a name=\"managing-the-biasvariance-tradeoff\"></a>\n",
        "## Managing the bias/variance tradeoff\n",
        "\n",
        "When building a decision tree, we need to find the right balance between making it too simple (underfitting) and too complex (overfitting).\n",
        "\n",
        "Let's explore how to find this balance.\n",
        "\n",
        "### 1. Control tree complexity\n",
        "We can control how detailed our tree becomes using parameters:\n",
        "- Maximum depth (how many questions we can ask)\n",
        "- Minimum samples per leaf (how many houses needed for a conclusion)\n",
        "- Minimum improvement threshold (how much better a split needs to be)\n",
        "\n",
        "### 2. Understanding training vs validation error\n",
        "\n",
        "Training error is how well our model predicts house prices for houses it learned from, while validation error is how well it predicts prices for houses it hasn't seen before.\n",
        "\n",
        "Think of it like this:\n",
        "- **Training error**: How well you can predict prices of houses you studied\n",
        "- **Validation error**: How well you can predict prices of new houses\n",
        "\n",
        "Let's look at how these errors change as we make our tree more complex:\n",
        "\n",
        "```\n",
        "Depth   Training error  Validation error   What's happening\n",
        "3       £250K           £260K              #  Tree is too simple\n",
        "                                           #  - Both errors are high\n",
        "                                           #  - Tree isn't learning enough patterns\n",
        "\n",
        "5       £180K           £200K              #  Tree is just right\n",
        "                                           #  - Both errors are reasonable\n",
        "                                           #  - Tree learns genuine patterns\n",
        "\n",
        "7       £120K           £220K              #  Tree is getting too complex\n",
        "                                           #  - Training error keeps dropping\n",
        "                                           #  - Validation error starts rising\n",
        "                                           #  - Starting to memorise training data\n",
        "\n",
        "10      £50K            £300K              #  Tree is way too complex\n",
        "                                           #  - Training error is very low\n",
        "                                           #  - Validation error is very high\n",
        "                                           #  - Tree has memorised training data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC5AXvNmvVOe"
      },
      "outputs": [],
      "source": [
        "# Generate sample housing data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Features\n",
        "X = np.random.randn(n_samples, 3)  # area, bedrooms, location_score\n",
        "# True price function with some noise\n",
        "y = (3 * X[:, 0] + 2 * X[:, 1] + X[:, 2] +\n",
        "     0.2 * (X[:, 0] ** 2) + 0.1 * (X[:, 1] ** 2) +\n",
        "     np.random.randn(n_samples) * 0.1)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test different depths\n",
        "max_depths = range(1, 15)\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for depth in max_depths:\n",
        "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_train, y_train)\n",
        "\n",
        "    train_pred = tree.predict(X_train)\n",
        "    val_pred = tree.predict(X_val)\n",
        "\n",
        "    train_errors.append(mean_squared_error(y_train, train_pred))\n",
        "    val_errors.append(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depths, train_errors, 'o-', label='Training Error')\n",
        "plt.plot(max_depths, val_errors, 'o-', label='Validation Error')\n",
        "plt.xlabel('Maximum Tree Depth')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Learning Curves: Training vs Validation Error')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find optimal depth\n",
        "optimal_depth = max_depths[np.argmin(val_errors)]\n",
        "print(f\"Optimal tree depth: {optimal_depth}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1MA_N_IvVOe"
      },
      "source": [
        "<a name=\"visual-indicators-of-biasvariance\"></a>\n",
        "## Visual indicators of bias/variance\n",
        "\n",
        "### 1. Learning curves\n",
        "\n",
        "![model-complexity-bias-variance-contributing-to-total-error](https://github.com/powell-clark/supervised-machine-learning/blob/main/static/images/model-complexity-bias-variance-contributing-to-total-error.png?raw=1)\n",
        "\n",
        "As the model complexity increases, the training error decreases and the validation error increases.\n",
        "\n",
        "Total error is the sum of bias (the error introduced by approximating a real-world problem with a simplified model) and variance (the error caused by the model's sensitivity to fluctuations in the training data).\n",
        "\n",
        "Underfitting occurs when the model is too simple (high bias), resulting in both training set and validation set total errors being high.\n",
        "\n",
        "Overfitting occurs when the model is too complex (high variance), resulting in a large gap between training and validation set total errors.\n",
        "\n",
        "![model-complexity-error-training-test-samples](https://github.com/powell-clark/supervised-machine-learning/blob/main/static/images/model-complexity-error-training-test-samples.png?raw=1)\n",
        "\n",
        "![performance-model-complexity-training-validation-sets-overfitting](https://github.com/powell-clark/supervised-machine-learning/blob/main/static/images/performance-model-complexity-training-validation-sets-overfitting.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXcw3twlvVOe"
      },
      "source": [
        "## Practical guidelines\n",
        "\n",
        "1. **Start simple**\n",
        "   - Begin with shallow trees\n",
        "   - Add complexity gradually\n",
        "   - Monitor performance changes\n",
        "\n",
        "2. **Use domain knowledge**\n",
        "   - Consider reasonable decision granularity\n",
        "   - Identify important feature interactions\n",
        "   - Set meaningful constraints\n",
        "\n",
        "3. **Regular validation**\n",
        "   - Test on unseen data\n",
        "   - Check prediction stability\n",
        "   - Monitor for overfitting signs\n",
        "\n",
        "Understanding this tradeoff is crucial for:\n",
        "- Setting appropriate tree depth\n",
        "- Choosing regularisation parameters\n",
        "- Deciding when to use ensemble methods\n",
        "\n",
        "Now that we understand how to build well-balanced decision trees, we need to know which features are driving their decisions.\n",
        "\n",
        "In the next section, we'll explore how decision trees determine which features are most important for making predictions (like whether location matters more than size for house prices) and discover their advanced capabilities in handling different types of data. This knowledge is crucial for building more effective models and gaining insights from your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjYv54JxvVOe"
      },
      "outputs": [],
      "source": [
        "def analyse_tree_complexity(X, y, max_depths=range(1, 15), cv=5):\n",
        "    \"\"\"Analyse decision tree performance across different depths using cross-validation.\"\"\"\n",
        "    cv_scores_mean = []\n",
        "    cv_scores_std = []\n",
        "\n",
        "    for depth in max_depths:\n",
        "        tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "        scores = cross_val_score(tree, X, y, cv=cv, scoring='neg_mean_squared_error')\n",
        "        cv_scores_mean.append(-scores.mean())  # Convert back to positive MSE\n",
        "        cv_scores_std.append(scores.std())\n",
        "\n",
        "    # Plot results with error bars\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.errorbar(max_depths, cv_scores_mean, yerr=cv_scores_std,\n",
        "                fmt='o-', capsize=5, capthick=1, elinewidth=1)\n",
        "    plt.xlabel('Maximum Tree Depth')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.title('Cross-Validation Performance vs Tree Depth')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Find optimal depth\n",
        "    optimal_depth = max_depths[np.argmin(cv_scores_mean)]\n",
        "    plt.axvline(optimal_depth, color='r', linestyle='--', alpha=0.5)\n",
        "    plt.text(optimal_depth + 0.1, plt.ylim()[0], f'Optimal depth: {optimal_depth}',\n",
        "             rotation=90, verticalalignment='bottom')\n",
        "\n",
        "    plt.show()\n",
        "    return optimal_depth, min(cv_scores_mean)\n",
        "\n",
        "# Example usage with housing data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate features with realistic relationships\n",
        "area = np.random.normal(2000, 500, n_samples)  # Area in sq ft\n",
        "bedrooms = np.random.randint(1, 6, n_samples)  # Number of bedrooms\n",
        "location_score = np.random.uniform(0, 1, n_samples)  # Location desirability\n",
        "\n",
        "# Generate prices with non-linear relationships and interaction effects\n",
        "base_price = 500000\n",
        "price = (base_price +\n",
        "         area * 200 * (1 + location_score) +  # Area effect varies by location\n",
        "         bedrooms * 50000 * (1 + 0.5 * location_score) +  # Bedroom effect also varies\n",
        "         location_score * 1000000 +  # Direct location effect\n",
        "         np.random.normal(0, 50000, n_samples))  # Random noise\n",
        "\n",
        "X = np.column_stack([area, bedrooms, location_score])\n",
        "optimal_depth, best_score = analyse_tree_complexity(X, price)\n",
        "print(f\"Best CV Score (MSE): £{best_score:,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD9NerPVvVOe"
      },
      "source": [
        "<a name=\"feature-importance-and-advanced-tree-based-methods\"></a>\n",
        "## Feature importance and advanced tree-based methods\n",
        "\n",
        "After exploring the fundamentals of decision trees, we'll examine how they evaluate feature importance and how this understanding leads to more sophisticated tree-based methods. This progression builds toward ATLAS (Automated Tree Learning Analysis System) through:\n",
        "\n",
        "1. Understanding how decision trees determine feature importance\n",
        "2. Recognising the limitations that motivated ensemble methods\n",
        "3. Building up to Random Forests - the foundation of parallel tree ensembles\n",
        "4. Understanding sequential learning with Gradient Boosting and XGBoost\n",
        "\n",
        "These concepts directly inform ATLAS's model selection and comparison strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5BpAVVvVOe"
      },
      "source": [
        "<a name=\"mathematical-foundation-of-feature-importance\"></a>\n",
        "### Mathematical foundation of feature importance\n",
        "\n",
        "Decision trees provide a natural measure of feature importance based on how much each feature contributes to reducing prediction error. For a feature $f$, its importance is calculated as:\n",
        "\n",
        "$importance(f) = \\sum_{nodes\\:using\\:f} \\frac{n_{node}}{N} \\cdot (impurity_{before} - impurity_{after})$\n",
        "\n",
        "where:\n",
        "- $n_{node}$ is the number of samples reaching the node\n",
        "- $N$ is the total number of samples\n",
        "- $impurity_{before}$ is the node's impurity before splitting\n",
        "- $impurity_{after}$ is the weighted sum of child node impurities\n",
        "\n",
        "For house price prediction:\n",
        "- $impurity_{before}$ is the variance of house prices at a node\n",
        "- The split that maximises impurity reduction is chosen\n",
        "- Features that create purer groups (more similar prices) get higher importance\n",
        "\n",
        "### Properties of feature importance\n",
        "\n",
        "1. **Scale Independence**\n",
        "   - Importance measures are relative\n",
        "   - Sum to 1 across all features\n",
        "   - Independent of feature scales\n",
        "\n",
        "2. **Hierarchy Effect**\n",
        "   - Splits near root affect more samples\n",
        "   - Early splits tend to use most important features\n",
        "   - Deep splits have limited impact on overall importance\n",
        "\n",
        "3. **Interaction Capture**\n",
        "   - Features that work well together get high importance\n",
        "   - Accounts for non-linear relationships\n",
        "   - Reflects real-world feature dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu41LOokvVOe"
      },
      "outputs": [],
      "source": [
        "def analyse_feature_importance(X, y, max_depth=5):\n",
        "    \"\"\"Analyse and visualise feature importance in decision trees\"\"\"\n",
        "\n",
        "    # Train the model\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
        "    tree.fit(X, y)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    importance = pd.Series(\n",
        "        tree.feature_importances_,\n",
        "        index=X.columns\n",
        "    ).sort_values(ascending=True)\n",
        "\n",
        "    # Analyse importance at different depths\n",
        "    importance_by_depth = []\n",
        "    for depth in range(1, max_depth + 1):\n",
        "        tree_depth = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "        tree_depth.fit(X, y)\n",
        "        importance_depth = pd.Series(\n",
        "            tree_depth.feature_importances_,\n",
        "            index=X.columns\n",
        "        )\n",
        "        importance_by_depth.append(importance_depth)\n",
        "\n",
        "    # Plot results\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Overall importance\n",
        "    importance.plot(kind='barh', ax=ax1)\n",
        "    ax1.set_title('Overall Feature Importance')\n",
        "    ax1.set_xlabel('Relative Importance')\n",
        "\n",
        "    # Importance evolution with depth\n",
        "    depth_df = pd.DataFrame(importance_by_depth).T\n",
        "    depth_df.columns = [f'Depth {i+1}' for i in range(max_depth)]\n",
        "    depth_df.plot(ax=ax2, marker='o')\n",
        "    ax2.set_title('Feature Importance by Tree Depth')\n",
        "    ax2.set_xlabel('Tree Depth')\n",
        "    ax2.set_ylabel('Relative Importance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return importance, depth_df\n",
        "\n",
        "# Generate example housing data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Features with known relationships\n",
        "X = pd.DataFrame({\n",
        "    'area': np.random.normal(2000, 500, n_samples),\n",
        "    'bedrooms': np.random.randint(1, 6, n_samples),\n",
        "    'location_score': np.random.uniform(0, 1, n_samples),\n",
        "    'age': np.random.randint(0, 50, n_samples),\n",
        "    'distance_to_center': np.random.uniform(1, 20, n_samples)\n",
        "})\n",
        "\n",
        "# Generate prices with known importance relationships\n",
        "y = (\n",
        "    X['area'] * 200 +  # Strong effect\n",
        "    X['location_score'] * 500000 +  # Very strong effect\n",
        "    X['bedrooms'] * 50000 +  # Moderate effect\n",
        "    X['age'] * -1000 +  # Weak effect\n",
        "    X['distance_to_center'] * -5000 +  # Medium effect\n",
        "    np.random.normal(0, 50000, n_samples)  # Noise\n",
        ")\n",
        "\n",
        "# Analyse importance\n",
        "importance, importance_by_depth = analyse_feature_importance(X, y)\n",
        "\n",
        "print(\"\\nFeature Importance Summary:\")\n",
        "print(importance)\n",
        "\n",
        "print(\"\\nImportance Evolution by Depth:\")\n",
        "print(importance_by_depth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP__J-qjvVOf"
      },
      "source": [
        "<a name=\"random-forests-ensemble-learning-from-first-principles\"></a>\n",
        "## Random Forests: Ensemble learning from first principles\n",
        "\n",
        "Random forests address the fundamental limitations of single decision trees through statistical principles of ensemble learning.\n",
        "\n",
        "Let's build up the concept from first principles.\n",
        "\n",
        "### The variance problem\n",
        "\n",
        "Consider a single decision tree trained on house price data. If we train it on slightly different datasets, we get significantly different trees:\n",
        "\n",
        "```\n",
        "Dataset 1 Tree:              Dataset 2 Tree:\n",
        "area > 2000?                 location_score > 0.7?\n",
        "├── Yes: £2.5M              ├── Yes: £2.8M\n",
        "└── No: bedrooms > 3?       └── No: area > 1800?\n",
        "```\n",
        "\n",
        "This high variance means:\n",
        "- Small data changes → large model changes\n",
        "- Overfitting to training data\n",
        "- Poor generalisation\n",
        "\n",
        "### Statistical solution: Averaging independent predictions\n",
        "\n",
        "If we have multiple independent predictions $\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_m$, their average has lower variance:\n",
        "\n",
        "$Var(\\frac{1}{m}\\sum_{i=1}^m \\hat{y}_i) = \\frac{\\sigma^2}{m}$ (if independent)\n",
        "\n",
        "But we only have one dataset! This leads to two key innovations:\n",
        "\n",
        "1. **Bootstrap sampling**\n",
        "   - Create multiple datasets by sampling with replacement\n",
        "   - Each sample is same size as original data\n",
        "   - ~63.2% unique samples in each bootstrap\n",
        "   - Remaining ~36.8% are duplicates\n",
        "\n",
        "2. **Feature randomisation**\n",
        "   - At each split, consider only random subset of features\n",
        "   - Makes trees more independent\n",
        "   - Default size: $\\sqrt{p}$ for classification, $p/3$ for regression\n",
        "   where $p$ is number of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK0IsJdAvVOf"
      },
      "source": [
        "### Mathematical foundation\n",
        "\n",
        "For a random forest with $M$ trees:\n",
        "\n",
        "$\\hat{f}_{rf}(x) = \\frac{1}{M}\\sum_{m=1}^M T_m(x)$\n",
        "\n",
        "where $T_m(x)$ is the prediction of the $m$th tree.\n",
        "\n",
        "The prediction variance is:\n",
        "\n",
        "$Var(\\hat{f}_{rf}) = \\rho \\sigma^2 + \\frac{1-\\rho}{M}\\sigma^2$\n",
        "\n",
        "where:\n",
        "- $\\rho$ is the correlation between trees\n",
        "- $\\sigma^2$ is the variance of individual trees\n",
        "\n",
        "This shows:\n",
        "- More trees ($M \\uparrow$) → lower variance\n",
        "- Lower correlation ($\\rho \\downarrow$) → lower variance\n",
        "- Feature randomisation reduces $\\rho$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv-l5AfkvVOf"
      },
      "outputs": [],
      "source": [
        "class SimpleRandomForest:\n",
        "    \"\"\"Simple Random Forest implementation to demonstrate core concepts\"\"\"\n",
        "\n",
        "    def __init__(self, n_trees=100, max_features='sqrt', max_depth=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.feature_importances_ = None\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        \"\"\"Generate bootstrap sample with replacement\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        return X.iloc[idxs] if hasattr(X, 'iloc') else X[idxs], y.iloc[idxs] if hasattr(y, 'iloc') else y[idxs]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit random forest using bootstrap samples\"\"\"\n",
        "        self.trees = []\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Calculate max_features\n",
        "        if self.max_features == 'sqrt':\n",
        "            max_features = int(np.sqrt(n_features))\n",
        "        elif self.max_features == 'log2':\n",
        "            max_features = int(np.log2(n_features))\n",
        "        else:\n",
        "            max_features = n_features\n",
        "\n",
        "        # Train each tree on bootstrap sample\n",
        "        tree_importances = []\n",
        "        for _ in range(self.n_trees):\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "            tree = DecisionTreeRegressor(\n",
        "                max_features=max_features,\n",
        "                max_depth=self.max_depth\n",
        "            )\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "            tree_importances.append(tree.feature_importances_)\n",
        "\n",
        "        # Calculate feature importance as mean across trees\n",
        "        self.feature_importances_ = np.mean(tree_importances, axis=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions by averaging tree predictions\"\"\"\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.mean(tree_preds, axis=0)\n",
        "\n",
        "def compare_tree_vs_forest(X, y, n_experiments=50):\n",
        "    \"\"\"Compare variance of single tree vs random forest\"\"\"\n",
        "    single_tree_preds = []\n",
        "    forest_preds = []\n",
        "\n",
        "    # Generate test point\n",
        "    X_test = pd.DataFrame([X.mean()]).reset_index(drop=True)\n",
        "\n",
        "    for _ in range(n_experiments):\n",
        "        # Bootstrap sample\n",
        "        idxs = np.random.choice(len(X), size=len(X), replace=True)\n",
        "        X_boot = X.iloc[idxs].reset_index(drop=True)\n",
        "        y_boot = y.iloc[idxs].reset_index(drop=True)\n",
        "\n",
        "        # Single tree\n",
        "        tree = DecisionTreeRegressor(max_depth=5)\n",
        "        tree.fit(X_boot, y_boot)\n",
        "        single_tree_preds.append(tree.predict(X_test)[0])\n",
        "\n",
        "        # Random forest\n",
        "        rf = SimpleRandomForest(n_trees=100, max_depth=5)\n",
        "        rf.fit(X_boot, y_boot)\n",
        "        forest_preds.append(rf.predict(X_test)[0])\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.hist(single_tree_preds, alpha=0.5, label='Single Tree', bins=20)\n",
        "    plt.hist(forest_preds, alpha=0.5, label='Random Forest', bins=20)\n",
        "    plt.title('Prediction Distribution: Single Tree vs Random Forest')\n",
        "    plt.xlabel('Predicted Price')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "    # Print statistics\n",
        "    print('Prediction Variance:')\n",
        "    print(f'Single Tree: {np.var(single_tree_preds):,.0f}')\n",
        "    print(f'Random Forest: {np.var(forest_preds):,.0f}')\n",
        "    print(f'Variance Reduction: {(1 - np.var(forest_preds)/np.var(single_tree_preds))*100:.1f}%')\n",
        "\n",
        "# Run demonstration\n",
        "compare_tree_vs_forest(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U55EoJl6vVOf"
      },
      "source": [
        "<a name=\"gradient-boosting-and-xgboost-sequential-learning\"></a>\n",
        "## Gradient Boosting and XGBoost: Sequential learning\n",
        "\n",
        "While random forests reduce variance through parallel ensemble learning, gradient boosting takes a sequential approach, building an ensemble by iteratively correcting errors. This leads to a powerful framework that the XGBoost library further optimises.\n",
        "\n",
        "### Core gradient boosting principles\n",
        "\n",
        "Gradient boosting builds an ensemble sequentially:\n",
        "```\n",
        "Initial Prediction → Error → New Tree → Updated Prediction → Error → New Tree ...\n",
        "```\n",
        "\n",
        "For house prices, this means:\n",
        "1. Make initial prediction (e.g., mean house price)\n",
        "2. Build tree to predict the errors\n",
        "3. Add scaled tree predictions to current predictions\n",
        "4. Repeat process focusing on remaining errors\n",
        "\n",
        "### Mathematical foundation\n",
        "\n",
        "For a house price prediction problem:\n",
        "\n",
        "1. **Initial Prediction**:\n",
        "   $F_0(x) = \\text{avg}(\\text{price})$\n",
        "\n",
        "2. **For each iteration** $m = 1$ to $M$:\n",
        "   - Compute residuals: $r_i = y_i - F_{m-1}(x_i)$\n",
        "   - Fit new tree: $h_m(x)$ to predict residuals\n",
        "   - Add scaled prediction: $F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$\n",
        "   \n",
        "   where $\\nu$ is the learning rate (typically 0.1)\n",
        "\n",
        "3. **Final Prediction**:\n",
        "   $F_M(x) = F_0(x) + \\sum_{m=1}^M \\nu \\cdot h_m(x)$\n",
        "\n",
        "This process ensures each new tree focuses on the mistakes of the ensemble so far.\n",
        "\n",
        "### XGBoost innovations\n",
        "\n",
        "XGBoost enhances gradient boosting through several key innovations:\n",
        "\n",
        "1. **Regularised objective**:\n",
        "   ```\n",
        "   Objective = Loss + Regularisation\n",
        "   ```\n",
        "   where regularisation penalises complex trees:\n",
        "   $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$\n",
        "   - $T$ is number of leaves\n",
        "   - $w_j$ are leaf weights\n",
        "   - $\\gamma, \\lambda$ are regularisation parameters\n",
        "\n",
        "2. **Approximate split finding**:\n",
        "   - Groups continuous features into buckets\n",
        "   - Evaluates splits only at bucket boundaries\n",
        "   - Makes training much faster\n",
        "   - Minimal accuracy impact\n",
        "\n",
        "3. **System optimisations**:\n",
        "   - Cache-aware access\n",
        "   - Parallel processing\n",
        "   - Out-of-core computing\n",
        "   - Distributed training support\n",
        "\n",
        "These innovations make XGBoost particularly effective for house price prediction, as it can:\n",
        "- Handle non-linear relationships efficiently\n",
        "- Manage high-dimensional feature spaces\n",
        "- Process large housing datasets quickly\n",
        "- Provide robust predictions with good generalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1hsK0FcvVOf"
      },
      "outputs": [],
      "source": [
        "class GradientBoostingAnalyser:\n",
        "    \"\"\"Demonstrates and analyses gradient boosting behaviour\"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = xgb.XGBRegressor(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            max_depth=3,\n",
        "            reg_lambda=1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def analyse_residuals(self, X, y):\n",
        "        \"\"\"Visualise how boosting reduces residuals over iterations\"\"\"\n",
        "        residuals = []\n",
        "        for i in range(1, self.n_estimators + 1):\n",
        "            model_partial = xgb.XGBRegressor(\n",
        "                n_estimators=i,\n",
        "                learning_rate=self.learning_rate,\n",
        "                max_depth=3,\n",
        "                reg_lambda=1,\n",
        "                random_state=42\n",
        "            )\n",
        "            model_partial.fit(X, y)\n",
        "            pred = model_partial.predict(X)\n",
        "            residuals.append(np.abs(y - pred).mean())\n",
        "        return residuals\n",
        "\n",
        "    def analyse_boosting_progress(self, X, y):\n",
        "        \"\"\"Analyse learning progression with validation\"\"\"\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Convert to DMatrix format for XGBoost\n",
        "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "        dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "        # Set parameters\n",
        "        params = {\n",
        "            'max_depth': 3,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'eval_metric': 'rmse'\n",
        "        }\n",
        "\n",
        "        # Train model with evaluation\n",
        "        evals_result = {}\n",
        "        bst = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=self.n_estimators,\n",
        "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "            evals_result=evals_result,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "\n",
        "        # Extract training and validation RMSE\n",
        "        train_rmse = evals_result['train']['rmse']\n",
        "        val_rmse = evals_result['val']['rmse']\n",
        "\n",
        "        # Plot learning curves\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_rmse, label='Training RMSE')\n",
        "        plt.plot(val_rmse, label='Validation RMSE')\n",
        "        plt.xlabel('Boosting Round')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.title('XGBoost Learning Progress')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        return train_rmse, val_rmse\n",
        "\n",
        "    def analyse_feature_importance(self, X, y):\n",
        "        \"\"\"Analyse and visualise feature importance\"\"\"\n",
        "        # Fit the model\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "        # Get feature importance\n",
        "        importance = self.model.feature_importances_\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': importance\n",
        "        }).sort_values('importance', ascending=True)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(importance_df['feature'], importance_df['importance'])\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title('XGBoost Feature Importance')\n",
        "        plt.show()\n",
        "\n",
        "        return importance_df\n",
        "\n",
        "def analyse_nonlinear_effects(X, y):\n",
        "    \"\"\"Analyse how XGBoost captures non-linear relationships\"\"\"\n",
        "    model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Create grid for partial dependence plots\n",
        "    feature_grids = {\n",
        "        'location_score': np.linspace(0, 1, 100),\n",
        "        'area': np.linspace(1000, 3000, 100)\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot partial dependence for each feature\n",
        "    for i, (feature, grid) in enumerate(feature_grids.items()):\n",
        "        predictions = []\n",
        "        for value in grid:\n",
        "            X_temp = X.copy()\n",
        "            X_temp[feature] = value\n",
        "            pred = model.predict(X_temp)\n",
        "            predictions.append(np.mean(pred))\n",
        "\n",
        "        ax = axes[i]\n",
        "        ax.plot(grid, predictions)\n",
        "        ax.set_title(f'Price vs {feature.replace(\"_\", \" \").title()}')\n",
        "        ax.set_xlabel(feature.replace(\"_\", \" \").title())\n",
        "        ax.set_ylabel('Predicted Price')\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate features\n",
        "X = pd.DataFrame({\n",
        "    'area': np.random.normal(2000, 500, n_samples),\n",
        "    'bedrooms': np.random.randint(1, 6, n_samples),\n",
        "    'location_score': np.random.uniform(0, 1, n_samples),\n",
        "    'age': np.random.randint(0, 50, n_samples),\n",
        "    'distance_to_center': np.random.uniform(1, 20, n_samples)\n",
        "})\n",
        "\n",
        "# Generate target with non-linear effects and interactions\n",
        "y = (\n",
        "    X['area'] * 200 * (1 + X['location_score']) +  # Interaction effect\n",
        "    X['bedrooms'] * 50000 * (1.1 - X['distance_to_center']/20) +  # Another interaction\n",
        "    np.exp(X['location_score'] * 2) * 100000 +  # Non-linear effect\n",
        "    X['age'] * -1000 * (1 + X['location_score']) +  # Age penalty varies by location\n",
        "    np.random.normal(0, 50000, n_samples)  # Random noise\n",
        ")\n",
        "\n",
        "# Initialise analyser\n",
        "analyser = GradientBoostingAnalyser(n_estimators=100)\n",
        "\n",
        "# Analyse boosting behaviour\n",
        "train_rmse, val_rmse = analyser.analyse_boosting_progress(X, y)\n",
        "\n",
        "# Analyse feature importance\n",
        "importance_df = analyser.analyse_feature_importance(X, y)\n",
        "\n",
        "print(\"\\nFinal Performance:\")\n",
        "print(f\"Training RMSE: £{train_rmse[-1]:,.2f}\")\n",
        "print(f\"Validation RMSE: £{val_rmse[-1]:,.2f}\")\n",
        "\n",
        "print(\"\\nFeature Importance Summary:\")\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Analyse non-linear relationships\n",
        "analyse_nonlinear_effects(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_6X2Ro2vVOf"
      },
      "outputs": [],
      "source": [
        "class TreeMethodsComparison:\n",
        "    \"\"\"Comprehensive comparison of tree-based methods for house price prediction\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "        }\n",
        "        self.results = {}\n",
        "\n",
        "    def compare_performance(self, X: pd.DataFrame, y: pd.Series) -> Dict:\n",
        "        \"\"\"Compare performance metrics across models\"\"\"\n",
        "        performance = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            # Cross validation scores\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X, y,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_squared_error'\n",
        "            )\n",
        "            rmse_scores = np.sqrt(-cv_scores)\n",
        "\n",
        "            performance[name] = {\n",
        "                'mean_rmse': rmse_scores.mean(),\n",
        "                'std_rmse': rmse_scores.std(),\n",
        "                'cv_scores': rmse_scores\n",
        "            }\n",
        "\n",
        "        self.results['performance'] = performance\n",
        "        return performance\n",
        "\n",
        "    def compare_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict:\n",
        "        \"\"\"Compare feature importance across models\"\"\"\n",
        "        importance = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            # Fit model\n",
        "            model.fit(X, y)\n",
        "\n",
        "            # Get feature importance\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                importance[name] = dict(zip(\n",
        "                    X.columns,\n",
        "                    model.feature_importances_\n",
        "                ))\n",
        "\n",
        "        self.results['importance'] = importance\n",
        "        return importance\n",
        "\n",
        "    def visualise_results(self):\n",
        "        \"\"\"Create visualisations comparing model performance\"\"\"\n",
        "        # Performance comparison\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot 1: RMSE comparison\n",
        "        plt.subplot(121)\n",
        "        perf = self.results['performance']\n",
        "        models = list(perf.keys())\n",
        "        rmse_means = [p['mean_rmse'] for p in perf.values()]\n",
        "        rmse_stds = [p['std_rmse'] for p in perf.values()]\n",
        "\n",
        "        plt.bar(models, rmse_means, yerr=rmse_stds)\n",
        "        plt.title('RMSE by Model Type')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylabel('RMSE (£)')\n",
        "\n",
        "        # Plot 2: Feature importance comparison\n",
        "        plt.subplot(122)\n",
        "        importance = self.results['importance']\n",
        "        feature_df = pd.DataFrame(importance)\n",
        "\n",
        "        sns.heatmap(feature_df, annot=True, fmt='.2f', cmap='YlOrRd')\n",
        "        plt.title('Feature Importance Comparison')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('Feature')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def predict_price_range(self, X: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Compare prediction ranges across models\"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            pred = model.predict(X)\n",
        "            predictions[name] = pred\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5417Dj5vVOg"
      },
      "outputs": [],
      "source": [
        "# Generate example housing data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Features with realistic relationships\n",
        "X = pd.DataFrame({\n",
        "    'area': np.random.normal(2000, 500, n_samples),\n",
        "    'bedrooms': np.random.randint(1, 6, n_samples),\n",
        "    'location_score': np.random.uniform(0, 1, n_samples),\n",
        "    'age': np.random.randint(0, 50, n_samples),\n",
        "    'distance_to_center': np.random.uniform(1, 20, n_samples)\n",
        "})\n",
        "\n",
        "# Generate prices with non-linear relationships and interactions\n",
        "y = (\n",
        "    X['area'] * 200 * (1 + X['location_score']) +  # Area-location interaction\n",
        "    X['bedrooms'] * 50000 * (1 + 0.5 * X['location_score']) +  # Bedroom-location interaction\n",
        "    np.exp(X['location_score'] * 2) * 100000 +  # Non-linear location effect\n",
        "    X['age'] * -1000 * (1 + X['location_score']) +  # Age penalty varies by location\n",
        "    np.random.normal(0, 50000, n_samples)  # Random noise\n",
        ")\n",
        "\n",
        "# Initialise comparison\n",
        "comparison = TreeMethodsComparison()\n",
        "\n",
        "# Compare performance\n",
        "performance = comparison.compare_performance(X, y)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "for model, metrics in performance.items():\n",
        "    print(f\"\\n{model}:\")\n",
        "    print(f\"Mean RMSE: £{metrics['mean_rmse']:,.2f} (±{metrics['std_rmse']:,.2f})\")\n",
        "\n",
        "# Compare feature importance\n",
        "importance = comparison.compare_feature_importance(X, y)\n",
        "print(\"\\nFeature Importance by Model:\")\n",
        "importance_df = pd.DataFrame(importance)\n",
        "print(importance_df)\n",
        "\n",
        "# Visualise comparisons\n",
        "comparison.visualise_results()\n",
        "\n",
        "# Compare predictions for specific examples\n",
        "test_cases = pd.DataFrame({\n",
        "    'area': [1500, 2500, 3500],\n",
        "    'bedrooms': [2, 3, 4],\n",
        "    'location_score': [0.3, 0.7, 0.9],\n",
        "    'age': [20, 10, 5],\n",
        "    'distance_to_center': [15, 8, 3]\n",
        "})\n",
        "\n",
        "predictions = comparison.predict_price_range(test_cases)\n",
        "print(\"\\nPrediction Comparison for Test Cases:\")\n",
        "pred_df = pd.DataFrame(predictions)\n",
        "print(pred_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFR7SPs4vVOg"
      },
      "source": [
        "<a name=\"ethical-considerations-for-decision-tree-models\"></a>\n",
        "## Ethical Considerations for Decision Tree Models\n",
        "\n",
        "When applying decision trees to make predictions, we must carefully consider the ethical and societal implications of our models.\n",
        "\n",
        "### 1. Bias in training data\n",
        "\n",
        "#### Understanding data bias\n",
        "\n",
        "Historical housing data often reflects societal inequalities and biases:\n",
        "- Certain areas may be over or under-represented\n",
        "- Quality of data may vary by neighborhood\n",
        "- Historical redlining effects may persist in the data\n",
        "- Property features may be inconsistently recorded across areas\n",
        "\n",
        "#### Example of data bias\n",
        "Consider two neighborhoods:\n",
        "\n",
        "**Affluent area:**\n",
        "- 1000+ property records\n",
        "- Complete feature sets (area, condition, amenities)\n",
        "- Regular price updates\n",
        "- Detailed property descriptions\n",
        "\n",
        "**Developing area:**\n",
        "- Only 100 property records\n",
        "- Missing features\n",
        "- Irregular price updates\n",
        "- Basic property information only\n",
        "\n",
        "This disparity in data quality and quantity can lead to:\n",
        "- Less accurate predictions in underrepresented areas\n",
        "- Reinforcement of existing price disparities\n",
        "- Lower confidence in predictions for certain areas\n",
        "\n",
        "#### Mitigation strategies\n",
        "\n",
        "1. **Data collection**\n",
        "   - Actively gather data from underrepresented areas\n",
        "   - Standardise data collection across all neighborhoods\n",
        "   - Partner with community organisations for local insights\n",
        "\n",
        "2. **Model development**\n",
        "   - Weight samples to balance representation\n",
        "   - Use stratified sampling across neighborhoods\n",
        "   - Include confidence intervals with predictions\n",
        "\n",
        "3. **Regular auditing**\n",
        "   - Monitor prediction accuracy across different areas\n",
        "   - Track error rates by neighborhood\n",
        "   - Assess impact on different communities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RodRjy85vVOg"
      },
      "source": [
        "### 2. Fairness and discrimination\n",
        "\n",
        "#### Protected characteristics\n",
        "\n",
        "Decision trees must not perpetuate discrimination based on:\n",
        "- Race, ethnicity, or national origin\n",
        "- Religion\n",
        "- Gender\n",
        "- Age\n",
        "- Disability status\n",
        "- Family status\n",
        "\n",
        "#### Direct and indirect bias\n",
        "\n",
        "Consider these two approaches:\n",
        "\n",
        "**Problematic approach:**\n",
        "```\n",
        "If neighborhood = \"historically_disadvantaged\":\n",
        "    Predict lower value\n",
        "```\n",
        "\n",
        "**Better approach:**\n",
        "```\n",
        "If distance_to_amenities < 1km:\n",
        "    If property_condition = \"excellent\":\n",
        "        Predict based on objective features\n",
        "```\n",
        "\n",
        "The second approach uses objective criteria rather than potentially biased historical patterns.\n",
        "\n",
        "#### Monitoring for fairness\n",
        "\n",
        "1. Track prediction ratios across different groups\n",
        "2. Compare error rates between communities\n",
        "3. Analyse the impact of model updates on different areas\n",
        "4. Review feature importance for potential proxy discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVUssu6vvVOg"
      },
      "outputs": [],
      "source": [
        "class FairnessMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = {}\n",
        "        self.error_distributions = {}\n",
        "\n",
        "    def analyse_predictions(self, y_true, y_pred, groups):\n",
        "        \"\"\"Analyse prediction fairness across different groups\"\"\"\n",
        "        group_metrics = {}\n",
        "\n",
        "        for group_name in groups.unique():\n",
        "            mask = groups == group_name\n",
        "\n",
        "            # Calculate metrics for this group\n",
        "            metrics = {\n",
        "                'count': sum(mask),\n",
        "                'mean_error': np.mean(np.abs(y_true[mask] - y_pred[mask])),\n",
        "                'mape': mean_absolute_percentage_error(y_true[mask], y_pred[mask]),\n",
        "                'avg_prediction': np.mean(y_pred[mask]),\n",
        "                'avg_actual': np.mean(y_true[mask])\n",
        "            }\n",
        "\n",
        "            group_metrics[group_name] = metrics\n",
        "\n",
        "        return pd.DataFrame(group_metrics).T\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate sample data\n",
        "data = {\n",
        "    'actual_price': np.random.lognormal(12, 0.5, n_samples),\n",
        "    'predicted_price': np.random.lognormal(12, 0.5, n_samples),\n",
        "    'neighborhood': np.random.choice(['A', 'B', 'C'], n_samples,\n",
        "                                    p=[0.5, 0.3, 0.2])\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Add some systematic bias\n",
        "bias_factor = {\n",
        "    'A': 1.0,   # No bias\n",
        "    'B': 1.1,   # 10% overprediction\n",
        "    'C': 0.9    # 10% underprediction\n",
        "}\n",
        "\n",
        "for neighborhood, factor in bias_factor.items():\n",
        "    mask = df['neighborhood'] == neighborhood\n",
        "    df.loc[mask, 'predicted_price'] *= factor\n",
        "\n",
        "# Analyse fairness\n",
        "monitor = FairnessMonitor()\n",
        "fairness_metrics = monitor.analyse_predictions(\n",
        "    df['actual_price'],\n",
        "    df['predicted_price'],\n",
        "    df['neighborhood']\n",
        ")\n",
        "\n",
        "print(\"Fairness Analysis by Neighborhood:\")\n",
        "print(fairness_metrics.round(2))\n",
        "\n",
        "# Visualise error distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "for neighborhood in df['neighborhood'].unique():\n",
        "    mask = df['neighborhood'] == neighborhood\n",
        "    errors = (df.loc[mask, 'predicted_price'] -\n",
        "             df.loc[mask, 'actual_price']) / df.loc[mask, 'actual_price']\n",
        "    plt.hist(errors, alpha=0.5, label=f'Neighborhood {neighborhood}',\n",
        "             bins=30)\n",
        "\n",
        "plt.title('Prediction Error Distribution by Neighborhood')\n",
        "plt.xlabel('Relative Error')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpvZcDNBvVOg"
      },
      "source": [
        "### 3. Market Impact and social responsibility\n",
        "\n",
        "#### Housing market effects\n",
        "\n",
        "Our models can influence:\n",
        "1. **Buyer behaviour**\n",
        "   - Setting price expectations\n",
        "   - Influencing negotiation starting points\n",
        "   - Affecting perceived neighborhood value\n",
        "\n",
        "2. **Market dynamics**\n",
        "   - Property valuation standards\n",
        "   - Investment patterns\n",
        "   - Neighborhood development\n",
        "\n",
        "3. **Housing accessibility**\n",
        "   - Affordability assessments\n",
        "   - Mortgage approvals\n",
        "   - Insurance rates\n",
        "\n",
        "#### Responsible implementation\n",
        "1. **Transparency**\n",
        "   - Clearly explain model limitations\n",
        "   - Provide confidence intervals\n",
        "   - Document all assumptions\n",
        "   - Share key factors affecting predictions\n",
        "\n",
        "2. **Community impact**\n",
        "   - Engage with local stakeholders\n",
        "   - Consider neighborhood stability\n",
        "   - Monitor displacement risks\n",
        "   - Support housing accessibility\n",
        "\n",
        "3. **Market stability**\n",
        "   - Avoid reinforcing speculation\n",
        "   - Maintain price prediction stability\n",
        "   - Consider local market conditions\n",
        "   - Support sustainable growth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR8nVpwXvVOg"
      },
      "source": [
        "### 4. Best practices for ethical use\n",
        "\n",
        "#### Development guidelines\n",
        "\n",
        "1. **Data collection**\n",
        "   - Ensure representative samples\n",
        "   - Document data sources\n",
        "   - Validate data quality\n",
        "   - Address historical biases\n",
        "\n",
        "2. **Model design**\n",
        "   - Use interpretable features\n",
        "   - Avoid proxy discrimination\n",
        "   - Include uncertainty measures\n",
        "   - Document design choices\n",
        "\n",
        "3. **Testing and validation**\n",
        "   - Test across diverse scenarios\n",
        "   - Validate with community input\n",
        "   - Monitor for unintended consequences\n",
        "   - Regular fairness audits\n",
        "\n",
        "#### Deployment considerations\n",
        "1. **Model release**\n",
        "   - Gradual rollout\n",
        "   - Monitor impact\n",
        "   - Gather feedback\n",
        "   - Ready to adjust\n",
        "\n",
        "2. **Ongoing oversight**\n",
        "   - Regular audits\n",
        "   - Community feedback\n",
        "   - Impact assessment\n",
        "   - Update protocols\n",
        "\n",
        "#### Documentation requirements\n",
        "\n",
        "Your model documentation should include:\n",
        "1. Training data sources and limitations\n",
        "2. Feature selection rationale\n",
        "3. Fairness considerations and tests\n",
        "4. Known biases and limitations\n",
        "5. Intended use guidelines\n",
        "6. Impact monitoring plan\n",
        "\n",
        "Ethical considerations aren't just a compliance checklist—they're fundamental to building models that serve society fairly and responsibly. Regular review and adjustment of these practices ensures our models contribute positively to communities in which they make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23ccKdF9vVOg"
      },
      "outputs": [],
      "source": [
        "class ModelCard:\n",
        "    def __init__(self, model_name, version, purpose):\n",
        "        self.model_name = model_name\n",
        "        self.version = version\n",
        "        self.purpose = purpose\n",
        "        self.creation_date = datetime.now()\n",
        "        self.model_details = {}\n",
        "        self.ethical_considerations = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    def add_data_description(self, data_description):\n",
        "        self.data_description = data_description\n",
        "\n",
        "    def add_model_details(self, architecture, parameters):\n",
        "        self.model_details.update({\n",
        "            'architecture': architecture,\n",
        "            'parameters': parameters\n",
        "        })\n",
        "\n",
        "    def add_ethical_considerations(self, considerations):\n",
        "        self.ethical_considerations = considerations\n",
        "\n",
        "    def add_performance_metrics(self, metrics):\n",
        "        self.performance_metrics = metrics\n",
        "\n",
        "    def generate_documentation(self):\n",
        "        doc = f\"\"\"# Model Card: {self.model_name} v{self.version}\n",
        "\n",
        "## Basic Information\n",
        "- **Purpose**: {self.purpose}\n",
        "- **Created**: {self.creation_date.strftime('%Y-%m-%d')}\n",
        "\n",
        "## Model Details\n",
        "- **Architecture**: {self.model_details.get('architecture', 'Not specified')}\n",
        "- **Parameters**: {json.dumps(self.model_details.get('parameters', {}), indent=2)}\n",
        "\n",
        "## Ethical Considerations\n",
        "\"\"\"\n",
        "        for category, details in self.ethical_considerations.items():\n",
        "            doc += f\"\\n### {category}\\n\"\n",
        "            for item in details:\n",
        "                doc += f\"- {item}\\n\"\n",
        "\n",
        "        doc += \"\\n## Performance Metrics\\n\"\n",
        "        for metric, value in self.performance_metrics.items():\n",
        "            doc += f\"- **{metric}**: {value}\\n\"\n",
        "\n",
        "        return doc\n",
        "\n",
        "# Example usage\n",
        "model_card = ModelCard(\n",
        "    model_name=\"London Housing Price Predictor\",\n",
        "    version=\"1.0\",\n",
        "    purpose=\"Predict house prices in London with ethical considerations\"\n",
        ")\n",
        "\n",
        "model_card.add_model_details(\n",
        "    architecture=\"Decision Tree Regressor\",\n",
        "    parameters={\n",
        "        'max_depth': 5,\n",
        "        'min_samples_leaf': 50,\n",
        "        'criterion': 'squared_error'\n",
        "    }\n",
        ")\n",
        "\n",
        "model_card.add_ethical_considerations({\n",
        "    \"Fairness\": [\n",
        "        \"Model tested for bias across different neighborhoods\",\n",
        "        \"Regular monitoring of prediction disparities\",\n",
        "        \"Balanced training data across areas\"\n",
        "    ],\n",
        "    \"Transparency\": [\n",
        "        \"Feature importance publicly available\",\n",
        "        \"Confidence intervals provided with predictions\",\n",
        "        \"Clear documentation of limitations\"\n",
        "    ],\n",
        "    \"Social Impact\": [\n",
        "        \"Monthly monitoring of market impact\",\n",
        "        \"Community feedback integration\",\n",
        "        \"Regular updates to prevent perpetuating biases\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "model_card.add_performance_metrics({\n",
        "    'MAE': '£50,000',\n",
        "    'R² Score': '0.85',\n",
        "    'Cross-Validation Score': '0.83 ±0.02',\n",
        "    'Fairness Disparity': '<10% across neighborhoods'\n",
        "})\n",
        "\n",
        "# Generate and print documentation\n",
        "print(model_card.generate_documentation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekBt_h4rvVOh"
      },
      "source": [
        "<a name=\"theory-conclusion\"></a>\n",
        "## Theory conclusion\n",
        "\n",
        "Now that we've explored the key concepts behind decision trees, let's summarise the main points and how they apply to our task:\n",
        "\n",
        "### Core concepts\n",
        "\n",
        "1. **Regression trees vs classification trees**\n",
        "   - For house price prediction, we use regression trees\n",
        "   - Unlike classification trees (Gini impurity/entropy), regression trees minimise variance in a target variable (house prices)\n",
        "   - Different metrics for different tasks:\n",
        "     - MSE for regression\n",
        "     - Gini/Entropy for classification\n",
        "\n",
        "2. **Splitting criterion**\n",
        "   - Regression trees use reduction in Mean Squared Error (MSE)\n",
        "   - At each node, algorithm chooses split maximising reduction:\n",
        "\n",
        "   $\\Delta MSE = MSE_{parent} - (w_{left} * MSE_{left} + w_{right} * MSE_{right})$\n",
        "\n",
        "   Where $w_{left}$ and $w_{right}$ are the proportions of samples in left and right child nodes\n",
        "\n",
        "3. **Recursive splitting**\n",
        "   - The tree is built by recursively applying the splitting process\n",
        "   - This creates a hierarchy of decision rules\n",
        "   - The algorithm will continues until a stopping condition is met:\n",
        "     - Maximum tree depth reached\n",
        "     - Minimum samples per leaf achieved\n",
        "     - No further improvement possible\n",
        "\n",
        "4. **Prediction process**\n",
        "   - Follow decision rules from root to leaf node\n",
        "   - Prediction is mean price of houses in leaf node\n",
        "   - Clear, interpretable decision path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3ZI_unvVOh"
      },
      "source": [
        "### Data handling and model characteristics\n",
        "\n",
        "5. **Data preparation**\n",
        "   - Numerical features: Trees can use directly without transformation\n",
        "   - Categorical features require encoding:\n",
        "     - One-hot encoding for low-cardinality\n",
        "     - Target encoding for high-cardinality\n",
        "     - Ordinal encoding for ordered categories\n",
        "   - Binary features: Simple 1/0 encoding\n",
        "\n",
        "6. **Interpretability**\n",
        "   - An advantage is we can visualise the tree and follow the decision path\n",
        "   - This allows insights into feature importance\n",
        "   - Trees follow clear decision rules for predictions\n",
        "   - Natural feature selection occurs through split choices\n",
        "\n",
        "7. **Bias-variance trade-off**\n",
        "   - Deeper trees: More complex relationships but risk overfitting (high variance)\n",
        "   - Shallower trees: More generalisable but may oversimplify (high bias)\n",
        "   - Balance crucial for optimal performance\n",
        "   - Cross-validation helps find optimal depth\n",
        "\n",
        "8. **Feature importance**\n",
        "   - Natural feature selection occurs through tree construction\n",
        "   - More important features appear:\n",
        "     - Higher in tree\n",
        "     - In more splits\n",
        "     - Provide the largest reductions in impurity\n",
        "\n",
        "9. **Advanced capabilities**\n",
        "   - Trees handle non-linear relationships well unlike linear regression\n",
        "   - They capture complex interactions between features\n",
        "   - No feature scaling required\n",
        "   - Modern tree libraries can naturally handle missing values\n",
        "\n",
        "10. **Limitations and solutions**\n",
        "    - Instability: Small data changes can result in very different trees\n",
        "    - Solution: Ensemble methods like Random Forests\n",
        "    - Trees struggles with smooth, linear relationships\n",
        "    - Tress can have limited extrapolation capability\n",
        "    - Biased trees can be created if the data is unbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD7Ez7-1vVOh"
      },
      "source": [
        "### Error metrics and evaluation\n",
        "\n",
        "11. **Understanding error metrics**\n",
        "    - Training uses MSE for splitting decisions\n",
        "    - Evaluation often uses MAE for interpretability\n",
        "    - MSE formula for node impurity:\n",
        "      $\\ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2 $\n",
        "\n",
        "<a name=\"looking-ahead\"></a>\n",
        "## Looking ahead to Lesson 2B: Decision Trees London Housing Practical\n",
        "\n",
        "In our next lesson, we'll translate these theoretical foundations into practical implementations, using decision trees to predict house prices in the London market.\n",
        "\n",
        "While our exploration of the mathematical principles has given us deep insights into how decision trees work, we'll now see how modern frameworks can help us build robust models for real estate valuation.\n",
        "\n",
        "We'll focus on practical aspects including:\n",
        "\n",
        "1. Exploring and applying tree-based methods to a real dataset\n",
        "2. Implementing optimal decision trees using scikit-learn\n",
        "3. Tuning hyperparameters for maximum prediction accuracy\n",
        "4. Building advanced tree-based models like Random Forests and XGBoost\n",
        "5. Interpreting and visualising tree decisions for house prices\n",
        "6. Understanding feature importance for property valuation\n",
        "7. Handling real-world data challenges\n",
        "8. Preparing models for production deployment\n",
        "9. Robust validation and testing\n",
        "\n",
        "As we move forward to apply these concepts to our London housing dataset, remember that while theory provides the foundation, the real insights come from experimenting with the data, tuning the model, and interpreting the results in the context of datasets!\n",
        "\n",
        "### Next lesson: [2b_decision_trees_practical.ipynb](./2b_decision_trees_practical.ipynb)\n",
        "\n",
        "<a name=\"further-reading\"></a>\n",
        "### Further reading\n",
        "\n",
        "For those interested in deepening their understanding of decision tree theory, we recommend these carefully curated resources:\n",
        "\n",
        "**Core decision tree theory**\n",
        "\n",
        "- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) by Hastie, Tibshirani, and Friedman\n",
        "  - Chapter 9 provides comprehensive coverage of decision trees\n",
        "  - Explains splitting criteria and tree construction in detail\n",
        "  - Mathematical foundation for advanced tree methods\n",
        "\n",
        "- [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) by Christopher Bishop\n",
        "  - Section 14.4 offers rigorous treatment of decision trees\n",
        "  - Clear explanation of information gain and entropy\n",
        "  - Links trees to probabilistic frameworks\n",
        "\n",
        "**Theoretical foundations**\n",
        "\n",
        "- [Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/mackay/itila/book.html) by David MacKay\n",
        "  - Fundamental principles behind tree-based learning\n",
        "  - Information theoretic perspective on splitting criteria\n",
        "  - Mathematical treatment of decision boundaries\n",
        "\n",
        "- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/) by Mohri, Rostamizadeh, and Talwalkar\n",
        "  - Theoretical guarantees for decision trees\n",
        "  - Computational complexity analysis\n",
        "  - Statistical learning theory perspective\n",
        "\n",
        "### Thanks for learning!\n",
        "\n",
        "This notebook is part of the Supervised Machine Learning from First Principles series.\n",
        "\n",
        "© 2025 Powell-Clark Limited. Licensed under Apache License 2.0.\n",
        "\n",
        "If you found this helpful, please cite as:\n",
        "```\n",
        "Powell-Clark (2025). Supervised Machine Learning from First Principles.\n",
        "GitHub: https://github.com/powell-clark/supervised-machine-learning\n",
        "```\n",
        "\n",
        "Questions or feedback? Contact emmanuel@powellclark.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}