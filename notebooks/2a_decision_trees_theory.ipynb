{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Decision Trees for House Price Prediction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Decision trees are a versatile machine learning model for both classification and regression tasks. In this lesson, we'll use decision trees to predict house prices based on features like location, size, and amenities.\n",
    "\n",
    "Imagine you're a real estate agent trying to estimate the fair price of a house based on its characteristics. This is where decision trees can help. They learn a set of rules from historical data to make predictions on new, unseen houses.\n",
    "\n",
    "Essentially, a decision tree is used to make predictions on the target variable - say price - by recursively splitting the data based on the values of the features, choosing splits that maximize the similarity of the target variable (prices) within each subset.\n",
    "\n",
    "The result is a tree-like model of decisions and their consequences.\n",
    "\n",
    "By the end of this lesson, you'll understand how decision trees work, how to train and interpret them, and how they compare to other models for regression tasks.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Intuition Behind Decision Trees](#Intuition-Behind-Decision-Trees)\n",
    "   - [Why Choose Decision Trees for House Prices?](#Why-Choose-Decision-Trees-for-House-Prices)\n",
    "3. [Anatomy of a Decision Tree](#Anatomy-of-a-Decision-Tree)\n",
    "4. [Splitting Criteria Explained](#Splitting-Criteria-Explained)\n",
    "   - [For Regression Tasks (e.g., Predicting House Prices)](#For-Regression-Tasks-eg-Predicting-House-Prices)\n",
    "     - [Mean Squared Error (MSE)](#Mean-Squared-Error-MSE)\n",
    "     - [Evaluating Decision Points: Understanding Split Quality in Decision Trees](#Evaluating-Decision-Points-Understanding-Split-Quality-in-Decision-Trees)\n",
    "   - [For Classification Tasks (e.g., Predicting if a House Will Sell Quickly)](#For-Classification-Tasks-eg-Predicting-if-a-House-Will-Sell-Quickly)\n",
    "     - [Gini Impurity](#Gini-Impurity)\n",
    "     - [Entropy](#Entropy)\n",
    "     - [Information Gain](#Information-Gain)\n",
    "       - [Comparison: Splits with Different Information Gains](#Comparison-Splits-with-Different-Information-Gains)\n",
    "5. [Mean Squared Error (MSE) vs Mean Absolute Error (MAE)](#Mean-Squared-Error-MSE-vs-Mean-Absolute-Error-MAE)\n",
    "6. [Theory Conclusion](#Theory-Conclusion)\n",
    "\n",
    "\n",
    "## Intuition Behind Decision Trees\n",
    "\n",
    "Imagine you're trying to predict the price of a house based on its features. You might start by asking broad questions like \"Is it in a desirable location?\" and then progressively get more specific: \"How many bedrooms does it have? What's the square footage?\".\n",
    "\n",
    "At each step, you're trying to split the houses into groups that are as similar as possible in terms of price. This is exactly how a decision tree works - it asks a series of questions about the features, each time trying to split the data into more homogeneous subsets.\n",
    "\n",
    "### Why Choose Decision Trees for House Prices?\n",
    "\n",
    "Decision trees are particularly well-suited for this task because of several key advantages that become apparent when comparing them to other popular algorithms:\n",
    "\n",
    "1. **Natural Handling of Mixed Data Types**\n",
    "   While algorithms like Linear Regression and Neural Networks require categorical variables to be encoded numerically, and K-Nearest Neighbors struggles with mixed data types, decision trees naturally handle our varied real estate data:\n",
    "   - Numerical: Price (£180,000 to £39,750,000), square footage (274 to 15,405 sq ft)\n",
    "   - Categorical: Location (\"Chelsea\", \"Hackney\"), house type (\"Flat\", \"House\", \"Penthouse\")\n",
    "   - Ordinal: Number of bedrooms (1-10), bathrooms (1-10), receptions (1-10)\n",
    "\n",
    "2. **No Feature Scaling Required**\n",
    "   Unlike many other algorithms, decision trees work with raw values directly. \n",
    "   \n",
    "   Compare this to:\n",
    "   - Linear/Logistic Regression: Requires scaling to prevent features with larger values from dominating the model\n",
    "   - Neural Networks: Needs normalized inputs (usually between 0-1) for stable gradient descent\n",
    "   - Support Vector Machines (SVM): Highly sensitive to feature scales, requires standardization\n",
    "   - K-Nearest Neighbors: Distance calculations are skewed by different scales, needs normalization\n",
    "\n",
    "   The tree makes splits based on relative ordering, not absolute values. \n",
    "   \n",
    "   For example, these splits are all equivalent to a decision tree:\n",
    "   ```python\n",
    "   # Original scale (Decision Tree works fine)\n",
    "   if square_footage > 2000:\n",
    "       predict_price = 1200000\n",
    "   else:\n",
    "       predict_price = 800000\n",
    "\n",
    "   # Scaled by 1000 (needed for Neural Networks)\n",
    "   if square_footage/1000 > 2:  # Same result for decision tree\n",
    "       predict_price = 1200000\n",
    "   else:\n",
    "       predict_price = 800000\n",
    "\n",
    "   # Standardized (needed for SVM)\n",
    "   if (square_footage - mean)/std > 1.2:  # Same result for decision tree\n",
    "       predict_price = 1200000\n",
    "   else:\n",
    "       predict_price = 800000\n",
    "   ```\n",
    "\n",
    "3. **Interpretable Decision Making**\n",
    "   While algorithms like Neural Networks act as \"black boxes\" and Linear Regression gives abstract coefficients, decision trees create clear, actionable rules:\n",
    "   ```python\n",
    "   if location == \"Chelsea\":\n",
    "       if square_footage > 2000:\n",
    "           predict_price = \"£2.5M\"\n",
    "       else:\n",
    "           predict_price = \"£1.2M\"\n",
    "   elif location == \"Hackney\":\n",
    "       if bedrooms > 3:\n",
    "           predict_price = \"£950K\"\n",
    "       else:\n",
    "           predict_price = \"£650K\"\n",
    "   ```\n",
    "   These rules are easy to explain to stakeholders, unlike trying to interpret neural network weights or SVM kernel transformations.\n",
    "\n",
    "4. **Handling Missing Data**\n",
    "   Real estate data often has missing values.\n",
    "   \n",
    "   While Linear Regression, Neural Networks, and SVM typically require imputation or removal of missing values, decision trees can handle this through:\n",
    "   - Using a technique called surrogate splits based on correlated features\n",
    "   - Making predictions even when some feature values are unknown\n",
    "   - Maintaining accuracy with incomplete information\n",
    "\n",
    "These advantages mean we can focus on understanding the relationships in our data rather than spending time on data transformation and scaling. \n",
    "\n",
    "Other algorithms would require significant preprocessing:\n",
    "- Linear Regression: Feature scaling + encoding categoricals + handling missing values\n",
    "- Neural Networks: Normalization + one-hot encoding + imputation\n",
    "- SVM: Standardization + kernel selection + handling categoricals\n",
    "- KNN: Feature scaling + careful handling of mixed data types\n",
    "\n",
    "This makes decision trees an excellent choice for our house price prediction task, especially when interpretability and ease of use are priorities.\n",
    "\n",
    "## Anatomy of a Decision Tree\n",
    "\n",
    "A decision tree is composed of:\n",
    "\n",
    "- Nodes: Where a feature is tested\n",
    "- Edges: The outcomes of the test\n",
    "- Leaves: Terminal nodes that contain the final predictions\n",
    "\n",
    "A simplified example of a house prices prediction decision tree might look like this:\n",
    "\n",
    "![structure of a house prices prediction decision tree](../static/house-prices-decision-tree-and-structure.png)\n",
    "\n",
    "The tree is built by splitting the data recursively, choosing at each step the feature and split point that results in the greatest reduction in impurity or error.\n",
    "\n",
    "## Splitting Criteria Explained:\n",
    "\n",
    "When building a decision tree, we need a way to determine the best feature and value to split on at each node. The goal is to create child nodes that are more \"pure\" or homogeneous than their parent node. The method for measuring this purity and choosing the best split differs between regression and classification tasks.\n",
    "\n",
    "### For Regression Tasks (e.g., Predicting House Prices):\n",
    "\n",
    "In regression problems, we're trying to predict a continuous value, like house prices. The goal is to split the data in a way that minimizes the variance of the target variable within each resulting group.\n",
    "\n",
    "The most common metric used for regression trees is the Mean Squared Error (MSE). This is the default criterion used by scikit-learn's DecisionTreeRegressor. Let's break down how this works:\n",
    "\n",
    "Imagine you're a real estate agent with a magical ability to instantly sort houses. Your goal? To group similar houses together as efficiently as possible. This is essentially what a decision tree does, but instead of magical powers, it uses mathematics. Let's dive in!\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "Imagine you're playing a house price guessing game. Your goal is to guess the prices of houses as accurately as possible.\n",
    "\n",
    "Let's say we have 5 houses, and their actual prices are:\n",
    "```\n",
    "House 1: £200,000\n",
    "House 2: £250,000\n",
    "House 3: £180,000\n",
    "House 4: £220,000\n",
    "House 5: £300,000\n",
    "```\n",
    "\n",
    "#### Step 1: Calculate the average price\n",
    "`(200,000 + 250,000 + 180,000 + 220,000 + 300,000) / 5 = £230,000`\n",
    "\n",
    "So, your guess for any house would be £230,000.\n",
    "\n",
    "#### Step 2: Calculate how wrong you are for each house\n",
    "```\n",
    "House 1: 230,000 - 200,000 = 30,000 \n",
    "House 2: 230,000 - 250,000 = -20,000\n",
    "House 3: 230,000 - 180,000 = 50,000\n",
    "House 4: 230,000 - 220,000 = 10,000\n",
    "House 5: 230,000 - 300,000 = -70,000\n",
    "```\n",
    "\n",
    "#### Step 3: Square these differences\n",
    "```\n",
    "House 1: 30,000² = 900,000,000\n",
    "House 2: (-20,000)² = 400,000,000\n",
    "House 3: 50,000² = 2,500,000,000\n",
    "House 4: 10,000² = 100,000,000\n",
    "House 5: (-70,000)² = 4,900,000,000\n",
    "```\n",
    "#### Step 4: Add up all these squared differences\n",
    "`\n",
    "900,000,000 + 400,000,000 + 2,500,000,000 + 100,000,000 + 4,900,000,000 = 8,800,000,000\n",
    "`\n",
    "#### Step 5: Divide by the number of houses\n",
    "\n",
    "`8,800,000,000 ÷ 5 = 1,760,000,000`\n",
    "\n",
    "This final number, 1,760,000,000, is your Mean Squared Error (MSE).\n",
    "\n",
    "In mathematical notation, this whole process looks like:\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2$\n",
    "\n",
    "Let's break this down:\n",
    "- $n$ is the number of houses (5 in our example)\n",
    "- $y_i$ is the actual price of each house\n",
    "- $\\hat{y}$ is your guess (the average price, £230,000 in our example)\n",
    "- $\\sum_{i=1}^n$ means \"add up the following calculation for each house from the first to the last\"\n",
    "- The $i$ in $y_i$ is just a counter, going from 1 to $n$ (1 to 5 in our example)\n",
    "\n",
    "As a python function, this would look like:\n",
    "```\n",
    "def calculate_mse(actual_prices, predicted_price):\n",
    "    n = len(actual_prices)\n",
    "    squared_errors = []\n",
    "    \n",
    "    for actual_price in actual_prices:\n",
    "        error = predicted_price - actual_price\n",
    "        squared_error = error ** 2\n",
    "        squared_errors.append(squared_error)\n",
    "    \n",
    "    mse = sum(squared_errors) / n\n",
    "    return mse\n",
    "\n",
    "# Example usage\n",
    "actual_prices = [200000, 250000, 180000, 220000, 300000]\n",
    "predicted_price = sum(actual_prices) / len(actual_prices)  # Average price\n",
    "\n",
    "mse = calculate_mse(actual_prices, predicted_price)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "```\n",
    "\n",
    "### Evaluating Decision Points: Understanding Split Quality in Decision Trees\n",
    "\n",
    "Now, when we split our houses into two groups, we want to measure if this split has made our predictions better. We do this by comparing the error before and after splitting using this formula:\n",
    "\n",
    "$\\Delta MSE = MSE_{before} - (({\\text{fraction of houses in left group} \\times MSE_{left}} + {\\text{fraction of houses in right group} \\times MSE_{right}}))$\n",
    "\n",
    "Let's work through a real example to understand this:\n",
    "\n",
    "Imagine we have 5 houses with these prices:\n",
    "```\n",
    "House 1: £200,000\n",
    "House 2: £250,000\n",
    "House 3: £180,000\n",
    "House 4: £220,000\n",
    "House 5: £300,000\n",
    "```\n",
    "\n",
    "We're considering splitting these houses based on whether they have more than 2 bedrooms:\n",
    "- Left group (≤2 bedrooms): Houses 1, 3 (£200,000, £180,000)\n",
    "- Right group (>2 bedrooms): Houses 2, 4, 5 (£250,000, £220,000, £300,000)\n",
    "\n",
    "#### 1. First, let's calculate $MSE_{before}$\n",
    "```\n",
    "Mean price = (200k + 250k + 180k + 220k + 300k) ÷ 5 = £230,000\n",
    "\n",
    "Squared differences from mean:\n",
    "House 1: (230k - 200k)² = 900,000,000\n",
    "House 2: (230k - 250k)² = 400,000,000\n",
    "House 3: (230k - 180k)² = 2,500,000,000\n",
    "House 4: (230k - 220k)² = 100,000,000\n",
    "House 5: (230k - 300k)² = 4,900,000,000\n",
    "\n",
    "MSE_before = (900M + 400M + 2,500M + 100M + 4,900M) ÷ 5\n",
    "           = 1,760,000,000\n",
    "```\n",
    "\n",
    "#### 2. Now for the left group (≤2 bedrooms):\n",
    "```\n",
    "Mean price = (200k + 180k) ÷ 2 = £190,000\n",
    "\n",
    "Squared differences:\n",
    "House 1: (190k - 200k)² = 100,000,000\n",
    "House 3: (190k - 180k)² = 100,000,000\n",
    "\n",
    "MSE_left = (100M + 100M) ÷ 2 = 100,000,000\n",
    "```\n",
    "\n",
    "#### 3. And the right group (>2 bedrooms):\n",
    "```\n",
    "Mean price = (250k + 220k + 300k) ÷ 3 = £256,667\n",
    "\n",
    "Squared differences:\n",
    "House 2: (256.67k - 250k)² = 44,448,889\n",
    "House 4: (256.67k - 220k)² = 1,344,448,889\n",
    "House 5: (256.67k - 300k)² = 1,877,778,889\n",
    "\n",
    "MSE_right = (44.45M + 1,344.45M + 1,877.78M) ÷ 3 = 1,088,892,222\n",
    "```\n",
    "\n",
    "#### 4. Finally, let's put it all together:\n",
    "```\n",
    "ΔMSE = MSE_before - ((2/5 × MSE_left) + (3/5 × MSE_right))\n",
    "\n",
    "The second part calculates our weighted mean MSE after splitting:\n",
    "\n",
    "- Left group has 2/5 of the houses, so we multiply its MSE by 2/5\n",
    "- Right group has 3/5 of the houses, so we multiply its MSE by 3/5\n",
    "\n",
    "This weighting ensures each house contributes equally to our final calculation.\n",
    "\n",
    "Let's solve it:\n",
    "     = 1,760,000,000 - ((2/5 × 100,000,000) + (3/5 × 1,088,892,222))\n",
    "     = 1,760,000,000 - (40,000,000 + 653,335,333)\n",
    "     = 1,760,000,000 - 693,335,333        # This is our weighted mean MSE after splitting\n",
    "     = 1,066,664,667                      # ΔMSE: The reduction in prediction error\n",
    "\n",
    "The ΔMSE (1,066,664,667) represents the difference between the original MSE and the weighted average MSE after splitting. This number is always non-negative due to a fundamental property of squared errors:\n",
    "\n",
    "1. MSE is always positive (we're squaring differences from the mean)\n",
    "2. When we split a group:\n",
    "   - The parent uses one mean for all samples\n",
    "   - Each subgroup uses its own mean, which minimises squared errors for that subgroup\n",
    "   - The subgroup means must perform at least as well as the parent mean (due to minimising squared errors locally)\n",
    "   - Therefore, the weighted average MSE of subgroups cannot exceed the parent MSE\n",
    "\n",
    "Therefore:\n",
    "- ΔMSE > 0 means the split has improved predictions (as in our case)\n",
    "- ΔMSE = 0 means the split makes no difference\n",
    "- ΔMSE < 0 is mathematically impossible\n",
    "```\n",
    "\n",
    "The larger the ΔMSE, the more effective the split is at creating subgroups with similar house prices. Our large ΔMSE of 1,066,664,667 indicates this is a very effective split.\n",
    "\n",
    "### A simplified decision tree algorithm in python:\n",
    "In practise, you'd use a library like `sklearn` to build a decision tree, but here's a simplified version in python to illustrate the concept:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class House:\n",
    "    def __init__(self, features: Dict[str, float], price: float):\n",
    "        self.features = features\n",
    "        self.price = price\n",
    "\n",
    "def find_best_split(houses: List[House], feature: str) -> tuple:\n",
    "    values = sorted(set(house.features[feature] for house in houses))\n",
    "    \n",
    "    best_split = None\n",
    "    best_delta_mse = float('-inf')\n",
    "\n",
    "    for i in range(len(values) - 1):\n",
    "        split_point = (values[i] + values[i+1]) / 2\n",
    "        left = [h for h in houses if h.features[feature] < split_point]\n",
    "        right = [h for h in houses if h.features[feature] >= split_point]\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            continue\n",
    "\n",
    "        mse_before = np.var([h.price for h in houses])\n",
    "        mse_left = np.var([h.price for h in left])\n",
    "        mse_right = np.var([h.price for h in right])\n",
    "\n",
    "        delta_mse = mse_before - (len(left)/len(houses) * mse_left + len(right)/len(houses) * mse_right)\n",
    "\n",
    "        if delta_mse > best_delta_mse:\n",
    "            best_delta_mse = delta_mse\n",
    "            best_split = split_point\n",
    "\n",
    "    return best_split, best_delta_mse\n",
    "\n",
    "def build_tree(houses: List[House], depth: int = 0, max_depth: int = 3) -> Dict[str, Any]:\n",
    "    if depth == max_depth or len(houses) < 2:\n",
    "        return {'type': 'leaf', 'value': np.mean([h.price for h in houses])}\n",
    "\n",
    "    features = houses[0].features.keys()\n",
    "    best_feature = None\n",
    "    best_split = None\n",
    "    best_delta_mse = float('-inf')\n",
    "\n",
    "    for feature in features:\n",
    "        split, delta_mse = find_best_split(houses, feature)\n",
    "        if delta_mse > best_delta_mse:\n",
    "            best_feature = feature\n",
    "            best_split = split\n",
    "            best_delta_mse = delta_mse\n",
    "\n",
    "    if best_feature is None:\n",
    "        return {'type': 'leaf', 'value': np.mean([h.price for h in houses])}\n",
    "\n",
    "    left = [h for h in houses if h.features[best_feature] < best_split]\n",
    "    right = [h for h in houses if h.features[best_feature] >= best_split]\n",
    "\n",
    "    return {\n",
    "        'type': 'node',\n",
    "        'feature': best_feature,\n",
    "        'split': best_split,\n",
    "        'left': build_tree(left, depth + 1, max_depth),\n",
    "        'right': build_tree(right, depth + 1, max_depth)\n",
    "    }\n",
    "\n",
    "def predict(tree: Dict[str, Any], house: House) -> float:\n",
    "    if tree['type'] == 'leaf':\n",
    "        return tree['value']\n",
    "    \n",
    "    if house.features[tree['feature']] < tree['split']:\n",
    "        return predict(tree['left'], house)\n",
    "    else:\n",
    "        return predict(tree['right'], house)\n",
    "\n",
    "# Example usage\n",
    "houses = [\n",
    "    House({'bedrooms': 2, 'area': 80, 'distance_to_tube': 15}, 200),\n",
    "    House({'bedrooms': 3, 'area': 120, 'distance_to_tube': 10}, 250),\n",
    "    House({'bedrooms': 2, 'area': 75, 'distance_to_tube': 20}, 180),\n",
    "    House({'bedrooms': 3, 'area': 100, 'distance_to_tube': 5}, 220),\n",
    "    House({'bedrooms': 4, 'area': 150, 'distance_to_tube': 2}, 300),\n",
    "    House({'bedrooms': 3, 'area': 110, 'distance_to_tube': 12}, 240),\n",
    "    House({'bedrooms': 2, 'area': 70, 'distance_to_tube': 25}, 190),\n",
    "    House({'bedrooms': 4, 'area': 140, 'distance_to_tube': 8}, 280),\n",
    "    House({'bedrooms': 3, 'area': 130, 'distance_to_tube': 6}, 260),\n",
    "    House({'bedrooms': 2, 'area': 85, 'distance_to_tube': 18}, 210)\n",
    "]\n",
    "\n",
    "tree = build_tree(houses)\n",
    "\n",
    "def print_tree(node, indent=\"\"):\n",
    "    if node['type'] == 'leaf':\n",
    "        print(f\"{indent}Predict price: £{node['value']:.2f}k\")\n",
    "    else:\n",
    "        print(f\"{indent}{node['feature']} < {node['split']:.2f}\")\n",
    "        print(f\"{indent}If True:\")\n",
    "        print_tree(node['left'], indent + \"  \")\n",
    "        print(f\"{indent}If False:\")\n",
    "        print_tree(node['right'], indent + \"  \")\n",
    "\n",
    "print_tree(tree)\n",
    "\n",
    "# Test prediction\n",
    "new_house = House({'bedrooms': 3, 'area': 105, 'distance_to_tube': 7}, 0)  # price set to 0 as it's unknown\n",
    "predicted_price = predict(tree, new_house)\n",
    "print(f\"\\nPredicted price for new house: £{predicted_price:.2f}k\")\n",
    "\n",
    "```\n",
    "\n",
    "### Mean Squared Error (MSE) vs Mean Absolute Error (MAE)\n",
    "\n",
    "When evaluating our decision tree's performance, we need to understand the difference between training metrics and evaluation metrics.\n",
    "\n",
    "![mean-squared-error-mean-absolute-error](../static/mean-squared-error-mean-absolute-error.png)\n",
    "\n",
    "Our decision tree algorithm uses MSE as the splitting criterion but measures final performance using MAE. \n",
    "\n",
    "Here's why we use these different metrics:\n",
    "\n",
    "##### 1. Mean Squared Error (MSE)\n",
    "\n",
    "   **Calculation:** (predicted house price - actual house price)²\n",
    "\n",
    "   For example, if we predict £200,000 for a house that's actually worth £150,000, the error is £50,000 and MSE is £50,000² = £2.5 billion\n",
    "\n",
    "   **Visualisation**\n",
    "\n",
    "   If we plot how wrong our house price prediction is (like £50,000 too high or -£50,000 too low) on the x-axis, and plot the squared value of this error (like £2.5 billion) on the y-axis, we get a U-shaped curve. Because MSE squares the errors, it gives more weight to data points that are further from the mean, making it a good measure of variance within groups.\n",
    "\n",
    "   **Purpose**\n",
    "\n",
    "   The decision tree uses MSE to decide where to split data because minimizing MSE is equivalent to minimizing the variance within each group, which helps find splits that create distinct groups of house prices.\n",
    "\n",
    "  ##### 2. Mean Absolute Error (MAE)\n",
    "\n",
    "   **Calculation:** |predicted house price - actual house price|\n",
    "\n",
    "   Using the same example, if we predict £200,000 for a £150,000 house, MAE is |£50,000| = £50,000\n",
    "\n",
    "   **Visualisation**\n",
    "\n",
    "   If we plot how wrong our prediction is on the x-axis (like £50,000 too high or -£50,000 too low), and plot the absolute value of this error on the y-axis (always positive, like £50,000), we get a V-shaped curve\n",
    "\n",
    "   **Purpose**\n",
    "   \n",
    "   We use MAE to evaluate our final model because it's easier to understand - it directly tells us how many pounds we're off by on average\n",
    "\n",
    "\\\n",
    "The decision tree uses MSE's mathematical properties to make splitting decisions, but we report MAE because \"off by £50,000 on average\" makes more sense than \"off by £2.5 billion squared pounds\"!\n",
    "\n",
    "\\\n",
    "Here's an example to illustrate the difference:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "y_true = [100, 200, 300]\n",
    "y_pred = [90, 210, 320]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Mean Squared Error: 200.00\n",
    "Mean Absolute Error: 13.33\n",
    "```\n",
    "\n",
    "In this example, MSE and MAE provide different views of the error. MSE is more sensitive to the larger error (20) in the third prediction, while MAE treats all errors equally.\n",
    "\n",
    "For house price prediction, MAE is often preferred as it directly translates to the average error in pounds. However, MSE is still commonly used as a splitting criterion in decision trees because minimizing MSE helps create groups with similar target values by minimizing the variance within each group.\n",
    "\n",
    "### For Classification Tasks (e.g., Predicting if a House Will Sell Quickly):\n",
    "\n",
    "In classification problems, we're trying to predict a categorical outcome, like whether a house will sell quickly or not. The goal is to split the data in a way that maximizes the \"purity\" of the classes within each resulting group.\n",
    "\n",
    "There are several metrics used for classification trees, with the most common being Gini Impurity and Entropy. These metrics measure how mixed the classes are within a group.\n",
    "\n",
    "Let's explore how different distributions of marbles affect our measures of impurity. We will then explore information gain, a measure used in conjuction with impurity metrics to decide how to split the data.\n",
    "\n",
    "We'll use red marbles to represent quick-selling houses and blue marbles for slow-selling houses.\n",
    "\n",
    "#### 1. Gini Impurity:\n",
    "   Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution in the set.\n",
    "\n",
    "   Formula: $Gini = 1 - \\sum_{i=1}^{c} (p_i)^2$\n",
    "\n",
    "   Where $c$ is the number of classes and $p_i$ is the probability of an object being classified to a particular class.\n",
    "\n",
    "   Let's compare three scenarios:\n",
    "\n",
    "```code\n",
    "   a) 10 marbles: 7 red, 3 blue\n",
    "      Fraction of red = 7/10 = 0.7\n",
    "      Fraction of blue = 3/10 = 0.3\n",
    "      \n",
    "      Gini = 1 - (0.7² + 0.3²) = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42\n",
    "```\n",
    "\n",
    "```code\n",
    "   b) 10 marbles: 5 red, 5 blue\n",
    "      Fraction of red = 5/10 = 0.5\n",
    "      Fraction of blue = 5/10 = 0.5\n",
    "      \n",
    "      Gini = 1 - (0.5² + 0.5²) = 1 - (0.25 + 0.25) = 1 - 0.5 = 0.5\n",
    "      most impure set\n",
    "```\n",
    "\n",
    "```code\n",
    "   c) 10 marbles: 9 red, 1 blue\n",
    "      Fraction of red = 9/10 = 0.9\n",
    "      Fraction of blue = 1/10 = 0.1\n",
    "      \n",
    "      Gini = 1 - (0.9² + 0.1²) = 1 - (0.81 + 0.01) = 1 - 0.82 = 0.18\n",
    "      purest set\n",
    "```\n",
    "\n",
    "**The lower the Gini Impurity, the purer the set. Scenario (c) has the lowest Gini Impurity, indicating it's the most homogeneous.**\n",
    "\n",
    "#### 2. Entropy:\n",
    "\n",
    "Entropy is another measure of impurity, based on the concept of information theory. It quantifies the amount of uncertainty or randomness in the data.\n",
    "\n",
    "$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
    "\n",
    "Where $c$ is the number of classes and $p_i$ is the probability of an object being classified to a particular class.\n",
    "\n",
    "Imagine you're playing a guessing game with marbles in a bag. Entropy measures how surprised you'd be when pulling out a marble. The more mixed the colours, the more surprised you might be, and the higher the entropy.\n",
    "\n",
    "#### Let's use our marble scenarios:\n",
    "\n",
    "10 marbles: 7 red, 3 blue\n",
    "\n",
    "To calculate entropy, we follow these steps:\n",
    "\n",
    "1. Calculate the fraction of each colour:\n",
    "```\n",
    "   Red: 7/10 = 0.7\n",
    "   Blue: 3/10 = 0.3\n",
    "```\n",
    "\n",
    "2. For each colour, multiply its fraction by the log2 of its fraction:   \n",
    "```\n",
    "   Red: 0.7 × log2(0.7) = 0.7 × -0.5146 = -0.360\n",
    "   Blue: 0.3 × log2(0.3) = 0.3 × -1.7370 = -0.5211\n",
    "```\n",
    "\n",
    "3. Sum these values and negate the result:\n",
    "```\n",
    "Entropy = -(-0.3602 + -0.5211) = 0.8813\n",
    "```\n",
    "\n",
    "#### Let's do this for all scenarios:\n",
    "\n",
    "a) 7 red, 3 blue\n",
    "```\n",
    "   Entropy = 0.8813\n",
    "```\n",
    "b) 5 red, 5 blue\n",
    "```\n",
    "   Red: 0.5 × log2(0.5) = 0.5 × -1 = -0.5\n",
    "   Blue: 0.5 × log2(0.5) = 0.5 × -1 = -0.5\n",
    "   Entropy = -(-0.5 + -0.5) = 1\n",
    "\n",
    "Highest entropy, least predictable set\n",
    "```\n",
    "\n",
    "c) 9 red, 1 blue\n",
    "```\n",
    "   Red: 0.9 × log2(0.9) = 0.9 × -0.1520 = -0.1368\n",
    "   Blue: 0.1 × log2(0.1) = 0.1 × -3.3219 = -0.3322\n",
    "   Entropy = -(-0.1368 + -0.3322) = 0.4690\n",
    "\n",
    "Lowest entropy, most predictable set\n",
    "```\n",
    "\n",
    "Lower entropy means less surprise or uncertainty. Scenario (c) has the lowest entropy, confirming it's the most predictable (or least mixed) set.\n",
    "\n",
    "In Python, we could calculate entropy like this:\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def calculate_entropy(marbles):\n",
    "    total = sum(marbles.values())\n",
    "    entropy = 0\n",
    "    for count in marbles.values():\n",
    "        fraction = count / total\n",
    "        entropy -= fraction * math.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "scenario_a = {\"red\": 7, \"blue\": 3}\n",
    "entropy_a = calculate_entropy(scenario_a)\n",
    "print(f\"Entropy for scenario A: {entropy_a:.4f}\")\n",
    "```\n",
    "\n",
    "#### 3. Information Gain:\n",
    "\n",
    "Information Gain measures how much a split improves our ability to predict the outcome. It's a way of measuring how much better you've sorted your marbles after dividing them into groups.\n",
    "\n",
    "Formula: $IG(T, a) = I(T) - \\sum_{v \\in values(a)} \\frac{|T_v|}{|T|} I(T_v)$\n",
    "\n",
    "Where:\n",
    "- $T$ is the parent set\n",
    "- $a$ is the attribute on which the split is made\n",
    "- $v$ represents each possible value of attribute $a$\n",
    "- $T_v$ is the subset of $T$ for which attribute $a$ has value $v$\n",
    "- $I(T)$ is the impurity measure (Entropy or Gini) of set $T$\n",
    "\n",
    "\n",
    "#### Let's use a scenario to calculate Information Gain:\n",
    "\n",
    "We have 20 marbles total, and we're considering splitting them based on a feature (e.g., house size: small or large).\n",
    "```\n",
    "Before split: 12 red, 8 blue\n",
    "```\n",
    "\n",
    "Step 1: Calculate the entropy before the split\n",
    "```\n",
    "Entropy_before = 0.9710 (calculated as we did above)\n",
    "```\n",
    "\n",
    "After split:\n",
    "```\n",
    "Small houses: 8 red, 2 blue\n",
    "Large houses: 4 red, 6 blue\n",
    "```\n",
    "Step 2: Calculate entropy for each group after the split\n",
    "Entropy_small = 0.7219 (calculated for 8 red, 2 blue)\n",
    "Entropy_large = 0.9710 (calculated for 4 red, 6 blue)\n",
    "\n",
    "Step 3: Calculate the weighted average of the split entropies\n",
    "```\n",
    "Weight_small = 10/20 = 0.5 (half the marbles are in small houses)\n",
    "Weight_large = 10/20 = 0.5 (half the marbles are in large houses)\n",
    "Weighted_entropy_after = (0.5 × 0.7219) + (0.5 × 0.9710) = 0.8465\n",
    "```\n",
    "\n",
    "Step 4: Calculate Information Gain\n",
    "```\n",
    "Information Gain = Entropy_before - Weighted_entropy_after\n",
    "                 = 0.9710 - 0.8465\n",
    "                 = 0.1245\n",
    "```\n",
    "\n",
    "This positive Information Gain indicates that the split has improved our ability to predict marble colours (or in our house analogy, to predict if a house will sell quickly).\n",
    "\n",
    "#### In Python, we could calculate Information Gain like this:\n",
    "\n",
    "```python\n",
    "def calculate_information_gain(before, after):\n",
    "    entropy_before = calculate_entropy(before)\n",
    "    \n",
    "    total_after = sum(sum(group.values()) for group in after)\n",
    "    weighted_entropy_after = sum(\n",
    "        (sum(group.values()) / total_after) * calculate_entropy(group)\n",
    "        for group in after\n",
    "    )\n",
    "    \n",
    "    return entropy_before - weighted_entropy_after\n",
    "\n",
    "# Example usage\n",
    "before_split = {\"red\": 12, \"blue\": 8}\n",
    "after_split = [\n",
    "    {\"red\": 8, \"blue\": 2},  # Small houses\n",
    "    {\"red\": 4, \"blue\": 6}   # Large houses\n",
    "]\n",
    "\n",
    "info_gain = calculate_information_gain(before_split, after_split)\n",
    "print(f\"Information Gain: {info_gain:.4f}\")\n",
    "```\n",
    "\n",
    "#### Comparison: Splits with Different Information Gains\n",
    "\n",
    "The decision tree algorithm always chooses the split that provides the most Information Gain. \n",
    "\n",
    "Let's consider two potential splits of our 20 marbles:\n",
    "\n",
    "1. Split by house size (small vs large):\n",
    "   - Small houses: 8 red, 2 blue\n",
    "   - Large houses: 4 red, 6 blue\n",
    "   - Information Gain: 0.1245\n",
    "\n",
    "2. Split by garage presence:\n",
    "   - Houses with garage: 6 red, 4 blue\n",
    "   - Houses without garage: 6 red, 4 blue\n",
    "   - Information Gain: 0\n",
    "\n",
    "The algorithm would choose the split by house size because it provides more Information Gain. \n",
    "\n",
    "Zero Information Gain occurs when a split doesn't change the distribution of the target variable (in this case, marble colours or house selling speed). This happens when the proportions in each resulting group are identical to the proportions in the parent group.\n",
    "\n",
    "In practice, splits with exactly zero Information Gain are rare. More commonly, you'll see splits with varying degrees of positive Information Gain, and the algorithm will choose the one with the highest value.\n",
    "\n",
    "Features that provide little or no Information Gain are typically less valuable for prediction and should be considered for removal from the model. Eliminating these low-impact features can simplify the model, potentially improving its generalization ability and computational efficiency without significantly compromising predictive performance.\n",
    "\n",
    "## Theory Conclusion\n",
    "\n",
    "Now that we've explored the key concepts behind decision trees, let's summarize the main points and how they apply to our house price prediction task:\n",
    "\n",
    "1. **Regression Trees vs Classification Trees**: \n",
    "   For our house price prediction problem, we're using regression trees. Unlike classification trees which use metrics like Gini impurity or entropy, regression trees aim to minimize the variance in the target variable (house prices) within each node.\n",
    "\n",
    "2. **Splitting Criterion**: \n",
    "   In regression trees, the splitting criterion is typically the reduction in Mean Squared Error (MSE) or equivalently, the reduction in variance. At each node, the algorithm chooses the feature and split point that maximizes this reduction:\n",
    "\n",
    "   $\\Delta MSE = MSE_{parent} - (w_{left} * MSE_{left} + w_{right} * MSE_{right})$\n",
    "\n",
    "   Where $w_{left}$ and $w_{right}$ are the proportions of samples in the left and right child nodes.\n",
    "\n",
    "3. **Recursive Splitting**: \n",
    "   The tree is built by recursively applying this splitting process, creating a hierarchy of decision rules. This continues until a stopping condition is met, such as a maximum tree depth or a minimum number of samples per leaf.\n",
    "\n",
    "4. **Prediction**: \n",
    "   To predict the price of a new house, we follow the decision rules from the root to a leaf node. The prediction is typically the mean price of all houses in that leaf node.\n",
    "\n",
    "5. **Interpretability**: \n",
    "   One of the key advantages of decision trees is their interpretability. We can visualize the tree and follow its decision path, which provides insights into which features are most important for predicting house prices.\n",
    "\n",
    "6. **Bias-Variance Trade-off**: \n",
    "   Deeper trees can capture more complex relationships in the data but risk overfitting (high variance). Shallower trees are more generalizable but might oversimplify the problem (high bias). Finding the right balance is crucial for optimal performance.\n",
    "\n",
    "7. **Feature Importance**: \n",
    "   Decision trees naturally perform feature selection. Features that appear higher in the tree or are used in more splits are generally more important for prediction.\n",
    "\n",
    "8. **Handling Non-linearity**: \n",
    "   Unlike linear regression, decision trees can capture non-linear relationships between features and the target variable, which is often the case in real estate markets.\n",
    "\n",
    "9. **Limitations**: \n",
    "   Decision trees can be unstable (small changes in data can result in very different trees) and may struggle with smooth, linear relationships. These limitations are often addressed by ensemble methods like Random Forests.\n",
    "\n",
    "As we move forward to apply these concepts to our London housing dataset, keep in mind that while the theory provides the foundation, the real insights often come from experimenting with the data, tuning the model, and interpreting the results in the context of the problem at hand.\n",
    "\n",
    "In the next lesson: 2a_decision_trees_practical.ipynb, we'll see how these theoretical concepts translate into practical implementation using Python and scikit-learn, and how we can use decision trees to gain insights into the London housing market.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
