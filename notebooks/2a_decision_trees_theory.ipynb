{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Decision Trees for House Price Prediction\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "# Decision Trees for House Price Prediction\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Intuition Behind Decision Trees](#intuition-behind-decision-trees)\n",
    "   - [Why Choose Decision Trees for House Prices?](#why-choose-decision-trees-for-house-prices)\n",
    "3. [Anatomy of a Decision Tree](#anatomy-of-a-decision-tree)\n",
    "4. [Preparing Data for Decision Trees](#preparing-data-for-decision-trees)\n",
    "   - [Numerical Data](#numerical-data)\n",
    "   - [Categorical Data](#categorical-data)\n",
    "   - [One-Hot Encoding](#one-hot-encoding)\n",
    "   - [Target Encoding](#target-encoding)\n",
    "   - [Smoothed Target Encoding](#smoothed-target-encoding)\n",
    "   - [Practical Guide to Smoothed Encoding](#practical-guide-to-smoothed-encoding)\n",
    "   - [Ordinal and Binary Features](#ordinal-and-binary-features)\n",
    "   - [Combining Different Encoding Methods](#combining-different-encoding-methods)\n",
    "   - [Guide to Choosing Encoding Methods](#guide-to-choosing-encoding-methods)\n",
    "5. [Splitting Criteria Explained](#splitting-criteria-explained)\n",
    "   - [For Regression Tasks](#for-regression-tasks-eg-predicting-house-prices)\n",
    "   - [Mean Squared Error](#mean-squared-error-mse)\n",
    "   - [Evaluating Decision Points](#evaluating-decision-points-understanding-split-quality-in-decision-trees)\n",
    "   - [Mean Squared Error vs Mean Absolute Error](#mean-squared-error-mse-vs-mean-absolute-error-mae)\n",
    "   - [For Classification Tasks](#for-classification-tasks-eg-predicting-if-a-house-will-sell-quickly)\n",
    "     - [Gini Impurity](#1-gini-impurity)\n",
    "     - [Entropy](#2-entropy)\n",
    "     - [Information Gain](#3-information-gain)\n",
    "     - [Comparison: Splits with Different Information Gains](#comparison-splits-with-different-information-gains)\n",
    "6. [Interpretability and Visualization](#interpretability-and-visualization)\n",
    "   - [Why Interpretability Matters](#why-interpretability-matters)\n",
    "   - [How to Interpret Decision Trees](#how-to-interpret-decision-trees)\n",
    "7. [Understanding Bias, Variance, Tree Depth and Complexity](#understanding-bias-variance-tree-depth-and-complexity)\n",
    "   - [Bias](#bias)\n",
    "   - [Variance](#variance)\n",
    "   - [Identifying the Bias/Variance Tradeoff](#identifying-the-biasvariance-tradeoff)\n",
    "   - [Managing the Bias/Variance Tradeoff](#managing-the-biasvariance-tradeoff)\n",
    "   - [Visual Indicators of Bias/Variance](#visual-indicators-of-biasvariance)\n",
    "8. [Feature Importance and Advanced Capabilities](#feature-importance-and-advanced-capabilities)\n",
    "   - [Feature Importance in Decision Trees](#feature-importance-in-decision-trees)\n",
    "   - [Advanced Capabilities](#advanced-capabilities)\n",
    "   - [Limitations and Solutions](#limitations-and-solutions)\n",
    "   - [Practical Applications](#practical-applications)\n",
    "9. [Limitations and Ethical Considerations](#limitations-and-ethical-considerations)\n",
    "   - [Technical Limitations](#technical-limitations)\n",
    "   - [Solutions and Mitigations](#solutions-and-mitigations)\n",
    "   - [Ethical Considerations for Decision Tree Models](#ethical-considerations-for-decision-tree-models)\n",
    "   - [Best Practices for Ethical Use](#4-best-practices-for-ethical-use)\n",
    "10. [Theory Conclusion](#theory-conclusion)\n",
    "    - [Core Concepts](#core-concepts)\n",
    "    - [Data Handling and Model Characteristics](#data-handling-and-model-characteristics)\n",
    "    - [Error Metrics and Evaluation](#error-metrics-and-evaluation)\n",
    "    - [Next Steps](#next-steps)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Decision trees are a versatile machine learning model for both classification and regression tasks. In this lesson, we'll use decision trees to predict house prices based on features like location, size, and amenities.\n",
    "\n",
    "Imagine you're a real estate agent trying to estimate the fair price of a house based on its characteristics. This is where decision trees can help. They learn a set of rules from historical data to make predictions on new, unseen houses.\n",
    "\n",
    "Essentially, a decision tree is used to make predictions on the target variable - say price - by recursively splitting the data based on the values of the features, choosing splits that maximize the similarity of the target variable (prices) within each subset.\n",
    "\n",
    "The result is a tree-like model of decisions and their consequences.\n",
    "\n",
    "By the end of this lesson, you'll understand how decision trees work, how to train and interpret them, and how they compare to other models for regression tasks.\n",
    "\n",
    "## Intuition Behind Decision Trees\n",
    "\n",
    "Imagine you're trying to predict the price of a house based on its features. You might start by asking broad questions like \"Is it in a desirable location?\" and then progressively get more specific: \"How many bedrooms does it have? What's the square footage?\".\n",
    "\n",
    "At each step, you're trying to split the houses into groups that are as similar as possible in terms of price. This is exactly how a decision tree works - it asks a series of questions about the features, each time trying to split the data into more homogeneous subsets.\n",
    "\n",
    "### Why Choose Decision Trees for House Prices?\n",
    "\n",
    "Decision trees are particularly well-suited for this task because of several key advantages that become apparent when comparing them to other popular algorithms:\n",
    "\n",
    "1. **Working with Different Types of Data**\n",
    "   While decision trees need numbers to make their calculations, they have elegant ways of handling different types of data:\n",
    "   - Numerical: Price (£180,000 to £39,750,000), square footage (274 to 15,405 sq ft)\n",
    "     - Used directly as they're already numbers\n",
    "   - Categorical: Location (\"Chelsea\", \"Hackney\"), house type (\"Flat\", \"House\", \"Penthouse\")\n",
    "     - Can be converted to numbers in smart ways:\n",
    "       - One-hot encoding: Like giving each location its own yes/no column\n",
    "       - Target encoding: Converting locations to average prices in that area\n",
    "     - We'll explore these in detail later in the course\n",
    "   - Ordinal: Number of bedrooms (1-10), bathrooms (1-10), receptions (1-10)\n",
    "     - Already in a natural order, easy to use\n",
    "\n",
    "2. **No Feature Scaling Required**\n",
    "   Unlike many other algorithms, decision trees work with raw values directly. \n",
    "   \n",
    "   Compare this to:\n",
    "   - Linear/Logistic Regression: Requires scaling to prevent features with larger values from dominating the model\n",
    "   - Neural Networks: Needs normalized inputs (usually between 0-1) for stable gradient descent\n",
    "   - Support Vector Machines (SVM): Highly sensitive to feature scales, requires standardization\n",
    "   - K-Nearest Neighbors: Distance calculations are skewed by different scales, needs normalization\n",
    "\n",
    "   The tree makes splits based on relative ordering, not absolute values. \n",
    "   \n",
    "   For example, these splits are all equivalent to a decision tree:\n",
    "   ```python\n",
    "   # Original scale (Decision Tree works fine)\n",
    "   if square_footage > 2000:\n",
    "       predict_price = 1200000\n",
    "   else:\n",
    "       predict_price = 800000\n",
    "\n",
    "   # Scaled by 1000 (needed for Neural Networks)\n",
    "   if square_footage/1000 > 2:  # Same result for decision tree\n",
    "       predict_price = 1200000\n",
    "   else:\n",
    "       predict_price = 800000\n",
    "\n",
    "   # Standardized (needed for SVM)\n",
    "   if (square_footage - mean)/std > 1.2:  # Same result for decision tree\n",
    "       predict_price = 1200000\n",
    "   else:\n",
    "       predict_price = 800000\n",
    "   ```\n",
    "\n",
    "3. **Interpretable Decision Making**\n",
    "   While algorithms like Neural Networks act as \"black boxes\" and Linear Regression gives abstract coefficients, decision trees create clear, actionable rules. Here's a simple example:\n",
    "   ```python\n",
    "   # The computer converts locations to simple yes/no questions\n",
    "   if location_hackney == 1:  # Is it in Hackney?\n",
    "       if square_footage > 1200:\n",
    "           predict_price = \"£950K\"\n",
    "       else:\n",
    "           predict_price = \"£650K\"\n",
    "   elif location_wimbledon == 1:  # Is it in Wimbledon?\n",
    "       if bedrooms > 3:\n",
    "           predict_price = \"£1.2M\"\n",
    "       else:\n",
    "           predict_price = \"£800K\"\n",
    "   ```\n",
    "   These rules are easy to explain to stakeholders, unlike trying to interpret neural network weights or SVM kernel transformations. The yes/no questions (location_hackney == 1) simply mean \"Is this property in Hackney?\" - a question anyone can understand!\n",
    "\n",
    "4. **Handling Missing Data**\n",
    "   Real estate data often has missing values. For example, some listings might not include the square footage or number of bathrooms.\n",
    "   \n",
    "   While most algorithms require these missing values to be filled in or removed, decision trees have clever ways to handle missing data:\n",
    "   - They can make predictions even when some feature values are unknown\n",
    "   - They can use alternative features when a preferred feature is missing\n",
    "   - They maintain good accuracy even with incomplete information\n",
    "\n",
    "These advantages mean we can focus on understanding the relationships in our data rather than spending time on complicated data preprocessing. \n",
    "\n",
    "This makes decision trees an excellent choice for our house price prediction task, especially when interpretability and ease of use are priorities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Decision Tree\n",
    "\n",
    "A decision tree is composed of:\n",
    "\n",
    "- Nodes: Where a feature is tested\n",
    "- Edges: The outcomes of the test\n",
    "- Leaves: Terminal nodes that contain the final predictions\n",
    "\n",
    "A simplified example of a house prices prediction decision tree might look like this:\n",
    "\n",
    "![structure of a house prices prediction decision tree](../static/house-prices-decision-tree-and-structure.png)\n",
    "\n",
    "The tree is built by splitting the data recursively, choosing at each step a feature and a numerical split point on that feature that results in the greatest reduction in impurity or error. For example, the first split could be on the feature \"square footage\" with a split point of 2000 sq ft because this results in the greatest reduction in impurity or error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Decision Trees\n",
    "\n",
    "Before we delve into how decision trees make split decisions it's important to first understand what data we can use.\n",
    "\n",
    "While decision trees can handle various types of data, we need to convert all features into numerical formats for training. This process is called encoding. \n",
    "\n",
    "Different types of features require different encoding approaches:\n",
    "\n",
    "1. **Numerical Features**\n",
    "   - Already in usable format (e.g., prices, areas)\n",
    "   - No encoding needed\n",
    "\n",
    "2. **Categorical Features**\n",
    "   - Need conversion to numbers\n",
    "   - Multiple encoding strategies available\n",
    "   - Examples: locations, house types\n",
    "\n",
    "3. **Ordinal Features**\n",
    "   - Categories with natural order\n",
    "   - Need to preserve order relationship\n",
    "   - Example: size (small, medium, large)\n",
    "\n",
    "4. **Binary Features**\n",
    "   - Yes/no features\n",
    "   - Simple 1/0 encoding\n",
    "   - Example: has_parking, has_garden\n",
    "\n",
    "Let's explore how to handle each type effectively, understanding the trade-offs and choosing the right approach for our data.\n",
    "\n",
    "### Numerical Data\n",
    "\n",
    "Numerical features provide a solid foundation for decision trees because they:\n",
    "- Work directly without transformation\n",
    "- Don't require scaling\n",
    "- Can handle different value ranges\n",
    "- Support both integers and floating-point numbers\n",
    "\n",
    "Common numerical features in housing data:\n",
    "- Price (e.g., £250,000)\n",
    "- Square footage (e.g., 1,500 sq ft)\n",
    "- Number of rooms (e.g., 3 bedrooms)\n",
    "- Age of property (e.g., 25 years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data\n",
    "\n",
    "Categorical features are variables that take on a limited number of discrete values. In housing data, these might include:\n",
    "- Location (Chelsea, Hackney, Mayfair)\n",
    "- Property type (Flat, House, Penthouse)\n",
    "- Style (Modern, Victorian, Georgian)\n",
    "\n",
    "We have three main approaches for encoding categorical data:\n",
    "\n",
    "1. **One-Hot Encoding**\n",
    "   - Creates binary columns for each category\n",
    "   - Best for low/medium cardinality - cardinality is the number of unique categories in a feature\n",
    "   - Preserves all category information\n",
    "   - No implied ordering\n",
    "\n",
    "2. **Target Encoding**\n",
    "   - Replaces categories with target statistics for each category, for example the mean price for each location\n",
    "   - Best for features with high cardinality as one-hot encoding will explode the number of features\n",
    "   - Two variants:\n",
    "     - Simple (target statistic per category - for instance the mean price for each location)\n",
    "     - Smoothed (statistic for the category balanced with global statistic)\n",
    "\n",
    "3. **Binary Encoding**\n",
    "   - For true yes/no features\n",
    "   - Simple 1/0 conversion\n",
    "   - Most memory efficient\n",
    "\n",
    "Let's examine each approach in detail:\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "One-hot encoding transforms categorical variables by:\n",
    "- Creating a new binary column for each category\n",
    "- Setting 1 where the category is present, 0 otherwise\n",
    "- No information loss or ordering implied\n",
    "\n",
    "**Ideal for:**\n",
    "- Categorical variables with few unique values\n",
    "- When memory isn't a constraint\n",
    "- When interpretability is important\n",
    "\n",
    "**Example:**\n",
    "Property Type (Flat, House, Penthouse) becomes:\n",
    "- property_type_flat: [1,0,0]\n",
    "- property_type_house: [0,1,0]\n",
    "- property_type_penthouse: [0,0,1]\n",
    "\n",
    "Let's implement one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample categorical data\n",
    "data = {\n",
    "    'property_type': ['Flat', 'House', 'Penthouse', 'Flat', 'House'],\n",
    "    'location': ['Chelsea', 'Hackney', 'Chelsea', 'Putney', 'Chelsea']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encode multiple columns\n",
    "df_encoded = pd.get_dummies(df, prefix=['type', 'loc'])\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(df)\n",
    "print(\"\\nFully encoded data:\")\n",
    "print(df_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding\n",
    "\n",
    "Target encoding replaces categorical values with statistics calculated from the target variable. For housing data, this means replacing each location with its average house price.\n",
    "\n",
    "**Advantages:**\n",
    "- Handles high cardinality efficiently\n",
    "- Captures relationship with target variable\n",
    "- Memory efficient\n",
    "- Works well for decision trees\n",
    "\n",
    "**Challenges:**\n",
    "- Risk of overfitting\n",
    "- Needs handling for rare categories\n",
    "- Requires cross-validation\n",
    "- Can leak target information - for example if we were predicting house prices and we encoded the location with the mean price for each location, the model would know the price of the houses in that location before they were predicted, which would be a problem. To avoid this in practice we split the data into a training and validation set and only use the training set to calculate the mean price for each location.\n",
    "\n",
    "**Simple Target Encoding Example:**\n",
    "```\n",
    "Location   | Count | Avg Price\n",
    "Chelsea    |   100 | £800,000\n",
    "Hackney    |    50 | £500,000\n",
    "Mayfair    |    10 | £2,000,000\n",
    "```\n",
    "\n",
    "Let's first look at basic target encoding before exploring smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data with clear price patterns\n",
    "data = {\n",
    "    'location': ['Chelsea', 'Chelsea', 'Chelsea', 'Hackney', 'Hackney',\n",
    "                 'Mayfair', 'Chelsea', 'Hackney', 'Mayfair', 'Chelsea'],\n",
    "    'price': [800000, 820000, 780000, 500000, 520000,\n",
    "              2000000, 810000, 510000, 1900000, 790000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Simple mean encoding, setting the mean price for each location\n",
    "location_means = df.groupby('location')['price'].mean()\n",
    "df['location_encoded'] = df['location'].map(location_means)\n",
    "\n",
    "# Show encoding results\n",
    "print(\"Original data with encoding:\")\n",
    "summary = df.groupby('location').agg({\n",
    "    'price': ['count', 'mean'],\n",
    "    'location_encoded': 'first'\n",
    "}).round(2)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Demonstrate potential overfitting with rare categories\n",
    "rare_data = df.copy()\n",
    "# Create new row with all columns\n",
    "new_row = pd.DataFrame({\n",
    "    'location': ['Knightsbridge'],\n",
    "    'price': [3000000],\n",
    "    'location_encoded': [None]  # Will be updated after encoding\n",
    "})\n",
    "rare_data = pd.concat([rare_data, new_row], ignore_index=True)\n",
    "\n",
    "# Encode including rare category\n",
    "rare_means = rare_data.groupby('location')['price'].mean()\n",
    "rare_data['location_encoded'] = rare_data['location'].map(rare_means)\n",
    "\n",
    "print(\"\\nEncoding with rare category:\")\n",
    "print(rare_data[rare_data['location'] == 'Knightsbridge'])\n",
    "\n",
    "display(rare_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a rare category such as \"Knightsbridge\" our simplified model has assigned it's actual mean price. This is a problem as the model has effectively leaked information from the validation set into the training set and is causing it to overfit to that one row.\n",
    "\n",
    "### Smoothed Target Encoding\n",
    "\n",
    "Smoothed target encoding addresses the instability of simple target encoding by balancing between:\n",
    "- The category's mean (which might be unstable)\n",
    "- The global mean (which is stable but loses category information)\n",
    "\n",
    "The smoothing formula is:\n",
    "```\n",
    "smoothed_value = (n × category_mean + α × global_mean) / (n + α)\n",
    "```\n",
    "Where:\n",
    "- n = number of samples in the category\n",
    "- α = smoothing factor\n",
    "- category_mean = mean price for the location\n",
    "- global_mean = mean price across all locations\n",
    "\n",
    "**Effect of Smoothing Factor (α):**\n",
    "- Large n (many samples):\n",
    "  - (n >> α) → result close to category mean\n",
    "  - Example: n=100, α=10 → mostly category mean\n",
    "- Small n (few samples):\n",
    "  - (n << α) → result close to global mean\n",
    "  - Example: n=2, α=10 → mostly global mean\n",
    "\n",
    "This balancing act helps prevent overfitting while preserving useful category information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def smoothed_target_encode(df, column, target, alpha=10):\n",
    "    \"\"\"\n",
    "    Apply smoothed target encoding\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - column: Category column name\n",
    "    - target: Target variable name\n",
    "    - alpha: Smoothing factor\n",
    "    \"\"\"\n",
    "    # Calculate global mean\n",
    "    global_mean = df[target].mean()\n",
    "    \n",
    "    # Calculate category stats\n",
    "    category_stats = df.groupby(column).agg({\n",
    "        target: ['count', 'mean']\n",
    "    }).reset_index()\n",
    "    category_stats.columns = [column, 'count', 'mean']\n",
    "    \n",
    "    # Apply smoothing\n",
    "    category_stats['smoothed_mean'] = (\n",
    "        (category_stats['count'] * category_stats['mean'] + alpha * global_mean) /\n",
    "        (category_stats['count'] + alpha)\n",
    "    )\n",
    "    \n",
    "    return dict(zip(category_stats[column], category_stats['smoothed_mean']))\n",
    "\n",
    "# Create sample data with varying category frequencies\n",
    "data = {\n",
    "    'location': ['Chelsea'] * 50 + ['Hackney'] * 20 + ['Mayfair'] * 5 + ['Putney'] * 2,\n",
    "    'price': ([800000 + np.random.randn() * 50000 for _ in range(50)] +  # Chelsea\n",
    "              [500000 + np.random.randn() * 30000 for _ in range(20)] +  # Hackney\n",
    "              [2000000 + np.random.randn() * 100000 for _ in range(5)] + # Mayfair\n",
    "              [600000 + np.random.randn() * 40000 for _ in range(2)])    # Putney\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compare different smoothing levels\n",
    "alphas = [0, 5, 20, 100]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for alpha in alphas:\n",
    "    encoded_values = smoothed_target_encode(df, 'location', 'price', alpha)\n",
    "    results[f'alpha_{alpha}'] = df['location'].map(encoded_values)\n",
    "\n",
    "# Add original mean for comparison\n",
    "original_means = df.groupby('location')['price'].mean()\n",
    "results['original_mean'] = df['location'].map(original_means)\n",
    "results['location'] = df['location']\n",
    "results['count'] = df.groupby('location')['price'].transform('count')\n",
    "\n",
    "# Show results for one location from each frequency group\n",
    "print(\"Effect of smoothing by location frequency:\")\n",
    "for loc in ['Chelsea', 'Hackney', 'Mayfair', 'Putney']:\n",
    "    sample = results[results['location'] == loc].iloc[0]\n",
    "    print(f\"\\n{loc} (n={int(sample['count'])})\")\n",
    "    print(f\"Original mean:  £{sample['original_mean']:,.0f}\")\n",
    "    for alpha in alphas:\n",
    "        print(f\"Alpha {alpha:3d}:      £{sample[f'alpha_{alpha}']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Guide to Smoothed Encoding\n",
    "\n",
    "**Choosing α (Smoothing Factor):**\n",
    "\n",
    "1. **Low α (1-5)**\n",
    "   - Minimal smoothing\n",
    "   - Use when categories are very distinct\n",
    "   - Good with large sample sizes\n",
    "   - Risk: Might not handle rare categories well\n",
    "\n",
    "2. **Medium α (10-20)**\n",
    "   - Balanced smoothing\n",
    "   - Good default choice\n",
    "   - Works well with mixed sample sizes\n",
    "   - Provides some protection against outliers\n",
    "\n",
    "3. **High α (50+)**\n",
    "   - Heavy smoothing\n",
    "   - Use with many rare categories\n",
    "   - Good for noisy data\n",
    "   - Risk: Might lose category signal\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. **Cross-Validation**\n",
    "   - Compute encoding using only training data\n",
    "   - Apply those mappings to validation/test data\n",
    "   - Never peek at test set statistics\n",
    "\n",
    "2. **Category Analysis**\n",
    "   - Check sample size distribution\n",
    "   - Consider higher α for skewed distributions\n",
    "   - Monitor rare categories carefully\n",
    "\n",
    "3. **Domain Knowledge**\n",
    "   - Use business context to validate encodings\n",
    "   - Watch for unexpected category relationships\n",
    "   - Consider grouping related rare categories\n",
    "\n",
    "### Ordinal and Binary Features\n",
    "\n",
    "Ordinal and binary features are simpler to handle than general categorical features, but proper encoding is still important.\n",
    "\n",
    "**Ordinal Features**\n",
    "- Have a natural order between categories\n",
    "- Examples:\n",
    "  - Property condition (Poor → Fair → Good → Excellent)\n",
    "  - Size category (Small → Medium → Large)\n",
    "  - Building quality (Basic → Standard → Luxury)\n",
    "\n",
    "**Binary Features**\n",
    "- Have exactly two possible values\n",
    "- Examples:\n",
    "  - Has parking (Yes/No)\n",
    "  - Is new build (Yes/No)\n",
    "  - Has garden (Yes/No)\n",
    "\n",
    "These features are simpler because:\n",
    "1. Ordinal features maintain their order relationship\n",
    "2. Binary features need only two values (0/1)\n",
    "\n",
    "Let's look at how to encode these properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data with ordinal and binary features\n",
    "data = {\n",
    "    'condition': ['Poor', 'Good', 'Excellent', 'Fair', 'Good'],\n",
    "    'size_category': ['Small', 'Medium', 'Large', 'Small', 'Large'],\n",
    "    'has_parking': ['Yes', 'No', 'Yes', 'No', 'Yes'],\n",
    "    'is_new_build': [True, False, True, False, True]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ordinal encoding using mapping\n",
    "condition_map = {\n",
    "    'Poor': 0,\n",
    "    'Fair': 1,\n",
    "    'Good': 2,\n",
    "    'Excellent': 3\n",
    "}\n",
    "\n",
    "size_map = {\n",
    "    'Small': 0,\n",
    "    'Medium': 1,\n",
    "    'Large': 2\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df['condition_encoded'] = df['condition'].map(condition_map)\n",
    "df['size_encoded'] = df['size_category'].map(size_map)\n",
    "\n",
    "# Binary encoding\n",
    "df['parking_encoded'] = (df['has_parking'] == 'Yes').astype(int)\n",
    "df['new_build_encoded'] = df['is_new_build'].astype(int)\n",
    "\n",
    "print(\"Original and encoded data:\")\n",
    "print(df)\n",
    "\n",
    "# Demonstrate mapping preservation\n",
    "print(\"\\nCondition value ordering:\")\n",
    "for condition, value in sorted(condition_map.items(), key=lambda x: x[1]):\n",
    "    print(f\"{condition}: {value}\")\n",
    "\n",
    "print(\"\\nSize category ordering:\")\n",
    "for size, value in sorted(size_map.items(), key=lambda x: x[1]):\n",
    "    print(f\"{size}: {value}\")\n",
    "\n",
    "# Memory usage comparison\n",
    "print(\"\\nMemory usage comparison:\")\n",
    "print(f\"Original condition column: {df['condition'].memory_usage()} bytes\")\n",
    "print(f\"Encoded condition column: {df['condition_encoded'].memory_usage()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Different Encoding Methods\n",
    "\n",
    "Real datasets usually require multiple encoding approaches. Let's create a complete example that:\n",
    "1. Handles numerical features directly\n",
    "2. One-hot encodes low-cardinality categoricals\n",
    "3. Target encodes high-cardinality categoricals\n",
    "4. Ordinally encodes ordered categories\n",
    "5. Binary encodes yes/no features\n",
    "\n",
    "This represents a typical data preparation pipeline for a housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a realistic housing dataset\n",
    "data = {\n",
    "    # Numerical features\n",
    "    'price': np.random.normal(800000, 200000, 100),\n",
    "    'square_feet': np.random.normal(1500, 300, 100),\n",
    "    'bedrooms': np.random.randint(1, 6, 100),\n",
    "    \n",
    "    # Low-cardinality categorical (one-hot encode)\n",
    "    'property_type': np.random.choice(['Flat', 'House', 'Penthouse'], 100),\n",
    "    \n",
    "    # High-cardinality categorical (target encode)\n",
    "    'location': np.random.choice([\n",
    "        'Chelsea', 'Hackney', 'Mayfair', 'Putney', 'Richmond',\n",
    "        'Hampstead', 'Islington', 'Brixton', 'Camden', 'Greenwich'\n",
    "    ], 100),\n",
    "    \n",
    "    # Ordinal features\n",
    "    'condition': np.random.choice(['Poor', 'Fair', 'Good', 'Excellent'], 100),\n",
    "    \n",
    "    # Binary features\n",
    "    'has_parking': np.random.choice(['Yes', 'No'], 100),\n",
    "    'is_new_build': np.random.choice([True, False], 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "class HousingEncoder:\n",
    "    \"\"\"Complete encoding pipeline for housing data\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=10):\n",
    "        self.alpha = alpha\n",
    "        self.encoders = {}\n",
    "        self.target_stats = {}\n",
    "    \n",
    "    def fit_transform(self, df, target_column='price'):\n",
    "        df_encoded = pd.DataFrame()\n",
    "        \n",
    "        # 1. Keep numerical features as is\n",
    "        numerical_features = ['square_feet', 'bedrooms']\n",
    "        df_encoded[numerical_features] = df[numerical_features]\n",
    "        \n",
    "        # 2. One-hot encode low-cardinality categorical\n",
    "        onehot_features = ['property_type']\n",
    "        onehot_encoded = pd.get_dummies(df[onehot_features])\n",
    "        df_encoded = pd.concat([df_encoded, onehot_encoded], axis=1)\n",
    "        \n",
    "        # 3. Target encode high-cardinality categorical\n",
    "        self.target_stats = self._compute_target_encoding(\n",
    "            df, 'location', target_column\n",
    "        )\n",
    "        df_encoded['location_encoded'] = df['location'].map(self.target_stats)\n",
    "        \n",
    "        # 4. Ordinal encode ordered categories\n",
    "        condition_map = {\n",
    "            'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3\n",
    "        }\n",
    "        df_encoded['condition_encoded'] = df['condition'].map(condition_map)\n",
    "        \n",
    "        # 5. Binary encode yes/no features\n",
    "        df_encoded['has_parking'] = (df['has_parking'] == 'Yes').astype(int)\n",
    "        df_encoded['is_new_build'] = df['is_new_build'].astype(int)\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def _compute_target_encoding(self, df, column, target):\n",
    "        \"\"\"Compute smoothed target encoding\"\"\"\n",
    "        global_mean = df[target].mean()\n",
    "        stats = df.groupby(column).agg({\n",
    "            target: ['count', 'mean']\n",
    "        }).reset_index()\n",
    "        stats.columns = [column, 'count', 'mean']\n",
    "        \n",
    "        # Apply smoothing\n",
    "        stats['smoothed_mean'] = (\n",
    "            (stats['count'] * stats['mean'] + self.alpha * global_mean) /\n",
    "            (stats['count'] + self.alpha)\n",
    "        )\n",
    "        \n",
    "        return dict(zip(stats[column], stats['smoothed_mean']))\n",
    "\n",
    "# Apply encoding\n",
    "encoder = HousingEncoder(alpha=10)\n",
    "df_encoded = encoder.fit_transform(df)\n",
    "\n",
    "# Display results\n",
    "print(\"Original data sample:\")\n",
    "display(df)\n",
    "\n",
    "# print(\"\\nFeature summary:\")\n",
    "# print(\"\\nNumerical features:\", df_encoded.select_dtypes(include=[np.number]).columns.tolist())\n",
    "print(\"\\nShape before encoding:\", df.shape)\n",
    "print(\"Shape after encoding:\", df_encoded.shape)\n",
    "\n",
    "display(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide to Choosing Encoding Methods\n",
    "\n",
    "#### Decision Framework\n",
    "\n",
    "1. **For Numerical Features**\n",
    "   - Use directly without encoding\n",
    "   - No scaling needed for decision trees\n",
    "   - Consider creating derived features if meaningful\n",
    "\n",
    "2. **For Categorical Features**\n",
    "   - **Use One-Hot Encoding when:**\n",
    "     - Few unique categories (<30)\n",
    "     - No natural order\n",
    "     - Memory isn't constrained\n",
    "     - Need model interpretability\n",
    "\n",
    "   - **Use Target Encoding when:**\n",
    "     - Many unique categories (30+)\n",
    "     - Strong relationship with target\n",
    "     - Memory is constrained\n",
    "     - Have sufficient samples per category\n",
    "\n",
    "3. **For Ordinal Features**\n",
    "   - Use ordinal encoding when clear order exists\n",
    "   - Maintain order relationship\n",
    "   - Document ordering logic\n",
    "\n",
    "4. **For Binary Features**\n",
    "   - Always use simple 1/0 encoding\n",
    "   - Consistent encoding for Yes/No values\n",
    "   - Consider combining related binary features\n",
    "\n",
    "#### Best Practices\n",
    "\n",
    "1. **Data Quality**\n",
    "   - Handle missing values before encoding\n",
    "   - Check for rare categories\n",
    "   - Validate category relationships\n",
    "\n",
    "2. **Cross-Validation**\n",
    "   - Compute encodings only on training data\n",
    "   - Apply same encodings to validation/test\n",
    "   - Never leak target information\n",
    "\n",
    "3. **Memory & Performance**\n",
    "   - Monitor memory usage for one-hot encoding\n",
    "   - Use target encoding for high-cardinality\n",
    "   - Consider feature importance in selection\n",
    "\n",
    "4. **Documentation**\n",
    "   - Document encoding decisions\n",
    "   - Save encoding mappings\n",
    "   - Track feature transformations\n",
    "\n",
    "Remember: The goal is to balance information preservation, model performance, and practical constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Criteria Explained\n",
    "\n",
    "To build a decision tree, we need a way to determine the best feature and value to split on at each node. \n",
    "\n",
    "The goal is to create child nodes that are more \"pure\" or homogeneous than their parent node. The method for measuring this purity and choosing the best split differs between regression and classification tasks.\n",
    "\n",
    "### For Regression Tasks (e.g., Predicting House Prices):\n",
    "\n",
    "In regression problems, we're trying to predict a continuous value, like house prices. The goal is to split the data in a way that minimizes the variance of the target variable within each resulting group.\n",
    "\n",
    "The most common metric used for regression trees is the Mean Squared Error (MSE). This is the default criterion used by scikit-learn's DecisionTreeRegressor. Let's break down how this works:\n",
    "\n",
    "Imagine you're a real estate agent with a magical ability to instantly sort houses. Your goal? To group similar houses together as efficiently as possible. This is essentially what a decision tree does, but instead of magical powers, it uses mathematics. Let's dive in!\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "Imagine you're playing a house price guessing game. Your goal is to guess the prices of houses as accurately as possible.\n",
    "\n",
    "Let's say we have 5 houses, and their actual prices are:\n",
    "```pre\n",
    "House 1: £200,000\n",
    "House 2: £250,000\n",
    "House 3: £180,000\n",
    "House 4: £220,000\n",
    "House 5: £300,000\n",
    "```\n",
    "\n",
    "#### Step 1: Calculate the average price\n",
    "`(200,000 + 250,000 + 180,000 + 220,000 + 300,000) / 5 = £230,000`\n",
    "\n",
    "So, your guess for any house would be £230,000.\n",
    "\n",
    "#### Step 2: Calculate how wrong you are for each house\n",
    "```pre\n",
    "House 1: 230,000 - 200,000 = 30,000 \n",
    "House 2: 230,000 - 250,000 = -20,000\n",
    "House 3: 230,000 - 180,000 = 50,000\n",
    "House 4: 230,000 - 220,000 = 10,000\n",
    "House 5: 230,000 - 300,000 = -70,000\n",
    "```\n",
    "\n",
    "#### Step 3: Square these differences\n",
    "```pre\n",
    "House 1: 30,000² = 900,000,000\n",
    "House 2: (-20,000)² = 400,000,000\n",
    "House 3: 50,000² = 2,500,000,000\n",
    "House 4: 10,000² = 100,000,000\n",
    "House 5: (-70,000)² = 4,900,000,000\n",
    "```\n",
    "#### Step 4: Add up all these squared differences\n",
    "`\n",
    "900,000,000 + 400,000,000 + 2,500,000,000 + 100,000,000 + 4,900,000,000 = 8,800,000,000\n",
    "`\n",
    "#### Step 5: Divide by the number of houses\n",
    "\n",
    "`8,800,000,000 ÷ 5 = 1,760,000,000`\n",
    "\n",
    "This final number, 1,760,000,000, is your Mean Squared Error (MSE).\n",
    "\n",
    "In mathematical notation, this whole process looks like:\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2$\n",
    "\n",
    "Let's break this down:\n",
    "- $n$ is the number of houses (5 in our example)\n",
    "- $y_i$ is the actual price of each house\n",
    "- $\\hat{y}$ is your guess (the average price, £230,000 in our example)\n",
    "- $\\sum_{i=1}^n$ means \"add up the following calculation for each house from the first to the last\"\n",
    "- The $i$ in $y_i$ is just a counter, going from 1 to $n$ (1 to 5 in our example)\n",
    "\n",
    "As a python function, this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(actual_prices, predicted_price):\n",
    "    n = len(actual_prices)\n",
    "    squared_errors = []\n",
    "    \n",
    "    for actual_price in actual_prices:\n",
    "        error = predicted_price - actual_price\n",
    "        squared_error = error ** 2\n",
    "        squared_errors.append(squared_error)\n",
    "    \n",
    "    mse = sum(squared_errors) / n\n",
    "    return mse\n",
    "\n",
    "# Example usage\n",
    "actual_prices = [200000, 250000, 180000, 220000, 300000]\n",
    "predicted_price = sum(actual_prices) / len(actual_prices)  # Average price\n",
    "\n",
    "mse = calculate_mse(actual_prices, predicted_price)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Decision Points: Understanding Split Quality in Decision Trees\n",
    "\n",
    "Now, when we split our houses into two groups, we want to measure if this split has made our predictions better. We do this by comparing the error before and after splitting using this formula:\n",
    "\n",
    "$\\Delta MSE = MSE_{before} - (({\\text{fraction of houses in left group} \\times MSE_{left}} + {\\text{fraction of houses in right group} \\times MSE_{right}}))$\n",
    "\n",
    "Let's work through a real example to understand this:\n",
    "\n",
    "Imagine we have 5 houses with these prices:\n",
    "```pre\n",
    "House 1: £200,000\n",
    "House 2: £250,000\n",
    "House 3: £180,000\n",
    "House 4: £220,000\n",
    "House 5: £300,000\n",
    "```\n",
    "\n",
    "We're considering splitting these houses based on whether they have more than 2 bedrooms:\n",
    "- Left group (≤2 bedrooms): Houses 1, 3 (£200,000, £180,000)\n",
    "- Right group (>2 bedrooms): Houses 2, 4, 5 (£250,000, £220,000, £300,000)\n",
    "\n",
    "#### 1. First, let's calculate $MSE_{before}$\n",
    "```pre\n",
    "Mean price = (200k + 250k + 180k + 220k + 300k) ÷ 5 = £230,000\n",
    "\n",
    "Squared differences from mean:\n",
    "House 1: (230k - 200k)² = 900,000,000\n",
    "House 2: (230k - 250k)² = 400,000,000\n",
    "House 3: (230k - 180k)² = 2,500,000,000\n",
    "House 4: (230k - 220k)² = 100,000,000\n",
    "House 5: (230k - 300k)² = 4,900,000,000\n",
    "\n",
    "MSE_before = (900M + 400M + 2,500M + 100M + 4,900M) ÷ 5\n",
    "           = 1,760,000,000\n",
    "```\n",
    "\n",
    "#### 2. Now for the left group (≤2 bedrooms):\n",
    "```pre\n",
    "Mean price = (200k + 180k) ÷ 2 = £190,000\n",
    "\n",
    "Squared differences:\n",
    "House 1: (190k - 200k)² = 100,000,000\n",
    "House 3: (190k - 180k)² = 100,000,000\n",
    "\n",
    "MSE_left = (100M + 100M) ÷ 2 = 100,000,000\n",
    "```\n",
    "\n",
    "#### 3. And the right group (>2 bedrooms):\n",
    "```pre\n",
    "Mean price = (250k + 220k + 300k) ÷ 3 = £256,667\n",
    "\n",
    "Squared differences:\n",
    "House 2: (256.67k - 250k)² = 44,448,889\n",
    "House 4: (256.67k - 220k)² = 1,344,448,889\n",
    "House 5: (256.67k - 300k)² = 1,877,778,889\n",
    "\n",
    "MSE_right = (44.45M + 1,344.45M + 1,877.78M) ÷ 3 = 1,088,892,222\n",
    "```\n",
    "\n",
    "#### 4. Finally, let's put it all together:\n",
    "```pre\n",
    "ΔMSE = MSE_before - ((2/5 × MSE_left) + (3/5 × MSE_right))\n",
    "```\n",
    "The second part calculates our weighted mean MSE after splitting:\n",
    "\n",
    "- Left group has 2/5 of the houses, so we multiply its MSE by 2/5\n",
    "- Right group has 3/5 of the houses, so we multiply its MSE by 3/5\n",
    "\n",
    "This weighting ensures each house contributes equally to our final calculation.\n",
    "\n",
    "Let's solve it:\n",
    "```pre\n",
    "     = 1,760,000,000 - ((2/5 × 100,000,000) + (3/5 × 1,088,892,222))\n",
    "     = 1,760,000,000 - (40,000,000 + 653,335,333)\n",
    "     = 1,760,000,000 - 693,335,333        # This is our weighted mean MSE after splitting\n",
    "     = 1,066,664,667                      # ΔMSE: The reduction in prediction error\n",
    "```\n",
    "The ΔMSE (1,066,664,667) represents the difference between the original MSE and the weighted average MSE after splitting. This number is always non-negative due to a fundamental property of squared errors:\n",
    "\n",
    "1. MSE is always positive (we're squaring differences from the mean)\n",
    "2. When we split a group:\n",
    "   - The parent uses one mean for all samples\n",
    "   - Each subgroup uses its own mean, which minimises squared errors for that subgroup\n",
    "   - The subgroup means must perform at least as well as the parent mean (due to minimising squared errors locally)\n",
    "   - Therefore, the weighted average MSE of subgroups cannot exceed the parent MSE\n",
    "\n",
    "Therefore:\n",
    "- ΔMSE > 0 means the split has improved predictions (as in our case)\n",
    "- ΔMSE = 0 means the split makes no difference\n",
    "- ΔMSE < 0 is mathematically impossible\n",
    "\n",
    "\n",
    "The larger the ΔMSE, the more effective the split is at creating subgroups with similar house prices. Our large ΔMSE of 1,066,664,667 indicates this is a very effective split.\n",
    "\n",
    "### A simplified decision tree algorithm in python:\n",
    "In practise, you'd use a library like `sklearn` to build a decision tree, but here's a simplified version in python to illustrate the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class House:\n",
    "    def __init__(self, features: Dict[str, float], price: float):\n",
    "        self.features = features\n",
    "        self.price = price\n",
    "\n",
    "def find_best_split(houses: List[House], feature: str) -> tuple:\n",
    "    values = sorted(set(house.features[feature] for house in houses))\n",
    "    \n",
    "    best_split = None\n",
    "    best_delta_mse = float('-inf')\n",
    "\n",
    "    for i in range(len(values) - 1):\n",
    "        split_point = (values[i] + values[i+1]) / 2\n",
    "        left = [h for h in houses if h.features[feature] < split_point]\n",
    "        right = [h for h in houses if h.features[feature] >= split_point]\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            continue\n",
    "\n",
    "        mse_before = np.var([h.price for h in houses])\n",
    "        mse_left = np.var([h.price for h in left])\n",
    "        mse_right = np.var([h.price for h in right])\n",
    "\n",
    "        delta_mse = mse_before - (len(left)/len(houses) * mse_left + len(right)/len(houses) * mse_right)\n",
    "\n",
    "        if delta_mse > best_delta_mse:\n",
    "            best_delta_mse = delta_mse\n",
    "            best_split = split_point\n",
    "\n",
    "    return best_split, best_delta_mse\n",
    "\n",
    "def build_tree(houses: List[House], depth: int = 0, max_depth: int = 3) -> Dict[str, Any]:\n",
    "    if depth == max_depth or len(houses) < 2:\n",
    "        return {'type': 'leaf', 'value': np.mean([h.price for h in houses])}\n",
    "\n",
    "    features = houses[0].features.keys()\n",
    "    best_feature = None\n",
    "    best_split = None\n",
    "    best_delta_mse = float('-inf')\n",
    "\n",
    "    for feature in features:\n",
    "        split, delta_mse = find_best_split(houses, feature)\n",
    "        if delta_mse > best_delta_mse:\n",
    "            best_feature = feature\n",
    "            best_split = split\n",
    "            best_delta_mse = delta_mse\n",
    "\n",
    "    if best_feature is None:\n",
    "        return {'type': 'leaf', 'value': np.mean([h.price for h in houses])}\n",
    "\n",
    "    left = [h for h in houses if h.features[best_feature] < best_split]\n",
    "    right = [h for h in houses if h.features[best_feature] >= best_split]\n",
    "\n",
    "    return {\n",
    "        'type': 'node',\n",
    "        'feature': best_feature,\n",
    "        'split': best_split,\n",
    "        'left': build_tree(left, depth + 1, max_depth),\n",
    "        'right': build_tree(right, depth + 1, max_depth)\n",
    "    }\n",
    "\n",
    "def predict(tree: Dict[str, Any], house: House) -> float:\n",
    "    if tree['type'] == 'leaf':\n",
    "        return tree['value']\n",
    "    \n",
    "    if house.features[tree['feature']] < tree['split']:\n",
    "        return predict(tree['left'], house)\n",
    "    else:\n",
    "        return predict(tree['right'], house)\n",
    "\n",
    "# Example usage\n",
    "houses = [\n",
    "    House({'bedrooms': 2, 'area': 80, 'distance_to_tube': 15}, 200),\n",
    "    House({'bedrooms': 3, 'area': 120, 'distance_to_tube': 10}, 250),\n",
    "    House({'bedrooms': 2, 'area': 75, 'distance_to_tube': 20}, 180),\n",
    "    House({'bedrooms': 3, 'area': 100, 'distance_to_tube': 5}, 220),\n",
    "    House({'bedrooms': 4, 'area': 150, 'distance_to_tube': 2}, 300),\n",
    "    House({'bedrooms': 3, 'area': 110, 'distance_to_tube': 12}, 240),\n",
    "    House({'bedrooms': 2, 'area': 70, 'distance_to_tube': 25}, 190),\n",
    "    House({'bedrooms': 4, 'area': 140, 'distance_to_tube': 8}, 280),\n",
    "    House({'bedrooms': 3, 'area': 130, 'distance_to_tube': 6}, 260),\n",
    "    House({'bedrooms': 2, 'area': 85, 'distance_to_tube': 18}, 210)\n",
    "]\n",
    "\n",
    "tree = build_tree(houses)\n",
    "\n",
    "def print_tree(node, indent=\"\"):\n",
    "    if node['type'] == 'leaf':\n",
    "        print(f\"{indent}Predict price: £{node['value']:.2f}k\")\n",
    "    else:\n",
    "        print(f\"{indent}{node['feature']} < {node['split']:.2f}\")\n",
    "        print(f\"{indent}If True:\")\n",
    "        print_tree(node['left'], indent + \"  \")\n",
    "        print(f\"{indent}If False:\")\n",
    "        print_tree(node['right'], indent + \"  \")\n",
    "\n",
    "print_tree(tree)\n",
    "\n",
    "# Test prediction\n",
    "new_house = House({'bedrooms': 3, 'area': 105, 'distance_to_tube': 7}, 0)  # price set to 0 as it's unknown\n",
    "predicted_price = predict(tree, new_house)\n",
    "print(f\"\\nPredicted price for new house: £{predicted_price:.2f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) vs Mean Absolute Error (MAE)\n",
    "\n",
    "When evaluating our decision tree's performance, we need to understand the difference between training metrics and evaluation metrics.\n",
    "\n",
    "![mean-squared-error-mean-absolute-error](../static/mean-squared-error-mean-absolute-error.png)\n",
    "\n",
    "Our decision tree algorithm uses MSE as the splitting criterion but measures final performance using MAE. \n",
    "\n",
    "Here's why we use these different metrics:\n",
    "\n",
    "##### 1. Mean Squared Error (MSE)\n",
    "\n",
    "   **Calculation:** (predicted house price - actual house price)²\n",
    "\n",
    "   For example, if we predict £200,000 for a house that's actually worth £150,000, the error is £50,000 and MSE is £50,000² = £2.5 billion\n",
    "\n",
    "   **Visualisation**\n",
    "\n",
    "   If we plot how wrong our house price prediction is (like £50,000 too high or -£50,000 too low) on the x-axis, and plot the squared value of this error (like £2.5 billion) on the y-axis, we get a U-shaped curve. Because MSE squares the errors, it gives more weight to data points that are further from the mean, making it a good measure of variance within groups.\n",
    "\n",
    "   **Purpose**\n",
    "\n",
    "   The decision tree uses MSE to decide where to split data because minimizing MSE is equivalent to minimizing the variance within each group, which helps find splits that create distinct groups of house prices.\n",
    "\n",
    "  ##### 2. Mean Absolute Error (MAE)\n",
    "\n",
    "   **Calculation:** |predicted house price - actual house price|\n",
    "\n",
    "   Using the same example, if we predict £200,000 for a £150,000 house, MAE is |£50,000| = £50,000\n",
    "\n",
    "   **Visualisation**\n",
    "\n",
    "   If we plot how wrong our prediction is on the x-axis (like £50,000 too high or -£50,000 too low), and plot the absolute value of this error on the y-axis (always positive, like £50,000), we get a V-shaped curve\n",
    "\n",
    "   **Purpose**\n",
    "   \n",
    "   We use MAE to evaluate our final model because it's easier to understand - it directly tells us how many pounds we're off by on average\n",
    "\n",
    "\\\n",
    "The decision tree uses MSE's mathematical properties to make splitting decisions, but we report MAE because \"off by £50,000 on average\" makes more sense than \"off by £2.5 billion squared pounds\"!\n",
    "\n",
    "\\\n",
    "Here's an example to illustrate the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "y_true = [100, 200, 300]\n",
    "y_pred = [90, 210, 320]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```pre\n",
    "Mean Squared Error: 200.00\n",
    "Mean Absolute Error: 13.33\n",
    "```\n",
    "\n",
    "In this example, MSE and MAE provide different views of the error. MSE is more sensitive to the larger error (20) in the third prediction, while MAE treats all errors equally.\n",
    "\n",
    "For house price prediction, MAE is often preferred as it directly translates to the average error in pounds. However, MSE is still commonly used as a splitting criterion in decision trees because minimizing MSE helps create groups with similar target values by minimizing the variance within each group.\n",
    "\n",
    "### For Classification Tasks (e.g., Predicting if a House Will Sell Quickly):\n",
    "\n",
    "In classification problems, we're trying to predict a categorical outcome, like whether a house will sell quickly or not. The goal is to split the data in a way that maximizes the \"purity\" of the classes within each resulting group.\n",
    "\n",
    "There are several metrics used for classification trees, with the most common being Gini Impurity and Entropy. These metrics measure how mixed the classes are within a group.\n",
    "\n",
    "Let's explore how different distributions of marbles affect our measures of impurity. We will then explore information gain, a measure used in conjuction with impurity metrics to decide how to split the data.\n",
    "\n",
    "We'll use red marbles to represent quick-selling houses and blue marbles for slow-selling houses.\n",
    "\n",
    "#### 1. Gini Impurity:\n",
    "   Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution in the set.\n",
    "\n",
    "   Formula: $Gini = 1 - \\sum_{i=1}^{c} (p_i)^2$\n",
    "\n",
    "   Where $c$ is the number of classes and $p_i$ is the probability of an object being classified to a particular class.\n",
    "\n",
    "   Let's compare three scenarios:\n",
    "\n",
    "```pre\n",
    "   a) 10 marbles: 7 red, 3 blue\n",
    "      Fraction of red = 7/10 = 0.7\n",
    "      Fraction of blue = 3/10 = 0.3\n",
    "      \n",
    "      Gini = 1 - (0.7² + 0.3²) = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42\n",
    "```\n",
    "\n",
    "```pre\n",
    "   b) 10 marbles: 5 red, 5 blue\n",
    "      Fraction of red = 5/10 = 0.5\n",
    "      Fraction of blue = 5/10 = 0.5\n",
    "      \n",
    "      Gini = 1 - (0.5² + 0.5²) = 1 - (0.25 + 0.25) = 1 - 0.5 = 0.5\n",
    "      most impure set\n",
    "```\n",
    "\n",
    "```pre\n",
    "   c) 10 marbles: 9 red, 1 blue\n",
    "      Fraction of red = 9/10 = 0.9\n",
    "      Fraction of blue = 1/10 = 0.1\n",
    "      \n",
    "      Gini = 1 - (0.9² + 0.1²) = 1 - (0.81 + 0.01) = 1 - 0.82 = 0.18\n",
    "      purest set\n",
    "```\n",
    "\n",
    "**The lower the Gini Impurity, the purer the set. Scenario (c) has the lowest Gini Impurity, indicating it's the most homogeneous.**\n",
    "\n",
    "#### 2. Entropy:\n",
    "\n",
    "Entropy is another measure of impurity, based on the concept of information theory. It quantifies the amount of uncertainty or randomness in the data.\n",
    "\n",
    "$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
    "\n",
    "Where $c$ is the number of classes and $p_i$ is the probability of an object being classified to a particular class.\n",
    "\n",
    "Imagine you're playing a guessing game with marbles in a bag. Entropy measures how surprised you'd be when pulling out a marble. The more mixed the colours, the more surprised you might be, and the higher the entropy.\n",
    "\n",
    "#### Let's use our marble scenarios:\n",
    "\n",
    "10 marbles: 7 red, 3 blue\n",
    "\n",
    "To calculate entropy, we follow these steps:\n",
    "\n",
    "1. Calculate the fraction of each colour:\n",
    "```pre\n",
    "   Red: 7/10 = 0.7\n",
    "   Blue: 3/10 = 0.3\n",
    "```\n",
    "\n",
    "2. For each colour, multiply its fraction by the log2 of its fraction:   \n",
    "```pre\n",
    "   Red: 0.7 × log2(0.7) = 0.7 × -0.5146 = -0.360\n",
    "   Blue: 0.3 × log2(0.3) = 0.3 × -1.7370 = -0.5211\n",
    "```\n",
    "\n",
    "3. Sum these values and negate the result:\n",
    "```pre\n",
    "Entropy = -(-0.3602 + -0.5211) = 0.8813\n",
    "```\n",
    "\n",
    "#### Let's do this for all scenarios:\n",
    "\n",
    "a) 7 red, 3 blue\n",
    "```pre\n",
    "   Entropy = 0.8813\n",
    "```\n",
    "b) 5 red, 5 blue\n",
    "```pre\n",
    "   Red: 0.5 × log2(0.5) = 0.5 × -1 = -0.5\n",
    "   Blue: 0.5 × log2(0.5) = 0.5 × -1 = -0.5\n",
    "   Entropy = -(-0.5 + -0.5) = 1\n",
    "\n",
    "Highest entropy, least predictable set\n",
    "```\n",
    "\n",
    "c) 9 red, 1 blue\n",
    "```pre\n",
    "   Red: 0.9 × log2(0.9) = 0.9 × -0.1520 = -0.1368\n",
    "   Blue: 0.1 × log2(0.1) = 0.1 × -3.3219 = -0.3322\n",
    "   Entropy = -(-0.1368 + -0.3322) = 0.4690\n",
    "\n",
    "Lowest entropy, most predictable set\n",
    "```\n",
    "\n",
    "Lower entropy means less surprise or uncertainty. Scenario (c) has the lowest entropy, confirming it's the most predictable (or least mixed) set.\n",
    "\n",
    "In Python, we could calculate entropy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_entropy(marbles):\n",
    "    total = sum(marbles.values())\n",
    "    entropy = 0\n",
    "    for count in marbles.values():\n",
    "        fraction = count / total\n",
    "        entropy -= fraction * math.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "scenario_a = {\"red\": 7, \"blue\": 3}\n",
    "entropy_a = calculate_entropy(scenario_a)\n",
    "print(f\"Entropy for scenario A: {entropy_a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Information Gain:\n",
    "\n",
    "Information Gain measures how much a split improves our ability to predict the outcome. It's a way of measuring how much better you've sorted your marbles after dividing them into groups.\n",
    "\n",
    "Formula: $IG(T, a) = I(T) - \\sum_{v \\in values(a)} \\frac{|T_v|}{|T|} I(T_v)$\n",
    "\n",
    "Where:\n",
    "- $T$ is the parent set\n",
    "- $a$ is the attribute on which the split is made\n",
    "- $v$ represents each possible value of attribute $a$\n",
    "- $T_v$ is the subset of $T$ for which attribute $a$ has value $v$\n",
    "- $I(T)$ is the impurity measure (Entropy or Gini) of set $T$\n",
    "\n",
    "\n",
    "#### Let's use a scenario to calculate Information Gain:\n",
    "\n",
    "We have 20 marbles total, and we're considering splitting them based on a feature (e.g., house size: small or large).\n",
    "```pre\n",
    "Before split: 12 red, 8 blue\n",
    "```\n",
    "\n",
    "Step 1: Calculate the entropy before the split\n",
    "```pre\n",
    "Entropy_before = 0.9710 (calculated as we did above)\n",
    "```\n",
    "\n",
    "After split:\n",
    "```pre\n",
    "Small houses: 8 red, 2 blue\n",
    "Large houses: 4 red, 6 blue\n",
    "```\n",
    "Step 2: Calculate entropy for each group after the split\n",
    "Entropy_small = 0.7219 (calculated for 8 red, 2 blue)\n",
    "Entropy_large = 0.9710 (calculated for 4 red, 6 blue)\n",
    "\n",
    "Step 3: Calculate the weighted average of the split entropies\n",
    "```pre\n",
    "Weight_small = 10/20 = 0.5 (half the marbles are in small houses)\n",
    "Weight_large = 10/20 = 0.5 (half the marbles are in large houses)\n",
    "Weighted_entropy_after = (0.5 × 0.7219) + (0.5 × 0.9710) = 0.8465\n",
    "```\n",
    "\n",
    "Step 4: Calculate Information Gain\n",
    "```pre\n",
    "Information Gain = Entropy_before - Weighted_entropy_after\n",
    "                 = 0.9710 - 0.8465\n",
    "                 = 0.1245\n",
    "```\n",
    "\n",
    "This positive Information Gain indicates that the split has improved our ability to predict marble colours (or in our house analogy, to predict if a house will sell quickly).\n",
    "\n",
    "#### In Python, we could calculate Information Gain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(before, after):\n",
    "    entropy_before = calculate_entropy(before)\n",
    "    \n",
    "    total_after = sum(sum(group.values()) for group in after)\n",
    "    weighted_entropy_after = sum(\n",
    "        (sum(group.values()) / total_after) * calculate_entropy(group)\n",
    "        for group in after\n",
    "    )\n",
    "    \n",
    "    return entropy_before - weighted_entropy_after\n",
    "\n",
    "# Example usage\n",
    "before_split = {\"red\": 12, \"blue\": 8}\n",
    "after_split = [\n",
    "    {\"red\": 8, \"blue\": 2},  # Small houses\n",
    "    {\"red\": 4, \"blue\": 6}   # Large houses\n",
    "]\n",
    "\n",
    "info_gain = calculate_information_gain(before_split, after_split)\n",
    "print(f\"Information Gain: {info_gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison: Splits with Different Information Gains\n",
    "\n",
    "The decision tree algorithm always chooses the split that provides the most Information Gain. \n",
    "\n",
    "Let's consider two potential splits of our 20 marbles:\n",
    "\n",
    "1. Split by house size (small vs large):\n",
    "   - Small houses: 8 red, 2 blue\n",
    "   - Large houses: 4 red, 6 blue\n",
    "   - Information Gain: 0.1245\n",
    "\n",
    "2. Split by garage presence:\n",
    "   - Houses with garage: 6 red, 4 blue\n",
    "   - Houses without garage: 6 red, 4 blue\n",
    "   - Information Gain: 0\n",
    "\n",
    "The algorithm would choose the split by house size because it provides more Information Gain. \n",
    "\n",
    "Zero Information Gain occurs when a split doesn't change the distribution of the target variable (in this case, marble colours or house selling speed). This happens when the proportions in each resulting group are identical to the proportions in the parent group.\n",
    "\n",
    "In practice, splits with exactly zero Information Gain are rare. More commonly, you'll see splits with varying degrees of positive Information Gain, and the algorithm will choose the one with the highest value.\n",
    "\n",
    "Features that provide little or no Information Gain are typically less valuable for prediction and should be considered for removal from the model. Eliminating these low-impact features can simplify the model, potentially improving its generalization ability and computational efficiency without significantly compromising predictive performance.\n",
    "\n",
    "## Interpretability and Visualization\n",
    "\n",
    "After understanding how decision trees split data using criteria like MSE and Gini impurity, it's crucial to explore one of their greatest strengths: interpretability.\n",
    "\n",
    "Unlike many machine learning models that act as \"black boxes,\" decision trees provide clear insights into their decision-making process.\n",
    "\n",
    "### Why Interpretability Matters\n",
    "\n",
    "For house price prediction, interpretability allows us to:\n",
    "- Explain predictions to stakeholders (buyers, sellers, agents)\n",
    "- Validate model logic against domain knowledge\n",
    "- Identify potential biases or errors\n",
    "- Meet regulatory requirements for transparency\n",
    "\n",
    "### How to Interpret Decision Trees\n",
    "\n",
    "#### 1. Reading Tree Structure\n",
    "\n",
    "Consider this simplified tree for house prices:\n",
    "```\n",
    "Area > 2000 sq ft?\n",
    "├── Yes: Location = \"Chelsea\"?\n",
    "│   ├── Yes: £2.5M (n=50)\n",
    "│   └── No: £1.8M (n=150)\n",
    "└── No: Number of bedrooms > 2?\n",
    "    ├── Yes: £950K (n=200)\n",
    "    └── No: £650K (n=100)\n",
    "```\n",
    "\n",
    "Each node tells us:\n",
    "- The decision rule (e.g., \"Area > 2000 sq ft?\")\n",
    "- The number of samples (n)\n",
    "- The predicted value (for leaf nodes)\n",
    "\n",
    "#### 2. Decision Paths\n",
    "\n",
    "Each path from root to leaf represents a complete prediction rule. For example:\n",
    "- IF area > 2000 sq ft AND location = \"Chelsea\" THEN price = £2.5M\n",
    "- IF area ≤ 2000 sq ft AND bedrooms > 2 THEN price = £950K\n",
    "\n",
    "This allows us to provide clear explanations for any prediction.\n",
    "\n",
    "#### 3. Feature Importance\n",
    "\n",
    "Decision trees naturally reveal feature importance through:\n",
    "\n",
    "a) Position in tree:\n",
    "- Features closer to root affect more predictions\n",
    "- Top-level splits handle larger portions of data\n",
    "\n",
    "b) Usage frequency:\n",
    "- Features used multiple times may be more important\n",
    "- Different contexts show feature interactions\n",
    "\n",
    "c) Impact on predictions:\n",
    "- Splits that create large value differences are important\n",
    "- Features that reduce variance significantly\n",
    "\n",
    "## Visualizing Decision Trees\n",
    "\n",
    "While our simple example above is easy to read, real trees can be much more complex. Here are key visualization approaches:\n",
    "\n",
    "1. **Full Tree Visualization**\n",
    "   - Shows complete structure\n",
    "   - Good for understanding overall patterns\n",
    "   - Can become overwhelming for deep trees\n",
    "\n",
    "2. **Pruned Tree Views**\n",
    "   - Show top few levels\n",
    "   - Focus on most important decisions\n",
    "   - More manageable for presentation\n",
    "\n",
    "3. **Feature Importance Plots**\n",
    "   - Bar charts of feature importance\n",
    "   - Easier to digest than full trees\n",
    "   - Good for high-level insights\n",
    "\n",
    "## Understanding Bias, Variance, Tree Depth and Complexity\n",
    "\n",
    "### Bias\n",
    "- **The error introduced by approximating a real-world problem with a simplified model**\n",
    "- Represents how far off the model's predictions are from the true values on average\n",
    "- High bias means the model consistently misses the true patterns (underfitting)\n",
    "\n",
    "    1. **Shallow Trees (High Bias)**\n",
    "    ```pre\n",
    "    Root: Area > 2000 sq ft?\n",
    "    ├── Yes: £2M\n",
    "    └── No: £800K\n",
    "    ```\n",
    "    - Very simple rules\n",
    "    - Misses many important factors\n",
    "    - Similar predictions for different houses\n",
    "\n",
    "### Variance\n",
    "- **The model's sensitivity to fluctuations in the training data**\n",
    "- Represents how much predictions change with different training sets\n",
    "- High variance means predictions vary significantly with small changes in training data (overfitting)\n",
    "\n",
    "    2. **Deep Trees (High Variance)**\n",
    "    ```pre\n",
    "    Root: Area > 2000 sq ft?\n",
    "    ├── Yes: Location = \"Chelsea\"?\n",
    "    │   ├── Yes: Bedrooms > 3?\n",
    "    │   │   ├── Yes: Garden = True?\n",
    "    │   │   │   ├── Yes: £3.2M\n",
    "    │   │   │   └── No: £2.9M\n",
    "    ...\n",
    "    ```\n",
    "    - Very specific rules\n",
    "    - Might memorize training data\n",
    "    - Can make unstable predictions\n",
    "\n",
    "\n",
    "## Identifying the Bias/Variance Tradeoff\n",
    "\n",
    "Consider these scenarios:\n",
    "\n",
    "### Scenario 1: Too Simple (High Bias)\n",
    "```python\n",
    "# Example of underfitting\n",
    "predictions = {\n",
    "    \"2500 sq ft in Chelsea\": £2M,\n",
    "    \"2500 sq ft in Hackney\": £2M,  # Same prediction despite location\n",
    "    \"2500 sq ft in Mayfair\": £2M   # Location ignored\n",
    "}\n",
    "```\n",
    "\n",
    "### Scenario 2: Too Complex (High Variance)\n",
    "```python\n",
    "# Example of overfitting\n",
    "predictions = {\n",
    "    \"2500 sq ft, Chelsea, 4 bed, garden\": £3.2M,\n",
    "    \"2500 sq ft, Chelsea, 4 bed, no garden\": £2.9M,\n",
    "    # Small changes lead to large prediction differences\n",
    "    \"2499 sq ft, Chelsea, 4 bed, garden\": £2.7M  # Just 1 sq ft difference\n",
    "}\n",
    "```\n",
    "\n",
    "### Scenario 3: Balanced\n",
    "```python\n",
    "# Example of good balance\n",
    "predictions = {\n",
    "    \"Large house in Chelsea\": £2.5M-3.0M,\n",
    "    \"Large house in Hackney\": £1.5M-2.0M,\n",
    "    # Reasonable variations based on key features\n",
    "}\n",
    "```\n",
    "\n",
    "## Managing the Bias/Variance Tradeoff\n",
    "\n",
    "When building a decision tree, we need to find the right balance between making it too simple (underfitting) and too complex (overfitting). \n",
    "\n",
    "Let's explore how to find this balance.\n",
    "\n",
    "### 1. Control Tree Complexity\n",
    "We can control how detailed our tree becomes using parameters:\n",
    "- Maximum depth (how many questions we can ask)\n",
    "- Minimum samples per leaf (how many houses needed for a conclusion)\n",
    "- Minimum improvement threshold (how much better a split needs to be)\n",
    "\n",
    "### 2. Understanding Training vs Validation Error\n",
    "\n",
    "Training error is how well our model predicts house prices for houses it learned from, while validation error is how well it predicts prices for houses it hasn't seen before.\n",
    "\n",
    "Think of it like this:\n",
    "- **Training Error**: How well you can predict prices of houses you studied\n",
    "- **Validation Error**: How well you can predict prices of new houses\n",
    "\n",
    "Let's look at how these errors change as we make our tree more complex:\n",
    "\n",
    "\n",
    "```code\n",
    "Depth   Training Error  Validation Error   What's Happening\n",
    "3       £250K           £260K              #  Tree is too simple\n",
    "                                           #  - Both errors are high\n",
    "                                           #  - Tree isn't learning enough patterns\n",
    " \n",
    "5       £180K           £200K              #  Tree is just right\n",
    "                                           #  - Both errors are reasonable\n",
    "                                           #  - Tree learns genuine patterns\n",
    " \n",
    "7       £120K           £220K              #  Tree is getting too complex\n",
    "                                           #  - Training error keeps dropping\n",
    "                                           #  - Validation error starts rising\n",
    "                                           #  - Starting to memorise training data\n",
    " \n",
    "10      £50K            £300K              #  Tree is way too complex\n",
    "                                           #  - Training error is very low\n",
    "                                           #  - Validation error is very high\n",
    "                                           #  - Tree has memorised training data\n",
    "```\n",
    "\n",
    "### 3. Finding the Best Depth Using Cross-Validation\n",
    "\n",
    "To find the best depth, we:\n",
    "1. Try different depths\n",
    "2. Test each one on multiple splits of our data\n",
    "3. Choose the depth with lowest validation error\n",
    "\n",
    "```python\n",
    "# Test different tree depths\n",
    "depths = [3, 5, 7, 10, 15]\n",
    "for depth in depths:\n",
    "    scores = cross_validate(tree, depth)\n",
    "    # Choose depth where validation error is lowest\n",
    "```\n",
    "\n",
    "In our example, depth=5 gives the best balance because:\n",
    "- Training error (£180K) shows it's learning meaningful patterns\n",
    "- Validation error (£200K) shows these patterns generalise well to new houses\n",
    "- The gap between training and validation error is reasonable\n",
    "\n",
    "This balance means our tree has learned genuine relationships in house prices without memorising specific examples from the training data.\n",
    "\n",
    "## Visual Indicators of Bias/Variance\n",
    "\n",
    "### 1. Learning Curves\n",
    "\n",
    "![model-complexity-bias-variance-contributing-to-total-error](../static/model-complexity-bias-variance-contributing-to-total-error.png)\n",
    "\n",
    "As the model complexity increases, the training error decreases and the validation error increases. \n",
    "\n",
    "Total error is the sum of bias (the error introduced by approximating a real-world problem with a simplified model) and variance (the error caused by the model's sensitivity to fluctuations in the training data).\n",
    "\n",
    "Underfitting occurs when the model is too simple (high bias), resulting in both training set and validation set total errors being high.\n",
    "\n",
    "Overfitting occurs when the model is too complex (high variance), resulting in a large gap between training and validation set total errors.\n",
    "\n",
    "![model-complexity-error-training-test-samples](../static/model-complexity-error-training-test-samples.png)\n",
    "\n",
    "\n",
    "![performance-model-complexity-training-validation-sets-overfitting](../static/performance-model-complexity-training-validation-sets-overfitting.png)\n",
    "\n",
    "\n",
    "## Practical Guidelines\n",
    "\n",
    "1. **Start Simple**\n",
    "   - Begin with shallow trees\n",
    "   - Add complexity gradually\n",
    "   - Monitor performance changes\n",
    "\n",
    "2. **Use Domain Knowledge**\n",
    "   - Consider reasonable decision granularity\n",
    "   - Identify important feature interactions\n",
    "   - Set meaningful constraints\n",
    "\n",
    "3. **Regular Validation**\n",
    "   - Test on unseen data\n",
    "   - Check prediction stability\n",
    "   - Monitor for overfitting signs\n",
    "\n",
    "Understanding this tradeoff is crucial for:\n",
    "- Setting appropriate tree depth\n",
    "- Choosing regularization parameters\n",
    "- Deciding when to use ensemble methods\n",
    "\n",
    "Now that we understand how to build well-balanced decision trees, we need to know which features are driving their decisions. \n",
    "\n",
    "In the next section, we'll explore how decision trees determine which features are most important for making predictions (like whether location matters more than size for house prices) and discover their advanced capabilities in handling different types of data. This knowledge is crucial for building more effective models and gaining insights from your data.\n",
    "\n",
    "## Feature Importance and Advanced Capabilities\n",
    "\n",
    "After understanding how to balance bias and variance, it's crucial to explore how decision trees determine feature importance and their advanced capabilities in handling different types of data and relationships.\n",
    "\n",
    "### Feature Importance in Decision Trees\n",
    "\n",
    "#### How Trees Measure Importance\n",
    "\n",
    "1. **Split Position**\n",
    "   ```pre\n",
    "   Root (Most Important)\n",
    "   ├── Level 1\n",
    "   │   ├── Level 2\n",
    "   │   └── Level 2\n",
    "   └── Level 1\n",
    "       ├── Level 2\n",
    "       └── Level 2\n",
    "   ```\n",
    "   - Higher splits affect more samples\n",
    "   - Root splits are most influential\n",
    "   - Earlier splits indicate greater importance\n",
    "\n",
    "2. **Impurity Reduction**\n",
    "   ```python\n",
    "   # Conceptual example\n",
    "   importance = sum([\n",
    "       node.samples * node.impurity_reduction\n",
    "       for node in feature_splits\n",
    "   ])\n",
    "   ```\n",
    "   - Larger reductions = more important\n",
    "   - Weighted by number of samples affected\n",
    "   - Accumulated across all splits using feature\n",
    "\n",
    "3. **Usage Frequency**\n",
    "   - Features used multiple times may be more important\n",
    "   - Different contexts show feature interactions\n",
    "   - Patterns of use reveal complexity of relationship\n",
    "\n",
    "#### Example: London Housing Features\n",
    "\n",
    "Typical importance hierarchy:\n",
    "```pre\n",
    "1. Location (30-40% importance)\n",
    "   - Primary price determinant\n",
    "   - Used in multiple splits\n",
    "   - Strong predictor at all levels\n",
    "\n",
    "2. Area (20-30% importance)\n",
    "   - Key size indicator\n",
    "   - Often appears near root\n",
    "   - Clear price relationship\n",
    "\n",
    "3. Property Type (15-20% importance)\n",
    "   - Important categorical feature\n",
    "   - Interacts with location/area\n",
    "   - Distinct price levels\n",
    "\n",
    "4. Bedrooms (10-15% importance)\n",
    "   - Secondary size indicator\n",
    "   - Often appears lower in tree\n",
    "   - Correlated with area\n",
    "```\n",
    "\n",
    "### Advanced Capabilities\n",
    "\n",
    "#### 1. Handling Non-linear Relationships\n",
    "\n",
    "Trees naturally capture non-linear patterns:\n",
    "```pre\n",
    "Price vs. Area Relationship:\n",
    "< 1000 sq ft: £500K\n",
    "1000-2000 sq ft: £1M\n",
    "2000-3000 sq ft: £2.5M\n",
    "> 3000 sq ft: £5M\n",
    "```\n",
    "- No assumption of linearity\n",
    "- Step-wise approximation\n",
    "- Automatic threshold finding\n",
    "\n",
    "### 2. Feature Interaction Detection\n",
    "\n",
    "Trees automatically find interactions:\n",
    "```pre\n",
    "Area > 2000 sq ft?\n",
    "├── Yes: Location = \"Chelsea\"?\n",
    "│   ├── Yes: £3M (premium location + large)\n",
    "│   └── No: £2M (large but standard location)\n",
    "└── No: Location = \"Chelsea\"?\n",
    "    ├── Yes: £1.5M (premium location but small)\n",
    "    └── No: £800K (standard location and small)\n",
    "```\n",
    "- Different location effects by size\n",
    "- Automatic path discovery\n",
    "- Hierarchical relationships\n",
    "\n",
    "#### 3. Missing Value Handling\n",
    "\n",
    "Trees can handle missing data through:\n",
    "\n",
    "1. **Surrogate Splits**\n",
    "```\n",
    "Primary Split: Area > 2000 sq ft?\n",
    "Surrogate: Bedrooms > 3?  # If area is missing\n",
    "```\n",
    "\n",
    "2. **Built-in Mechanisms**\n",
    "```python\n",
    "# Conceptual handling\n",
    "if value_is_missing(area):\n",
    "    use_surrogate_split(bedrooms)\n",
    "else:\n",
    "    use_primary_split(area)\n",
    "```\n",
    "\n",
    "#### 4. Categorical Variable Handling\n",
    "\n",
    "Automatic handling of:\n",
    "- Property types\n",
    "- Locations\n",
    "- Amenities\n",
    "- No encoding needed\n",
    "\n",
    "### Limitations and Solutions\n",
    "\n",
    "#### 1. Instability\n",
    "```python\n",
    "# Small data changes can cause different splits\n",
    "# Example:\n",
    "data1 = [..., 999, 1001, ...]  # Splits at 1000\n",
    "data2 = [..., 1001, 999, ...]  # Splits elsewhere\n",
    "```\n",
    "\n",
    "**Solution:** Ensemble methods\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "- Bagging\n",
    "\n",
    "#### 2. Linear Relationship Inefficiency\n",
    "```\n",
    "True relationship: price = 1000 * area\n",
    "Tree approximation:\n",
    "area <= 1000: price = 1M\n",
    "area <= 2000: price = 2M\n",
    "area <= 3000: price = 3M\n",
    "```\n",
    "\n",
    "**Solution:** Feature engineering\n",
    "- Create derived features\n",
    "- Transform variables\n",
    "- Combine with linear models\n",
    "\n",
    "#### 3. Extrapolation Limitation\n",
    "```python\n",
    "# Training data: areas up to 3000 sq ft\n",
    "# Cannot reliably predict for:\n",
    "area = 4000  # Beyond training range\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Domain constraints\n",
    "- Careful feature ranges\n",
    "- Hybrid models\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "1. **Feature Selection**\n",
    "   - Use importance scores to select features\n",
    "   - Remove redundant variables\n",
    "   - Focus on strongest predictors\n",
    "\n",
    "2. **Model Improvement**\n",
    "   - Identify key interaction patterns\n",
    "   - Guide feature engineering\n",
    "   - Inform data collection\n",
    "\n",
    "3. **Business Insights**\n",
    "   - Understand market drivers\n",
    "   - Identify value factors\n",
    "   - Guide decision making\n",
    "\n",
    "Understanding these capabilities helps in:\n",
    "- Choosing appropriate problems for decision trees\n",
    "- Setting realistic expectations\n",
    "- Leveraging tree strengths while mitigating weaknesses\n",
    "\n",
    "In the next section, we'll explore practical limitations and ethical considerations when using decision trees for real-world applications.\n",
    "\n",
    "## Limitations and Ethical Considerations\n",
    "\n",
    "Having explored the capabilities of decision trees, it's crucial to understand their limitations and the ethical considerations in their application, particularly in sensitive domains like housing prices.\n",
    "\n",
    "### Technical Limitations\n",
    "\n",
    "#### 1. Decision Boundary Limitations\n",
    "\n",
    "```python\n",
    "# Decision trees create rectangular decision boundaries\n",
    "# Example regions in feature space:\n",
    "regions = {\n",
    "    \"Chelsea, >2000 sq ft\": \"High Price\",\n",
    "    \"Chelsea, ≤2000 sq ft\": \"Medium Price\",\n",
    "    \"Other, >2000 sq ft\": \"Medium Price\",\n",
    "    \"Other, ≤2000 sq ft\": \"Low Price\"\n",
    "}\n",
    "\n",
    "# Cannot easily represent diagonal or curved boundaries\n",
    "# May need many splits to approximate smooth transitions\n",
    "```\n",
    "\n",
    "#### 2. Data Fragmentation\n",
    "```python\n",
    "# Deep trees can create very specific rules\n",
    "path = \"Area > 2000 sq ft AND Location = 'Chelsea' AND \n",
    "        Bedrooms = 4 AND Has_Garden = True AND...\"\n",
    "\n",
    "# Problem: Few samples per leaf\n",
    "samples_per_rule = {\n",
    "    \"specific_rule_1\": 2,  # Too few to be reliable\n",
    "    \"specific_rule_2\": 3,\n",
    "    \"specific_rule_3\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. Prediction Discontinuities\n",
    "```\n",
    "Area   Price\n",
    "1999   £800K  # Just below threshold\n",
    "2001   £1.2M  # Just above threshold\n",
    "```\n",
    "\n",
    "### Solutions and Mitigations\n",
    "\n",
    "#### 1. Ensemble Methods\n",
    "```python\n",
    "# Instead of single tree:\n",
    "predictions = {\n",
    "    'random_forest': sum(tree.predict() for tree in trees) / len(trees),\n",
    "    'gradient_boost': sum(tree.predict() for tree in sequential_trees),\n",
    "    'bagging': weighted_average(tree.predict() for tree in bagged_trees)\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. Regularization Techniques\n",
    "```python\n",
    "tree_params = {\n",
    "    'max_depth': 10,          # Prevent excessive splitting\n",
    "    'min_samples_leaf': 20,   # Ensure reliable leaf nodes\n",
    "    'min_impurity_decrease': 0.01  # Require meaningful splits\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. Cross-Validation\n",
    "```python\n",
    "# Test different parameter combinations\n",
    "params_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_leaf': [10, 20, 50],\n",
    "    'min_impurity_decrease': [0.01, 0.02, 0.05]\n",
    "}\n",
    "```\n",
    "\n",
    "## Ethical Considerations for Decision Tree Models\n",
    "\n",
    "When applying decision trees to housing price prediction, we must carefully consider the ethical implications and societal impact of our models.\n",
    "\n",
    "### 1. Bias in Training Data\n",
    "\n",
    "#### Understanding Data Bias\n",
    "\n",
    "Historical housing data often reflects societal inequalities and biases:\n",
    "- Certain areas may be over or under-represented\n",
    "- Quality of data may vary by neighborhood\n",
    "- Historical redlining effects may persist in the data\n",
    "- Property features may be inconsistently recorded across areas\n",
    "\n",
    "#### Example of Data Bias\n",
    "Consider two neighborhoods:\n",
    "\n",
    "**Affluent Area:**\n",
    "- 1000+ property records\n",
    "- Complete feature sets (area, condition, amenities)\n",
    "- Regular price updates\n",
    "- Detailed property descriptions\n",
    "\n",
    "**Developing Area:**\n",
    "- Only 100 property records\n",
    "- Missing features\n",
    "- Irregular price updates\n",
    "- Basic property information only\n",
    "\n",
    "This disparity in data quality and quantity can lead to:\n",
    "- Less accurate predictions in underrepresented areas\n",
    "- Reinforcement of existing price disparities\n",
    "- Lower confidence in predictions for certain areas\n",
    "\n",
    "#### Mitigation Strategies\n",
    "\n",
    "1. **Data Collection**\n",
    "   - Actively gather data from underrepresented areas\n",
    "   - Standardize data collection across all neighborhoods\n",
    "   - Partner with community organizations for local insights\n",
    "\n",
    "2. **Model Development**\n",
    "   - Weight samples to balance representation\n",
    "   - Use stratified sampling across neighborhoods\n",
    "   - Include confidence intervals with predictions\n",
    "\n",
    "3. **Regular Auditing**\n",
    "   - Monitor prediction accuracy across different areas\n",
    "   - Track error rates by neighborhood\n",
    "   - Assess impact on different communities\n",
    "\n",
    "### 2. Fairness and Discrimination\n",
    "\n",
    "#### Protected Characteristics\n",
    "\n",
    "Decision trees must not perpetuate discrimination based on:\n",
    "- Race, ethnicity, or national origin\n",
    "- Religion\n",
    "- Gender\n",
    "- Age\n",
    "- Disability status\n",
    "- Family status\n",
    "\n",
    "#### Direct and Indirect Bias\n",
    "\n",
    "Consider these two approaches:\n",
    "\n",
    "**Problematic Approach:**\n",
    "```pre\n",
    "If neighborhood = \"historically_disadvantaged\":\n",
    "    Predict lower value\n",
    "```\n",
    "\n",
    "**Better Approach:**\n",
    "```pre\n",
    "If distance_to_amenities < 1km:\n",
    "    If property_condition = \"excellent\":\n",
    "        Predict based on objective features\n",
    "```\n",
    "\n",
    "The second approach uses objective criteria rather than potentially biased historical patterns.\n",
    "\n",
    "#### Monitoring for Fairness\n",
    "\n",
    "1. Track prediction ratios across different groups\n",
    "2. Compare error rates between communities\n",
    "3. Analyze the impact of model updates on different areas\n",
    "4. Review feature importance for potential proxy discrimination\n",
    "\n",
    "### 3. Market Impact and Social Responsibility\n",
    "\n",
    "#### Housing Market Effects\n",
    "\n",
    "Our models can influence:\n",
    "1. **Buyer Behaviour**\n",
    "   - Setting price expectations\n",
    "   - Influencing negotiation starting points\n",
    "   - Affecting perceived neighborhood value\n",
    "\n",
    "2. **Market Dynamics**\n",
    "   - Property valuation standards\n",
    "   - Investment patterns\n",
    "   - Neighborhood development\n",
    "\n",
    "3. **Housing Accessibility**\n",
    "   - Affordability assessments\n",
    "   - Mortgage approvals\n",
    "   - Insurance rates\n",
    "\n",
    "#### Responsible Implementation\n",
    "1. **Transparency**\n",
    "   - Clearly explain model limitations\n",
    "   - Provide confidence intervals\n",
    "   - Document all assumptions\n",
    "   - Share key factors affecting predictions\n",
    "\n",
    "2. **Community Impact**\n",
    "   - Engage with local stakeholders\n",
    "   - Consider neighborhood stability\n",
    "   - Monitor displacement risks\n",
    "   - Support housing accessibility\n",
    "\n",
    "3. **Market Stability**\n",
    "   - Avoid reinforcing speculation\n",
    "   - Maintain price prediction stability\n",
    "   - Consider local market conditions\n",
    "   - Support sustainable growth\n",
    "\n",
    "### 4. Best Practices for Ethical Use\n",
    "\n",
    "#### Development Guidelines\n",
    "\n",
    "1. **Data Collection**\n",
    "   - Ensure representative samples\n",
    "   - Document data sources\n",
    "   - Validate data quality\n",
    "   - Address historical biases\n",
    "\n",
    "2. **Model Design**\n",
    "   - Use interpretable features\n",
    "   - Avoid proxy discrimination\n",
    "   - Include uncertainty measures\n",
    "   - Document design choices\n",
    "\n",
    "3. **Testing and Validation**\n",
    "   - Test across diverse scenarios\n",
    "   - Validate with community input\n",
    "   - Monitor for unintended consequences\n",
    "   - Regular fairness audits\n",
    "\n",
    "#### Deployment Considerations\n",
    "1. **Model Release**\n",
    "   - Gradual rollout\n",
    "   - Monitor impact\n",
    "   - Gather feedback\n",
    "   - Ready to adjust\n",
    "\n",
    "2. **Ongoing Oversight**\n",
    "   - Regular audits\n",
    "   - Community feedback\n",
    "   - Impact assessment\n",
    "   - Update protocols\n",
    "\n",
    "#### Documentation Requirements\n",
    "\n",
    "Your model documentation should include:\n",
    "1. Training data sources and limitations\n",
    "2. Feature selection rationale\n",
    "3. Fairness considerations and tests\n",
    "4. Known biases and limitations\n",
    "5. Intended use guidelines\n",
    "6. Impact monitoring plan\n",
    "\n",
    "Ethical considerations aren't just a compliance checklist—they're fundamental to building models that serve society fairly and responsibly. Regular review and adjustment of these practices ensures our models contribute positively to the housing market and community well-being.\n",
    "\n",
    "## Theory Conclusion\n",
    "\n",
    "Now that we've explored the key concepts behind decision trees, let's summarize the main points and how they apply to our house price prediction task:\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Regression Trees vs Classification Trees** \n",
    "   - For house price prediction, we use regression trees\n",
    "   - Unlike classification trees (Gini impurity/entropy), regression trees minimize variance in target variable (house prices) within each node\n",
    "   - Different metrics for different tasks:\n",
    "     - MSE for regression\n",
    "     - Gini/Entropy for classification\n",
    "\n",
    "2. **Splitting Criterion**\n",
    "   - Regression trees use reduction in Mean Squared Error (MSE)\n",
    "   - At each node, algorithm chooses split maximizing reduction:\n",
    "\n",
    "   $\\Delta MSE = MSE_{parent} - (w_{left} * MSE_{left} + w_{right} * MSE_{right})$\n",
    "\n",
    "   Where $w_{left}$ and $w_{right}$ are the proportions of samples in left and right child nodes\n",
    "\n",
    "3. **Recursive Splitting**\n",
    "   - Tree built by recursively applying splitting process\n",
    "   - Creates hierarchy of decision rules\n",
    "   - Continues until stopping condition met:\n",
    "     - Maximum tree depth reached\n",
    "     - Minimum samples per leaf achieved\n",
    "     - No further improvement possible\n",
    "\n",
    "4. **Prediction Process**\n",
    "   - Follow decision rules from root to leaf node\n",
    "   - Prediction is mean price of houses in leaf node\n",
    "   - Clear, interpretable decision path\n",
    "\n",
    "### Data Handling and Model Characteristics\n",
    "\n",
    "5. **Data Preparation**\n",
    "   - Numerical features: Use directly without transformation\n",
    "   - Categorical features require encoding:\n",
    "     - One-hot encoding for low-cardinality\n",
    "     - Target encoding for high-cardinality\n",
    "     - Ordinal encoding for ordered categories\n",
    "   - Binary features: Simple 1/0 encoding\n",
    "\n",
    "6. **Interpretability**\n",
    "   - Can visualize tree and follow decision path\n",
    "   - Provides insights into feature importance\n",
    "   - Clear decision rules for predictions\n",
    "   - Natural feature selection through split choices\n",
    "\n",
    "7. **Bias-Variance Trade-off**\n",
    "   - Deeper trees: More complex relationships but risk overfitting (high variance)\n",
    "   - Shallower trees: More generalizable but may oversimplify (high bias)\n",
    "   - Balance crucial for optimal performance\n",
    "   - Cross-validation helps find optimal depth\n",
    "\n",
    "8. **Feature Importance**\n",
    "   - Natural feature selection through tree construction\n",
    "   - More important features appear:\n",
    "     - Higher in tree\n",
    "     - In more splits\n",
    "     - With larger reduction in impurity\n",
    "\n",
    "9. **Advanced Capabilities**\n",
    "   - Handles non-linear relationships unlike linear regression\n",
    "   - Captures complex interactions between features\n",
    "   - No feature scaling required\n",
    "   - Natural handling of missing values\n",
    "\n",
    "10. **Limitations and Solutions**\n",
    "    - Instability: Small data changes can result in very different trees\n",
    "    - Solution: Ensemble methods like Random Forests\n",
    "    - Struggles with smooth, linear relationships\n",
    "    - Limited extrapolation capability\n",
    "    - May create biased trees if data is unbalanced\n",
    "\n",
    "### Error Metrics and Evaluation\n",
    "\n",
    "11. **Understanding Error Metrics**\n",
    "    - Training uses MSE for splitting decisions\n",
    "    - Evaluation often uses MAE for interpretability\n",
    "    - MSE formula for node impurity:\n",
    "      $MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2$\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "As we move to practical implementation, we'll focus on:\n",
    "1. Applying these concepts to real housing data\n",
    "2. Using scikit-learn's decision tree implementation\n",
    "3. Tuning hyperparameters for optimal performance\n",
    "4. Interpreting and visualizing tree decisions\n",
    "5. Understanding feature importance\n",
    "6. Handling real-world data challenges\n",
    "\n",
    "This theoretical foundation prepares us for the practical challenges of implementing decision trees for house price prediction, while understanding both the power and limitations of the approach. The next lesson will demonstrate how to implement these concepts using Python and scikit-learn, and how to gain insights into the London housing market using decision trees.\n",
    "\n",
    "As we move forward to apply these concepts to our London housing dataset, keep in mind that while the theory provides the foundation, the real insights often come from experimenting with the data, tuning the model, and interpreting the results in the context of the problem at hand.\n",
    "\n",
    "### Next lesson: [2b_decision_trees_practical.ipynb](./2b_decision_trees_practical.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
